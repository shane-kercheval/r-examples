---
title: "Regression Examples"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
#library(rtools)
library(tidyverse)
# library(scales)
# library(stringr)
# library(lubridate)
# library(ggrepel)
# library(forecast)
library(knitr)

options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)

source('regression_helpers.R')

theme_set(theme_light())

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)
```

This data contains weekly prices and sales for three OJ brands, as well as an indicator `feature` showing whether each brand was advertised (in store or flyer) that week. (Business Data Science, pg 43).

```{r data}
weekly_oj_sales <- read.csv('data/oj.csv') %>% rename(featured = feat) %>% mutate(featured = featured == 1)
head(weekly_oj_sales)
```


```{r}
weekly_oj_sales %>%
    ggplot(aes(x=price, y=sales, color = brand)) +
    geom_point(alpha=0.1) +
    geom_smooth() +
    facet_wrap(~ featured)
```

```{r}
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales), color = brand)) +
    geom_point(alpha=0.1) +
    geom_smooth() +
    facet_wrap(~ featured)
```


```{r}
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales), color = brand)) +
    geom_point(alpha=0.1) +
    geom_smooth(method = 'lm') +
    facet_wrap(~ featured)
```

- Sales decrease with price.

# Regression

## Results

```{r}
reg_results <- lm(log(sales) ~ log(price)*brand*featured, data = weekly_oj_sales)
summary(reg_results)
```

```{r}
names(reg_results)
```

```{r}
coefficients(reg_results)
```

```{r}
coefficients(reg_results)['log(price)']
```

```{r}
reg_results %>%
    tidy() %>%
    mutate(conf.low = estimate - (2 * std.error),
           conf.high = estimate + (2 * std.error)) %>%
    filter(term != '(Intercept)') %>%
    mutate(color = case_when(
        estimate > 0 & conf.low > 0 ~ 'blue',
        estimate < 0 & conf.high < 0 ~ 'red',
        TRUE ~ 'grey'
    )) %>%
    mutate(term = fct_reorder(term, estimate)) %>%
    ggplot(aes(x = term, y=estimate, color=color)) +
    geom_point() +
    scale_color_manual(values=c('blue','dark grey','red')) +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    coord_flip() +
    theme(legend.position = 'none') +
    labs(title = 'Regression Coefficients')
```

```{r}
actual_values <- log(weekly_oj_sales$sales)
predicted_values <- predict(reg_results) %>% as.numeric()
residual_values <- residuals(reg_results) %>% as.numeric()

equal <- function(.expected, .actual, .precision=6) {
    round(.expected, .precision) == round(.actual, .precision)
}
all(equal(actual_values - predicted_values, residual_values))
```

## Anova Table

https://stats.stackexchange.com/questions/115304/interpreting-output-from-anova-when-using-lm-as-input
https://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq
http://rinterested.github.io/statistics/anova_of_OLS_models.html

Why use anova?

The Anova Table can be used to calculate the percent of "explained variance" of the model.

Also, from https://stats.stackexchange.com/questions/115304/interpreting-output-from-anova-when-using-lm-as-input

> First of all, you may be perfectly satisfied with the summary output, and that's fine. However, the ANOVA table may offer some advantages.
> 
> First, if you have a categorical / factor variable with more than two levels, the summary output is hard to interpret. It will give you tests of individual levels against the reference level, but won't give you a test of the factor as a whole.
>
> Another reason you might prefer to look at an ANOVA table is that it allows you to use information about the possible associations between your independent variables and

```{r}
anova_table <- reg_results %>%
    anova() %>%
    tidy() %>%
    mutate(p.value = round(p.value, 5)) %>%
    arrange(term)
anova_table
```

The sum of `sumsq` (i.e. `sum of squares`) is the same as `Total Sum of Suares (SST)` explained in Introductory Econometrics 7e pg. 34.

```{r}
# Total Sum of Squares (SST)
(SST <- sum((log(weekly_oj_sales$sales) - mean(log(weekly_oj_sales$sales)))^2))
```

```{r}
equal(SST, sum(anova_table$sumsq))
```

The `sumsq` value of `Residuals` is the same as the `Residual Sum of Squares (SSR)`

Sum of Residuals

```{r}
# Residual Sum of Squares ()
(SSR <- sum(residual_values^2))
```

```{r}
SSR == anova_table %>% filter(term == 'Residuals') %>% pull(sumsq)
```

The `Explained Sum of Squares (SSE)` is the same as all of the `sumsq` values added together, excluding the `Residuals` value.

`SST = SSE + SSR`

```{r}
# Explained Sum of Squares (SSE)
(SSE <- sum((fitted(reg_results) - mean(log(weekly_oj_sales$sales)))^2))
```

```{r}
equal(SSE, anova_table %>% filter(term != 'Residuals') %>% pull(sumsq) %>% sum())
```

```{r}
equal(SST, SSE + SSR)
```

R-squared is simply the percent of variation explained by the model (i.e. SSE / SST)

```{r}
summary(reg_results)$r.squared
```

```{r}
SSE / SST
```

```{r}
# can also use the R2 function provided by Taddy in Business Data Science pg 72
# which will work for models beyond linear/logistic regression
R2(y=log(weekly_oj_sales$sales), pred = predicted_values, family = 'gaussian')
```

However, R-squared will go up every time you add a new feature. So you can artificially inflate this number.

`Adjusted R-Squared` "imposes a penatly for adding additional independent variables to a model." (Introductory Econometrics 7E)

```{r}
summary(reg_results)$adj.r.squared
```

## Amount of Variation Explained

```{r}
anova_table %>%
    select(term, sumsq) %>%
    mutate(percent_variation = percent(sumsq / sum(sumsq))) %>%
    arrange(desc(sumsq))
```

```{r}
plot_regression_variance_explained(reg_results)
```

## Anova to Compare Models

https://bookdown.org/ndphillips/YaRrr/comparing-regression-models-with-anova.html

> To compare the fits of two models, you can use the anova() function with the regression objects as two separate arguments. The anova() function will take the model objects as arguments, and return an ANOVA testing whether the more complex model is significantly better at capturing the data than the simpler model. If the resulting p-value is sufficiently low (usually less than 0.05), we conclude that the more complex model is significantly better than the simpler model, and thus favor the more complex model. If the p-value is not sufficiently low (usually greater than 0.05), we should favor the simpler model.

(This only make statistical sense if the models are nested.)

```{r}
anova(lm(log(sales) ~ log(price)+brand+featured, data = weekly_oj_sales),
      lm(log(sales) ~ log(price)*brand*featured, data = weekly_oj_sales))
```

```{r}
#install.packages('effects')
plot(effects::effect(c('log(price)', 'brand'), reg_results))
```

## Plot Assumptions

```{r}
plot(reg_results) 
```

```{r}
plot_actual_vs_predicted(reg_results)
```

```{r}
plot_residual_vs_predicted(model=reg_results)
```

```{r}
plot_residual_vs_variable(model=reg_results,
                          predictor = 'price',
                          dataset = weekly_oj_sales %>% mutate(`log(price)` = log(price)))
```

## Interpretation

### Elasticities (i.e. regression slopes)

`log(sales) = intercept + log(price) + e` gives a regression line describing the expected sales given price. This is the price elasticity of sales (i.e. elasticity of sales based on price).

```{r}
reg_results <- lm(log(sales) ~ log(price), data = weekly_oj_sales)
get_regression_equation(reg_results)
coef(reg_results)['log(price)']
```

This is an example of a log-log model, which have an interpretation of 

> sales increase by B% for every 1% increase in price (Business Data Science pg. 45-47)

Where `B` is the coefficient on price.

So `sales` drop (i.e. increase by negative coefficient) by about ``r paste0(round(coef(reg_results)['log(price)'], 1), '%')`` for every `1%` increase in `price`.

```{r}
#breaks <- c(-0.5, 0, 0.5, 1, 1.5)
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales))) +
    geom_point(alpha=0.2) +
    geom_smooth(method='lm')# +
   # scale_x_continuous(breaks = breaks, labels = round(exp(breaks), 1))
```

---

`log(sales) = intercept + log(price) + brand + e` gives a regression line that has a different intercept for each brand, but the same price elasticities. This equation says "even though all brand sales have the same elasticity to price, at the same price they will have different expected sales." i.e. they have the same regression slope.

```{r}
reg_results <- lm(log(sales) ~ log(price) + brand, data = weekly_oj_sales)

price_elasticity <- coef(reg_results)['log(price)']

dominicks_intercept <- coefficients(reg_results)['(Intercept)']
minute_maid_intercept <- coefficients(reg_results)['(Intercept)'] + coefficients(reg_results)['brandminute.maid']
tropicana_intercept <- coefficients(reg_results)['(Intercept)'] + coefficients(reg_results)['brandtropicana']

get_regression_equation(reg_results)
```

In this example, we see that when we control for brand, `sales` are expected to drop by about ``r paste0(round(abs(price_elasticity), 1), '%')`` for every `1%` increase in `price`.

Each brand is allowed to have it's own intercept, meaning that "even though all brand sales have the same elasticity to price, at the same price they will have different expected sales." (BDS pg 46)

The intercept gives the value for `Dominick`'s log sales at a log price of 0 (i.e. when all variables have a value of zero i.e. `log(price)` = $0, `brandminute.maid` = 0, `brandtropicana` = 0)

- Intercept for Dominicks: ``r dominicks_intercept``
- Intercept for Minute Maid: ``r minute_maid_intercept``
- Intercept for Tropicana: ``r tropicana_intercept``

```{r}
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales), color=brand)) +
    geom_point(alpha=0.2) +
    geom_abline(slope = price_elasticity, intercept = dominicks_intercept, color='red', size=1.5) +
    geom_abline(slope = price_elasticity, intercept = minute_maid_intercept, color='green', size=1.5) +
    geom_abline(slope = price_elasticity, intercept = tropicana_intercept, color='blue', size=1.5)
```

As mentioned, all brands in this model share that same elasticity (regression slope). "This is unrealistic: money is less of an issue for Tropicana customer than it is for the average Dominick's customer." (BDS pg 47)

---

`log(sales) = intercept + log(price) + brand + log(price)*brand + e` allows us to add an interaction term. (regression shorthand is: `log(sales) ~ log(price) * brand`). The result is a separate intercept (which we had in the last model) as well as a separate slope (elasticity) for each brand.

```{r}
reg_results <- lm(log(sales) ~ log(price) * brand, data = weekly_oj_sales) # same as lm(log(sales) ~ log(price) + brand + log(price)*brand, data = weekly_oj_sales)

dominicks_price_elasticity <- coef(reg_results)['log(price)']
minute_maid_price_elasticity <- coef(reg_results)['log(price)'] + coef(reg_results)['log(price):brandminute.maid']
tropicana_price_elasticity <- coef(reg_results)['log(price)'] + coef(reg_results)['log(price):brandtropicana']

get_regression_equation(reg_results)
```

Now it no longer makes sense to talk about an overall elasticity number, but rather elasticity for each brand.

- Elasticity for Dominicks: ``r dominicks_price_elasticity`` (`sales` are expected to drop by about ``r paste0(round(abs(dominicks_price_elasticity), 1), '%')`` for every `1%` increase in `price`)
- Intercept for Minute Maid: ``r minute_maid_price_elasticity`` (`sales` are expected to drop by about ``r paste0(round(abs(minute_maid_price_elasticity), 1), '%')`` for every `1%` increase in `price`)
- Intercept for Tropicana: ``r tropicana_price_elasticity`` (`sales` are expected to drop by about ``r paste0(round(abs(tropicana_price_elasticity), 1), '%')`` for every `1%` increase in `price`)

```{r}
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales), color=brand)) +
    geom_point(alpha=0.2) +
    geom_smooth(method='lm')
```

---

Now let's look at the affect of advertising using the `featured` dummy variable, "indicating that a given brand was promoted with either an in-store display promo or a flier ad during the week that sales and prices were recorded. The ads can increase sales at all prices, and they can change the price sensitivity, and they can do both of these things in a brand-specific manner." (BDS pg 48)

We add a 3-way interaction between price, brand, and featured; meaning that each brand will have different elasticities and intercepts, which will be different for featured vs not-featured.

```{r}
reg_results <- lm(log(sales) ~ log(price) * brand * featured, data = weekly_oj_sales)

format_elasticity <- function(.x) {
    paste0(round(.x, 1), '%')
}

dominicks_not_featured_price_elasticity <- format_elasticity(coef(reg_results)['log(price)'])
dominicks_featured_price_elasticity <- format_elasticity(coef(reg_results)['log(price)'] + coef(reg_results)['log(price):featuredTRUE'])

# relevant coefficients
# 'log(price)'
# 'log(price):brandminute.maid'
# 'log(price):brandminute.maid:featuredTRUE'
# 'log(price):featuredTRUE'
minute_maid_not_featured_price_elasticity <- coef(reg_results)['log(price)'] + coef(reg_results)['log(price):brandminute.maid']
minute_maid_featured_price_elasticity <- format_elasticity(minute_maid_not_featured_price_elasticity + 
                                                               coef(reg_results)['log(price):brandminute.maid:featuredTRUE'] +
                                                               coef(reg_results)['log(price):featuredTRUE'])
minute_maid_not_featured_price_elasticity <- format_elasticity(minute_maid_not_featured_price_elasticity)

topicana_not_featured_price_elasticity <- coef(reg_results)['log(price)'] + coef(reg_results)['log(price):brandtropicana']
topicana_featured_price_elasticity <- format_elasticity(topicana_not_featured_price_elasticity +
                                                            coef(reg_results)['log(price):brandtropicana:featuredTRUE'] +
                                                            coef(reg_results)['log(price):featuredTRUE'])
topicana_not_featured_price_elasticity <- format_elasticity(topicana_not_featured_price_elasticity)

get_regression_equation(reg_results)
```

Now it no longer makes sense to talk about an overall elasticity number for each brand, but rather elasticity per brand depending on `featured`.

| Featured | Dominick's | Minute Maid | Tropicana |
| ---------|------------|-------------|-----------|
|  No      | ``r dominicks_not_featured_price_elasticity`` |  ``r minute_maid_not_featured_price_elasticity`` | ``r topicana_not_featured_price_elasticity``  |
|  Yes     | ``r dominicks_featured_price_elasticity`` |  ``r minute_maid_featured_price_elasticity`` | ``r topicana_featured_price_elasticity``  |

```{r}
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales), color=brand)) +
    geom_point(alpha=0.2) +
    geom_smooth(method='lm') +
    facet_wrap(~ featured)
```

... "It could be that the demand curve is nonlinear" ....

```{r}
weekly_oj_sales %>%
    ggplot(aes(x=log(price), y=log(sales), color=brand)) +
    geom_point(alpha=0.2) +
    geom_smooth() +
    facet_wrap(~ featured)
```

## Regression Coefficient Uncertainty

BDS pg. 58

For example, "the usual standard errors will be wrong if there are heteroskedastic errors" or "if there is any dependence betweeen observations".

"The `AER` package can be used to obtain HC standard errors with little effort." "It turns out that, for OLS, the parameter variance estimates you get from the nonparametric bootstrap actually approximated by the HC procedure. That is, you can use the HC standard errors as a fast alternative to bootstrapping for OLS." 

```{r}
library(AER)
coefficient_variances <- vcovHC(reg_results)
```

"This is the sampling covariance matrix of the coefficients; variances are along the diagonal. To get the standard error, which is the sampling standard deviation, you need to take the square root of the diagnal variance estimate." BDS pg 60

```{r}
coefficient_names <- colnames(coefficient_variances) %>% rtools::rt_remove_val('(Intercept)')
coefficient_starndard_errors <- map_dbl(coefficient_names, ~sqrt(coefficient_variances[ ., .]))
names(coefficient_starndard_errors) <- coefficient_names
coefficient_starndard_errors
```

```{r}
reg_results %>%
    tidy() %>%
    filter(term != '(Intercept)') %>%
    dplyr::select(term, `std.error`) %>%
    rename(original_standard_errors = std.error) %>%
    mutate(new_standard_errors = coefficient_starndard_errors,
           perc_diff = percent((coefficient_starndard_errors - original_standard_errors) / original_standard_errors))
```

## Out of Sample R-Squared







