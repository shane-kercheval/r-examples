---
title: "Regression Examples"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
#library(rtools)
library(tidyverse)
# library(scales)
# library(stringr)
# library(lubridate)
# library(ggrepel)
# library(forecast)
library(knitr)

options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)

source('../regression/regression_helpers.R')

theme_set(theme_light())

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)
```

This data contains weekly prices and sales for three OJ brands, as well as an indicator `feature` showing whether each brand was advertised (in store or flyer) that week. (Business Data Science, pg 43).

```{r data}
emails <- read.csv('data/spam.csv')
head(emails[, c(1:4, ncol(emails))])
```

Logistic Regression is a linear model for log odds. The odds of an event are the probability that it happens over the probability that it doesn't.

`log[ p / (1-p) ] = B0 + B1X1 + B2X2 + ... + error`

```{r}
reg_results <- glm(spam ~ ., data=emails, family='binomial')
summary(reg_results)
```

```{r}
get_regression_equation(reg_results)
```

```{r}
log_odds_to_probability <- function(.log_odds) {
    if(.log_odds < 0) {
        
        .odds <- 1 / exp(.log_odds)
        return (1 - (.odds / (1 + .odds)))
        
    } else {
    
        .odds <- exp(.log_odds)
        # log odds = probability / 1 - probability
        return (.odds / (1 + .odds))
    }
}
(log_odds_free <- coef(reg_results)['word_free'])
(odds_free <- exp(log_odds_free))
# log odds = probability / 1 - probability
(probability_free <- odds_free / (1 + odds_free))
probability_free_check <- log_odds_to_probability(log_odds_free)
```

The log odds that an email is spam increase by ``r round(log_odds_free, 1)`` if the email contains the word `free`.

The odds that an email is spam increase by ``r round(odds_free, 1)`` times if the email contains the word `free`.

The probably that an email is spam is ``r percent(probability_free)`` if the email contains the word `free`. ??? not sure if this is correct interpretation.

```{r}
log_odds_to_probability(1)
log_odds_to_probability(-1)
```

```{r}
coef(reg_results)['word_george']
-1 / exp(coef(reg_results)['word_george'])
log_odds_to_probability(coef(reg_results)['word_george'])
```

```{r}
.log_odds <- 5
exp(.log_odds)

.log_odds <- -5
-1 / exp(.log_odds)
```

```{r}
new_data <- c(rep(0, ncol(emails) - 4), mean(emails$capital_run_length_average), mean(emails$capital_run_length_longest), mean(emails$capital_run_length_total))
new_data <- c(rep(0, ncol(emails) - 1))
names(new_data) <- colnames(emails[, 1:(ncol(emails)-1)])
new_data <- t(as.data.frame(new_data)) %>% as.data.frame()
predict(reg_results,
        newdata = new_data,
        type = 'response')

new_data[1, 'word_free'] <- 1
predict(reg_results,
        newdata = new_data,
        type = 'response')

new_data[1, 'word_george'] <- 1
predict(reg_results,
        newdata = new_data,
        type = 'response')
```

```{r}
R2(y=emails$spam, pred=predict(reg_results, type='response'), family = 'binomial')
```

"The difference between Residual Deviance and Null Deviance is due to information contained in teh covariates. An important metric of how tightly your regression fits is the proprortion of deviance explained by x, R-Squared" (BDS pg 56)

```{r}
(reg_results$null.deviance - reg_results$deviance) / reg_results$null.deviance
```

```{r}
emails %>%
    mutate(spam = as.logical(spam),
           predictions = predicted_values) %>%
    ggplot(aes(x=spam, y=predicted_values, group=spam)) +
    geom_boxplot()
```

```{r}
confusion_matrix <- function(actual_classes, predicted_classes, positive_class) {
    print(table(actual_classes, predicted_classes))
    
    stopifnot(positive_class %in% actual_classes)
    stopifnot(positive_class %in% predicted_classes)

    actual_classes <- ifelse(actual_classes == positive_class, TRUE, FALSE)
    predicted_classes <- ifelse(predicted_classes == positive_class, TRUE, FALSE)
    
    actual_positives = sum(actual_classes)
    actual_negatives = sum(!actual_classes)
    total_observations <- length(actual_classes)
    
    stopifnot(actual_positives + actual_negatives == total_observations)

    true_positives = sum(actual_classes & predicted_classes)
    true_negatives = sum(!actual_classes & !predicted_classes)
    false_positives = sum(predicted_classes & !actual_classes)
    false_negatives = sum(!predicted_classes & actual_classes)
    
    stopifnot(true_positives + false_negatives == actual_positives)

    # todo doesn't handle divide-by-zero, etc.
    print(paste('true_positive_rate a.k.a sensitivity:', round(true_positives / actual_positives, 4)))
    print(paste('true_negative_rate a.k.a specificity:', round(true_negatives / actual_negatives, 4)))
    print(paste('false_positive_rate:', round(false_positives / actual_negatives, 4)))
    print(paste('false_negative_rate:', round(false_negatives / actual_positives, 4)))
    # if you predict it's going to be positive (denominator), how often is it (numerator)?
    print(paste('positive_predictive_value:', round(true_positives / (true_positives + false_positives), 4)))
    # if you predict it's going to be negative (denominator), how often is it (numerator)?
    print(paste('negative_predictive_value:', round(true_negatives / (true_negatives + false_negatives), 4)))
    print(paste('accuracy:', round((true_negatives + true_positives) / total_observations, 4)))
    print(paste('error_rate:', round((false_positives + false_negatives) / total_observations, 4)))
    print(paste('prevalence:', round(actual_positives / total_observations, 4)))
}

confusion_matrix(actual_classes=emails$spam,
                 predicted_classes=round(predict(reg_results, type='response')),
                 positive_class=1)
```
