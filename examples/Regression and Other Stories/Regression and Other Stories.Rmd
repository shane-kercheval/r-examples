---
title: "Regression and Other Stories"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
#library(rtools)
# library(stringr)
# library(ggrepel)
# library(forecast)
#library(scales)
#library(lubridate)

library(knitr)

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_5 <- calculate_plot_width(5)
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)

pivot_longer_all <- function(.x, names_to='name', values_to='value') {
    .x %>%
        mutate(temp____index = row_number()) %>%
        pivot_longer(-temp____index, names_to=names_to, values_to=values_to) %>%
        select(-temp____index)
}
```

```{r downloading_data, eval=FALSE, include=FALSE}
download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat',
              destfile = 'data/hibbs.dat', quiet = TRUE)

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Congress/data/congress.csv',
              destfile = 'data/congress.csv', quiet = TRUE)

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/NES/data/nes.txt',
              destfile = 'data/nes.csv', quiet = TRUE)

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/KidIQ/data/kidiq.csv',
              destfile = 'data/kidiq.csv', quiet = TRUE)

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Earnings/data/earnings.csv',
              destfile = 'data/earnings.csv', quiet = TRUE)

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mesquite/data/mesquite.dat',
              destfile = 'data/mesquite.dat')

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Student/data/student-merged.csv',
              destfile = 'data/student-merged.csv')

download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Arsenic/data/wells.csv',
              destfile = 'data/wells.csv')
```

# Overview

This document includes examples and exercises from `Regression and Other Stories` by Andrew Gelman, Jennifer Hill and Aki Vehtari, 2020, first edition.

# Resources

[Errata](https://avehtari.github.io/ROS-Examples/errata.html)

[Authors' Code](https://avehtari.github.io/ROS-Examples/examples.html#Examples_by_chapters)

# Packages

## statistical packages

```{r message=FALSE, warning=FALSE}
library(rstan)
library(rstanarm)
library(bayesplot)
library(loo)
```

## base packages

```{r message=FALSE, warning=FALSE}
library(broom)
library(broom.mixed)

library(tidyverse)
library(ggplot2)
library(scales)
library(ggridges)
```

## Settings

```{r}
theme_set(theme_light())
options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)
```

```{r include=FALSE}
congress <- read_csv("data/congress.csv")
earnings <- read_csv("data/earnings.csv")
hibbs <- read.table('data/hibbs.dat', header = TRUE)
kid_iq <- kidiq
mesquite <- read.table('data/mesquite.dat', header = TRUE)
metabolic_rate <- read_csv("data/Primate Body Mass and Basal Metabolic Rate.csv")
nes_data <- read.csv("data/nes.csv", sep = ' ')
students <- read_csv("data/student-merged.csv") %>%
    rename(math_score = G3mat) %>% # "third period math grade given student's school" https://avehtari.github.io/ROS-Examples/Student/student.html
    select(-starts_with("G", ignore.case = FALSE)) %>%
    filter(math_score > 0)
wells <- read_csv("data/wells.csv")
```

# Chapter 1 - Overview

## Simple Example with `stan_glm`

Load in the data:

```{r}
head(hibbs)
```

---

Graph data:

```{r figure.1.1.a, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hibbs %>%
    ggplot(aes(x=growth, y=vote)) +
    geom_hline(yintercept = 50, color="red", linetype="dashed") +
    geom_text(aes(label=year)) +
    geom_smooth(method='lm') +
    scale_x_continuous(labels=function(.x)paste0(.x, '%')) +
    scale_y_continuous(labels=function(.x)paste0(.x, '%')) +
    labs(title="Forecasting the election from the economy",
         y="Incuming party's vote share",
         x="Average recent growth in personal",
         caption='Figure 1.1')
```

---

Build Model:

```{r}
model <- stan_glm(vote ~ growth, data=hibbs)
```

---

Model Summary:

```{r}
summary(model)
```

---

Print()

```{r}
print(model)
```

---

Model Coefficients:

```{r}
coef(model)
```

---

Compare to `lm()`:

```{r}
summary(lm(vote ~ growth, data=hibbs))
```

(pretty close)

---

Graph using model coefficients rather than `geom_smooth(method='lm')`

```{r figure.1.1.a2, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hibbs %>%
    ggplot(aes(x=growth, y=vote)) +
    geom_hline(yintercept = 50, color="red", linetype="dashed") +
    geom_text(aes(label=year)) +
    #geom_smooth(method='lm') +
    geom_abline(intercept = coef(model)['(Intercept)'], slope = coef(model)['growth']) +
    scale_x_continuous(labels=function(.x)paste0(.x, '%')) +
    scale_y_continuous(labels=function(.x)paste0(.x, '%')) +
    labs(title="Forecasting the election from the economy",
         y="Incuming party's vote share",
         x="Average recent growth in personal",
         caption='Figure 1.1')
```

```{r include=FALSE}
rm(model)
```

---

# Chapter 3 - Basic Methods

## Log-Log Interpretation

Example from `Regression & Other Stories pg 39. (Data found elsewhere.)

### Example Model

```{r message=FALSE, warning=FALSE}
head(metabolic_rate)
```

```{r chapter_3_log_log_not_scaled, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
metabolic_rate %>%
    ggplot(aes(x=`Primate Mass`, y=`Metabolic Rate`)) +
    geom_point() +
    geom_text(data=metabolic_rate, 
              aes(x=`Primate Mass`, y=`Metabolic Rate`,
                  label=glue::glue("({ `Primate Mass` }, { `Metabolic Rate` })"),
                  color=NULL),
              vjust=-0.5, check_overlap = TRUE) +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "(Not Scaled)")
```

```{r chapter_3_log_log_scaled, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
metabolic_rate %>%
    ggplot(aes(x=`Primate Mass`, y=`Metabolic Rate`)) +
    geom_point() +
    geom_text(data=metabolic_rate, 
              aes(x=`Primate Mass`, y=`Metabolic Rate`,
                  label=glue::glue("({ `Primate Mass` }, { `Metabolic Rate` })"),
                  color=NULL),
              vjust=-0.5, check_overlap = TRUE) +
    scale_x_log10() +
    scale_y_log10() +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "Log Scaled")
```

```{r}
model <- lm(log(`Metabolic Rate`) ~ log(`Primate Mass`), data=metabolic_rate)
summary(model)
```

```{r}
coef(model)
```

### Interpretation

#### Interpretation from `Regression and Other Stores`

From `pg. 40`

"For example, when increasing body mass on this curve by a factor of 2, metabolic rate is multiplied by `2^0.74` = `1.7`. Multiplying body mass by `10` corresponds to multiplying metabolic rate by `10^0.74` = `5.5`, and so forth."

```{r}
(primate_mass_coefficient <- unname(coef(model)['log(`Primate Mass`)']))
```

Lets test this. We'll start with a body mass of `105`.

```{r}
(log_prediction_105 <- predict(model, newdata = data.frame(`Primate Mass`=105, check.names = FALSE)))
(prediction_105 <- exp(log_prediction_105))
```

Our prediction of the metabolic rate is ``r prediction_105``.

What is our prediction if we **double** the body mass?

We would expect a Metabolic Rate of ``r 2^primate_mass_coefficient * prediction_105``.

```{r}
2^primate_mass_coefficient * prediction_105
```

What is the actual prediction if we double the body mass?

```{r}
log_prediction_210 <- predict(model, newdata = data.frame(`Primate Mass`=210, check.names = FALSE))
(prediction_210 <- exp(log_prediction_210))
```

---

What is our prediction if we multiply body mass by `10`?

We would expect a Metabolic Rate of ``r 10^primate_mass_coefficient * prediction_105``.

```{r}
10^primate_mass_coefficient * prediction_105
```

What is the actual prediction if we double the body mass?

```{r}
log_prediction_1050 <- predict(model, newdata = data.frame(`Primate Mass`=1050, check.names = FALSE))
(prediction_1050 <- exp(log_prediction_1050))
```

---- 

#### Interpretation from `Introductory Econometrics 7e`

> "A one-unit difference in `log x` corresponds to an additive difference of `b` in `log y`. (R&OS pg. 39)

This is essentially "elasticity" and is interpreted as a `1%` change in x has a `b`% change in `y` (Intro Econometrics pg. 39), although this does not hold for large changes in `y` (Intro Econometrics pg. 186).

```{r}
format_percent <- function(.x) { paste0(round(.x, 3), '%')}
format_percent_scale <- function(.x) { paste0(round(.x * 100, 3), '%')}

(expected_one_percent_change <- format_percent(primate_mass_coefficient))
```

So, the interpretation of the `log(Primate Mass)` coefficient is that a `1%` change in `Primate Mass` results in a `~`r expected_one_percent_change`` change in `Metabolic Rate`.


Let's see if this is true.

If the `Primate Mass` is `105` then our prediction of `log(Metabolic Rate`) is ``r log_prediction_105``, which means our prediction of `Metabolic Rate` is ``r prediction_105``, which matches the point on the graph.

Now let's change `Primate Mass` (x) by `1%` and see how much `Metabolic Rate` (y) changes by.

```{r}
(one_percent_change <- (0.01 * 105) + 105)
one_percent_change_prediction <- predict(model, newdata = data.frame(`Primate Mass`=one_percent_change, check.names = FALSE))
(one_percent_change_prediction <- exp(one_percent_change_prediction))
```

So if `Primate Mass` is ``r one_percent_change`` then we predict that `Metabolic Rate` is ``r one_percent_change_prediction``.

So a `1%` change in `Primate Mass` resulted in the following percent change for `Metabolic Rate`:

```{r}
format_percent_scale((one_percent_change_prediction - prediction_105) / prediction_105)
```

Which matches the expected percent change from the coefficient.

```{r}
primate_mass_coefficient
```

---

Now, lets try with a larger change.

Lets predict `Metabolic Change` for a `Primate Mass` of `9,500`

```{r}
(percent_change <- (9500 - 105) / 105)
```

The percent-change in `Primate Mass` between these two values is ``r format_percent_scale(percent_change)``.

```{r}
(expected_percent_change_y <- as.numeric(coef(model)["log(`Primate Mass`)"]) * percent_change)
```

Therefore, if `Primate Mass` changes by ``r format_percent_scale(percent_change)``, the **expected** percent change in `Metabolic Rate` should be ``r format_percent_scale(expected_percent_change_y)``.

But the actual percent change is:

```{r}
(prediction_9500 <- exp(predict(model, newdata = data.frame(`Primate Mass`=9500, check.names = FALSE))))
format_percent_scale((prediction_9500 - prediction_105) / prediction_105)
```

Which is quite lower than what we expected.

So this rule of thumb doesn't work across large changes in `y`.

```{r include=FALSE}
rm(model)
rm(expected_one_percent_change)
rm(expected_percent_change_y)
rm(log_prediction_105)
rm(log_prediction_1050)
rm(log_prediction_210)
rm(one_percent_change)
rm(one_percent_change_prediction)
rm(percent_change)
rm(prediction_105)
rm(prediction_1050)
rm(prediction_210)
rm(prediction_9500)
rm(primate_mass_coefficient)
```

### Predictions

According to `Introductory Econometrics, Wooldridge (pg. 206)`,

> Because the exponential undoes the log, our first guess for predicting y (when dependent variable is log(y)) is to simply exponentiate the predicted value for `log(y): y_predicted = exp(log y_predicted))`. **This does not work; in fact, it will systematically underestimate the expected value of y**.

One adjustment that the author describes is the `Duan smearing estimate`.

> Given an esimtate `a0`, we can predict `y` as 
>
> `y_predicted = a0 * exp(log y_predicted)`

where, 

> `a0 = sum(exp(residuals)) / N`

The code below implements the `Duan smearing estimate`.

Note, that this adjustment necessary, not just for `log y = log x` models, but for `log y = x` models (any time the dependent variable is log y, regardless of X).

```{r}
#' This method returns the adjustment coefficient described in `Introductory Econometrics, Wooldridge (pg. 206)`
#' @param .model the regression model
duan_smearing_adjustment <- function(.model) {
    model_residuals <- residuals(.model)
    duan_smearing_coefficient <- sum(exp(model_residuals)) / length(model_residuals)
    
    return (duan_smearing_coefficient)
}

#' This method returns the prediction of `y`, when the dependent variable is `log y`, based on the adjustment coefficient described in `Introductory Econometrics, Wooldridge (pg. 206)`
#' @param .model the regression model
#' @param .newdata the data (e.g. test data) from which predictions will be generated
predict_from_log_y <- function(.model, .newdata) {
    
    duan_smearing_coefficient <- duan_smearing_adjustment(.model)
    .predictions <- predict(.model, newdata=.newdata)
    .predictions <- duan_smearing_coefficient*exp(.predictions)

    return (.predictions)
}
```

Now, let's create predictions for all `Primate Masses` between `50` and `80000`, which is around the interval of our data.

```{r}
model <- lm(log(`Metabolic Rate`) ~ log(`Primate Mass`), data=metabolic_rate)
all_predictions <- data.frame(`Primate Mass`=seq(50, 80000), check.names = FALSE)
log_y_predictions <- predict(model, newdata=all_predictions)

# this simple adjustment is also described on pg 206 but assumes normal residuals
all_predictions$`Predictions - Unadjusted` <- exp(log_y_predictions)
all_predictions$`Predictions - Adjusted - Simple` <- exp(((summary(model)$sigma)^2)/2)*exp(log_y_predictions)
all_predictions$`Predictions - Adjusted - Duan` <- predict_from_log_y(.model=model, .newdata=all_predictions)
head(all_predictions)
```

```{r chapter_3_log_log_scaled_regression, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
predictions_long <- all_predictions %>% 
    # get the original values, where applicable
    #left_join(metabolic_rate, by = 'Primate Mass') %>%
    pivot_longer(-`Primate Mass`, names_to = 'Prediction Type', values_to='Metabolic Rate')

metabolic_rate_plot <- predictions_long %>%
    ggplot(aes(x=`Primate Mass`, y=`Metabolic Rate`, color=`Prediction Type`)) +
    geom_point(data=metabolic_rate, aes(x=`Primate Mass`, y=`Metabolic Rate`, color=NULL)) +
    geom_text(data=metabolic_rate, 
              aes(x=`Primate Mass`, y=`Metabolic Rate`,
                  label=glue::glue("({ `Primate Mass` }, { `Metabolic Rate` })"),
                  color=NULL),
              vjust=-0.5, check_overlap = TRUE) +
    geom_line()

metabolic_rate_plot +
    scale_x_log10() +
    scale_y_log10() +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "Log Scaled")
```

```{r chapter_3_log_log_unscaled_regression, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
metabolic_rate_plot +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "Unscaled")
```

```{r include=FALSE}
rm(model)
rm(all_predictions)
rm(log_y_predictions)
rm(predictions_long)
rm(metabolic_rate_plot)
```

---

### Simulation of Log-Log Predictions

See discussion in previous section regarding systematic underprediction of `y` when dependent varaible is `log y`.

`log y = a + b*log x + random-noise`

```{r chapter_3_log_log_predictions_simulated, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
set.seed(1)
intercept <- 4
b_coefficient <- 3
simulated_x <- rexp(100000, rate = 0.5) + 1
noise <- rnorm(100000, sd = 0.5)

simulated_y <- exp(intercept + b_coefficient*log(simulated_x) + noise)

simulated_data <- data.frame(simulated_x, simulated_y)
simulated_data %>%
    ggplot(aes(x=simulated_x, y=simulated_y)) +
    geom_point(alpha=0.1) +
    scale_x_log10(breaks=2^seq(0,6)) +
    scale_y_log10(breaks=10^seq(1,6)) +
    labs(title="Simulated Data (`log y = a + b*log x + random-noise`)",
         subtitle = "Graphed on a log-scale.")
```

---

Create training/test sets, fit model on training data.

```{r}
training_indices <- sample.int(n=nrow(simulated_data), size = 0.7*nrow(simulated_data), replace = FALSE)
training_data <- simulated_data[training_indices,]
test_data <- simulated_data[-training_indices,]
model <- lm(log(simulated_y) ~ log(simulated_x), data=training_data)
summary(model)
```

```{r include=FALSE}
# make sure we don't use training data by accident
rm(training_data)
rm(training_indices)
```

---

Predict on test data.

```{r}
log_y_predictions <- predict(model, newdata=test_data)

# this simple adjustment is also described on pg 206 but assumes normal residuals
test_data$`Predictions - Unadjusted` <- exp(log_y_predictions)
test_data$`Predictions - Adjusted - Simple` <- exp(((summary(model)$sigma)^2)/2)*exp(log_y_predictions)
test_data$`Predictions - Adjusted - Duan` <- predict_from_log_y(.model=model, .newdata=test_data)
head(test_data)
```

---

```{r chapter_3_log_log_scaled_simulation, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
predictions_long <- test_data %>%
    select(-simulated_y) %>%
    # get the original values, where applicable
    #left_join(metabolic_rate, by = 'Primate Mass') %>%
    pivot_longer(-simulated_x, names_to = 'Prediction Type', values_to='Predicted Y')

simulated_plot <- predictions_long %>%
    ggplot(aes(x=simulated_x, y=`Predicted Y`, color=`Prediction Type`)) +
    geom_point(data=test_data, aes(x=simulated_x, y=simulated_y, color=NULL), alpha=0.2) +
    geom_line()

simulated_plot +
    scale_x_log10(breaks=2^seq(0,6)) +
    scale_y_log10(breaks=10^seq(1,6)) +
    labs(title="Simulated Data & Predictions",
         subtitle = "Log Scaled",
         caption="The 'Simple' and 'Duan' adjustments are slow close that they overlap,\nand the Duan is being hidden behind the Simple.")
```

---

```{r chapter_3_log_log_unscaled_simulation, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
simulated_plot +
    labs(title="Simulated Data & Predictions",
         subtitle = "Not Scaled",
         caption="The 'Simple' and 'Duan' adjustments are slow close that they overlap,\nand the Duan is being hidden behind the Simple.")
```

---

The adjustments seem to make sense, visualy; but we can check the `Root Mean Square Error` (`RMSE`) to see if the adjustments actually results in a better **out-of-sample** fit, below (which it appears is the case; there is a lower error for the adjusted predictions).

```{r}
rmse = function(fitted, actual){
  sqrt(mean((fitted - actual)^2))
}

rmse(fitted=test_data$`Predictions - Unadjusted`, actual=test_data$simulated_y)
rmse(fitted=test_data$`Predictions - Adjusted - Simple`, actual=test_data$simulated_y)
rmse(fitted=test_data$`Predictions - Adjusted - Duan`, actual=test_data$simulated_y)
```

NOTE: the "Simple" adjustment (found in `Introductory Econometrics, Wooldridge (pg. 206)`) is only valid for residuals that are normally distributed. In this simulation, the residuals are normally distributed; and the Simple and Duan adjustments are quite similar.

---

We can do another performance check using `R-Squared`, which is a measure of the percent of variance in the dependent variable (y) that is explained by the model. The higher the R-squared, the better.

```{r}
#' these functions are from `Business Data Science, Taddy, pg 72`
#' @param y the actual y values
#' @param pred the predictions; must be probabilities (0<pred<1) for binomial
deviance <- function(y, pred, family=c("gaussian","binomial")){
    family <- match.arg(family)
    if(family=="gaussian"){
        return( sum( (y-pred)^2 ) )
    }else{
        if(is.factor(y)) y <- as.numeric(y)>1
        return( -2*sum( y*log(pred) + (1-y)*log(1-pred) ) )
    }
}
#' these functions are from `Business Data Science, Taddy, pg 72`
#' returns r-squared which is a measure of the percent of variance in the dependent variable (y) that is explained by the model.
#' @param y the actual y values
#' @param pred the predictions; must be probabilities (0<pred<1) for binomial
R2 <- function(y, pred, family=c("gaussian","binomial")){
    fam <- match.arg(family)
    if(fam=="binomial"){
        if(is.factor(y)){ y <- as.numeric(y)>1 }
    }
    dev <- deviance(y, pred, family=fam)
    dev0 <- deviance(y, mean(y), family=fam)
    return(1-dev/dev0)
}
```

We can see that the adjusted predictions do have a higher `R-Squared` than the unadjsuted.

```{r}
R2(pred=test_data$`Predictions - Unadjusted`, y=test_data$simulated_y)
R2(pred=test_data$`Predictions - Adjusted - Simple`, y=test_data$simulated_y)
R2(pred=test_data$`Predictions - Adjusted - Duan`, y=test_data$simulated_y)
```

```{r include=FALSE}
rm(intercept)
rm(b_coefficient)
rm(b_coefficient)
rm(simulated_x)
rm(noise)
rm(simulated_y)
rm(simulated_data)
rm(test_data)
rm(model)
rm(log_y_predictions)
rm(predictions_long)
rm(simulated_plot)
```

# Chapter 4 - Statistcal Inference

## Confidence Interval of Proportion

```{r}
sample_size <- 1000
respond_yes <- 700

(estimate <- respond_yes / sample_size)
(standard_error <- sqrt(estimate * (1 - estimate) / sample_size))
(z_scores <- qnorm(c(0.025, 0.975)))
(confidence_interval_95 <- estimate + (z_scores * standard_error))
```

---

Compare to R's `prop.test`.

```{r}
prop.test(x=respond_yes, n=sample_size)
```

```{r include=FALSE}
rm(sample_size)
rm(respond_yes)
rm(estimate)
rm(standard_error)
rm(z_scores)
rm(confidence_interval_95)
```

# Chapter 5 - Simulation

## How many girls in 400 births?

```{r}
probability_girl <- 0.488
sample_size <- 400
# one random sample (simulated)
number_of_simulated_samples <- 1
set.seed(1)
rbinom(n = number_of_simulated_samples, size=sample_size, prob = probability_girl)
```

```{r}
number_of_simulated_samples <- 1000
set.seed(1)
simulations <- rbinom(n = number_of_simulated_samples, size=sample_size, prob = probability_girl)
simulations[1:10]
```

95% Interval

```{r}
quantile(simulations, c(0.025, 0.975))
```

```{r simulations_girls, fig.height=5, fig.width=plot_width_height_5}
hist(simulations)
```

---

Accounting for twins.

First, simulate birth types of 400 births.

```{r}
set.seed(2)
probability_girl_if_twins <- 0.495
probability_of_twins_fraternal <- 1/125  # each has a 49.5% of being a girl
probability_of_twins_identical <- 1/300  # 49.5% chance of being a pair of girls.

simulate_birth_types <- function() {
    
    sample(x=c('fraternal twin', 'identical twin', 'single born'),
           size = sample_size,
           replace = TRUE,
           prob = c(probability_of_twins_fraternal,
                    probability_of_twins_identical,
                    1 - probability_of_twins_fraternal - probability_of_twins_identical))
}
birth_type <- simulate_birth_types()
table(birth_type)
```

Simulating Number of Girls if Fraternal Twin. In this scenario, there are `2` different people, each of which has a ``r format_percent_scale(probability_girl_if_twins)` percent chance of being a girl. So the possible outcomes are `0`, `1`, `2`

- `0`: 0 girls (both boys)
- `1`: 1 girl
- `2`: 2 both

```{r simulations_fraternal_twins, fig.height=5, fig.width=plot_width_height_5}
hist(rbinom(n=10000,  # simulate this scenario 10K times
            size=2, # two different possible chances of being a girl
            prob=probability_girl_if_twins))
```

Simulating Number of Girls if Identical Twin. In this scenario, they are either both boys, or both girls. So the possible outcomes are either `0` or `2`

- `0`: 0 girls (both boys)
- `2`: both girls

In other words, we'll simulate the binary outcome of either both girls or both boys, but the former means we have to count 2 people, so we'll multiple the number of people by 2

```{r simulations_identical_twins, fig.height=5, fig.width=plot_width_height_5}
hist(2 * rbinom(n=10000,  # simulate this scenario 10K times
                size=1, # two different possible chances of being a girl
                prob=probability_girl_if_twins))
```

Now, we will do 1000 simulations of births, considering fraternal/identical twins; each simulation has a sample-size of 400.

```{r}
set.seed(1)
simulation_results <- replicate(1000, {
    birth_type <- simulate_birth_types()
    
    sample_of_girls <- map_dbl(birth_type, ~ {

        if(. == 'fraternal twin') {
            # simulate number of girls for this particular simulated birth_type
            number_of_girls <- rbinom(n=1,
                   size=2, # two different possible chances of being a girl
                   prob=probability_girl_if_twins)
            
        } else if(. == 'identical twin') {
            # simulate number of girls for this particular simulated birth_type
            number_of_girls <- 2 * rbinom(n=1,
                size=1, # two different possible chances of being a girl
                prob=probability_girl_if_twins)
            
        } else if(. == 'single born') {
            # simulate number of girls for this particular simulated birth_type
            number_of_girls <- rbinom(1, 1, probability_girl)
            
        } else {
            stop()
        }
        
        return(number_of_girls)
    })
    stopifnot(length(sample_of_girls) == sample_size)
    return (sum(sample_of_girls))
})
```

```{r}
quantile(simulation_results, c(0.025, 0.975))
```

```{r simulations_twins, fig.height=5, fig.width=plot_width_height_5}
hist(simulation_results)
```

## Simulation of continuous and mixed discrete/continuous models

```{r}
n_sims <- 1000
y1 <- rnorm(n = n_sims, mean = 3, sd = 0.5)
y2 <- exp(x = y1)
y3 <- rbinom(n = n_sims, size = 20, prob = 0.6)
y4 <- rpois(n = n_sims, lambda = 5)
```

```{r simulations_continuous, echo=FALSE, fig.height=6, fig.width=plot_width_height_6}
par(mar=c(4,3,4,3),  mgp=c(1.5,.5,0), tck=-.01)
par(mfrow=c(2,2))
hist(y1, breaks=seq(floor(min(y1)), ceiling(max(y1)), 0.2), main="1000 draws from normal dist with dist. with mean 3, sd 0.5")
hist(y2, breaks=seq(0, ceiling(max(y2)) + 5, 5),  main="1000 draws from corresponding lognormal dist.")
hist(y3, breaks=seq(-0.5, 20.5, 1), main="1000 draws from binomial dist. with 20 tries, probability 0.6")
hist(y4, breaks=seq(-0.5, max(y4) + 1, 1), main="1000 draws from Poisson dist. with mean 5")
```

## Median Absolute Deviation (`MAD SD`)

```{r simulations_mad_sd, fig.height=6, fig.width=plot_width_height_6}
set.seed(1)
z <- rnorm(10000, 5, 2)
hist(z)
```

```{r}
rnd3 <- function(.x) {
    round(.x, 3)
}
glue::glue("mean = { rnd3(mean(z)) }, sd = { rnd3(sd(z)) }, median = { rnd3(median(z)) }, mad sd = { rnd3(mad(z)) }")
```

```{r}
constant <- 1.4826  # 1.483 defined R&OS pg 73; but function uses more specific number
constant * median(abs(z - median(z)))
mad(z)
```

---

The above is a single sample if `10,000` observations.

The `standard error` (i.e. expected **standard deviation** of the **distribution** of **sample means** (if we were going to run this simulation many times (or draw a random sample from the population many times))) is:

```{r}
sd(z) / sqrt(length(z))
```

We can also simulate this.

First, lets run a `1,000` simulations of drawing a sample of `10,000` observations. Each time we'll take the mean of those 10,000 observations. So we'll have a distribution of (1000) sample means.

The standard deviation of the distribution of sample means is:

```{r}
set.seed(1)
simulations <- replicate(1000, mean(rnorm(10000, 5, 2)))
sd(simulations)
```

Which is fairly close to our calculated standard error.

```{r}
quantile(simulations, c(0.025, 0.975))
```

```{r simulations_sample_means, fig.height=6, fig.width=plot_width_height_6}
hist(simulations, main = "Distribution of Sample Means")
```

```{r include=FALSE}
rm(probability_girl)
rm(sample_size)
rm(number_of_simulated_samples)
rm(simulations)
rm(probability_girl_if_twins)
rm(probability_of_twins_fraternal)
rm(probability_of_twins_identical)
rm(simulate_birth_types)
rm(birth_type)
rm(simulation_results)
rm(n_sims)
rm(y1)
rm(y2)
rm(y3)
rm(y4)
rm(z)
rm(constant)
```

# Chapter 9 - Prediction and Bayesian Inference

```{r}
head(hibbs)
```

```{r}
# refresh = 0 supresses the default Stan sampling progress output
set.seed(1)
model <- stan_glm(vote ~ growth, data=hibbs, refresh=0)
print(model)
```

> These numbers are summaries of a matrix of simulations representing different possible values of the parameters (intercept/slope/residual-standard-deviation). `pg. 113`

> We have a set of `posterior simulations` rather than a single point estimate because we have uncertainty about these parameters. `pg. 113`

```{r}
simulations <- as.matrix(model)
head(simulations)
```

The above matrix has columns for each model parameter (plus a column for `sigma` which is the `residual standard deviation`). Each row is a simulated outcome.

In the first simulation, the `Intercept` is ``r simulations[1, '(Intercept)']``, the `growth` coefficient is ``r simulations[1, 'growth']``, and the `residual standard deviation` is ``r simulations[1, 'sigma']``.

```{r}
model %>% tidy()
```

```{r}
median(simulations[, '(Intercept)'])
mad(simulations[, '(Intercept)'])
```

```{r}
median(simulations[, 'growth'])
mad(simulations[, 'growth'])
```

```{r}
median(simulations[, 'sigma'])
mad(simulations[, 'sigma'])
```

# pg. 104 describes how sigma (residual standard deviation) is calculated.

They mention that 

> a natural way to estimate sigma would be to simply take the standard deviation of the residuals [...], but this would slightly underestimate sigma because of overfitting... 

You can see that if we take the standard deviation of the residuals, we get a slightly lower number than the sigma that is given from the model.

```{r}
sd(residuals(model))
```

Direct calculation:

```{r}
num_coefficients <- length(coefficients(model))
sqrt(sum(residuals(model)^2) / (length(residuals(model)) - num_coefficients))
```

Although this is still lower, I assume because there is some simulation happening rather than the direct calculation.

The residuals are equivalent to `actual - fitted`:

```{r}
all(model$y - fitted(model) == residuals(model))
```

---

```{r chapter_9_coefficient_joy_plot, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
simulations %>%
    as.data.frame() %>%
    pivot_longer_all() %>%
    #mutate(stat.sig = factor(stat.sig, levels=c("TRUE", "FALSE"))) %>%
    #mutate(name = fct_reorder(name, coef_median)) %>%
    ggplot(aes(x=value, y=name)) +
    geom_density_ridges(alpha=0.5) +
    geom_vline(xintercept = 0, color='red')
```

---

```{r chapter_9_coefficient_tiefighter_plot, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
simulations %>%
    as.data.frame() %>%
    pivot_longer_all() %>%
    group_by(name) %>%
    summarise(coef_median = median(value),
              coef_mad = mad(value),
              conf.low = coef_median - 2*coef_mad,
              conf.high = coef_median + 2*coef_mad,
              # this is sudo p.value
              # it's the percent of times the simulated values are below zero (if coef_median is greater than zero), or vice-versa
              p.value=ifelse(coef_median > 0, sum(value <= 0) / n(), sum(value >= 0) / n()),
              stat.sig = p.value <= 0.05) %>%
    ungroup() %>%
    mutate(stat.sig = factor(stat.sig, levels=c("TRUE", "FALSE"))) %>%
    mutate(name = fct_reorder(name, coef_median)) %>%
    ggplot(aes(x=coef_median, y=name, color=stat.sig)) +
    geom_point() +
    geom_vline(xintercept = 0, color='red') +
    geom_errorbar(aes(xmin=conf.low, xmax=conf.high)) +
    scale_color_manual(values=c("#37B57F", "#DF585C"))
```

## Point Prediction

```{r}
new_data <- data.frame(growth=2.0)
predict(object = model, newdata = new_data)
```

## Linear Predictor with Uncertainty 

> We can use `posterior_linpred` to get uncertainty in the value of the fitted regression line. (pg. 116)

```{r}
y_linpredict <- posterior_linpred(object=model, newdata = new_data)
head(y_linpredict)
```

> Equivalently we can use the function `posterior_epred`, which returns the expected prediction for a new data point. For linear regression, the expected value is the same as the linear predcitor, but as we dicuss in Chapter 13, these two quantities differ for nonlinear models. (pg 116)

```{r}
head(posterior_epred(object=model, newdata = new_data))
```

To understand what these predictions are, we could have calculated them by hand using the simulations output.

```{r}
simulations %>%
    as.data.frame() %>%
    select(-sigma) %>%
    mutate(prediction = `(Intercept)` + growth * 2.0) %>%
    head()
```

```{r}
simulate_election_results_linpred_conf_int <- function(.growth) {

    predictions <- posterior_linpred(object=model, newdata = data.frame(growth=.growth))
    confidence_intervals <- quantile(predictions, c(0.025, 0.975))
    data.frame(prediction=median(predictions),
               conf.low=unname(confidence_intervals[1]),
               conf.high=unname(confidence_intervals[2]))
}
simulate_election_results_linpred_conf_int(2)
```

```{r chapter_9_posterior_linpred, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
data.frame(growth = -6:6) %>%
    mutate(results = map(growth, ~simulate_election_results_linpred_conf_int(.growth=.))) %>%
    unnest(results) %>%
    ggplot(aes(x=growth, y=prediction)) +
    geom_point() +
    geom_errorbar(aes(ymin=conf.low, ymax=conf.high)) +
    scale_x_continuous(breaks = pretty_breaks(20)) +
    scale_y_continuous(breaks = pretty_breaks(10)) +
    labs(title="Uncertainty in the value of the fitted regression LINE (not the prediction)",
         subtitle="NOTE: We can use this type of prediction to make statements about uncertainty in the population.")
```

We can use this type of prediction to make statements about uncertainty in the population. In order to make display uncertainty for individuals, we need to use `posterior_predict`, described next.

## Predictive Distribution

> We can construct a vector representing **predictive uncertainty** in a single election. 

Note the confidence intervals are being simulated so the numbers/graph will change each time unless a seed is set.

```{r}
new_data
```

```{r}
set.seed(1)
y_pred <- posterior_predict(object = model, newdata = new_data)
head(y_pred)
```

If we were to do this by hand, we do the same as before, but add the error term to the computation (which assumes normal distribution).

Therefore, in generating the values of `prediction` (i.e. the prediction distribution for the single prediction we are concerned with in this example) `posterior_predict` is taking into account both the uncertainty of the coefficients (because each simulation uses the simulated coefficients) but also the uncertainty in the prediction (because each simulation is simulating the error from the residual standard error, sigma).

```{r}
set.seed(1)
simulations %>%
    as.data.frame() %>%
    mutate(error = rnorm(nrow(simulations), 0, sigma)) %>%
    mutate(prediction = `(Intercept)` + growth*2.0 + error) %>%  # 2.0 for the growth we are using for our prediction
    head()
```

---

```{r}
simulate_election_results_predict_conf_int <- function(.growth) {

    predictions <- posterior_predict(object=model, newdata = data.frame(growth=.growth))
    confidence_intervals_68 <- quantile(predictions, c(0.16, 1-0.16))
    confidence_intervals_95 <- quantile(predictions, c(0.025, 0.975))
    data.frame(prediction=median(predictions),
               conf.low.68=unname(confidence_intervals_68[1]),
               conf.high.68=unname(confidence_intervals_68[2]),
               conf.low.95=unname(confidence_intervals_95[1]),
               conf.high.95=unname(confidence_intervals_95[2]),
               probability_50_plus = mean(predictions > 50)) %>%
        mutate(stat.sig = !between(50, conf.low.95, conf.high.95))
}
set.seed(1)
simulate_election_results_predict_conf_int(2)
```

```{r}
set.seed(1)
results <- data.frame(growth = -6:6) %>%
    mutate(results = map(growth, ~simulate_election_results_predict_conf_int(.growth=.))) %>%
    unnest(results) 
results
```

Note the confidence intervals are being simulated so the numbers/graph will change each time unless a seed is set.

---

```{r chapter_9_posterior_predict, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
results %>%
    ggplot(aes(x=growth, y=prediction, color=stat.sig)) +
    geom_hline(yintercept = 50, color="#DF585C") +
    geom_point() +
    geom_errorbar(aes(ymin=conf.low.68, ymax=conf.high.68), width=0.5) +
    geom_errorbar(aes(ymin=conf.low.95, ymax=conf.high.95), width=0.5, alpha=0.5) +
    scale_x_continuous(breaks = pretty_breaks(10), labels = function(.x)paste0(.x, "%")) +
    scale_y_continuous(breaks = pretty_breaks(10), labels = function(.x)paste0(.x, "%")) +
    scale_color_manual(values=c("#7A7A7A", "#7AA9CF")) +
    labs(title="Prediction of Incumbent party's vote share, given various economic growth values.",
         subtitle="We can use this type of prediction to make statements about uncertainty about an individual",
         x="Economic Growth (% from previous year)",
         y="Prediction of Incumbent party's vote share.")
```

---

```{r chapter_9_posterior_predict_ribbon, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
results %>%
    ggplot(aes(x=growth, y=prediction)) +
    geom_hline(yintercept = 50, color="#DF585C") +
    geom_ribbon(aes(ymin=conf.low.68, ymax=conf.high.68), alpha=0.3) +
    geom_ribbon(aes(ymin=conf.low.95, ymax=conf.high.95), alpha=0.3) +
    geom_point(aes(color=stat.sig)) +
    #geom_point() +
    #geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=0.5) +
    scale_x_continuous(breaks = pretty_breaks(10), labels = function(.x)paste0(.x, "%")) +
    scale_y_continuous(breaks = pretty_breaks(10), labels = function(.x)paste0(.x, "%")) +
    scale_color_manual(values=c("black", "purple")) +
    labs(title="Prediction of Incumbent party's vote share, given various economic growth values.",
         subtitle="We can use this type of prediction to make statements about uncertainty about an individual",
         x="Economic Growth (% from previous year)",
         y="Prediction of Incumbent party's vote share.")
```

---

```{r chapter_9_posterior_predict_prob, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
results %>%
    mutate(growth = factor(growth)) %>%
    ggplot(aes(x=growth, y=probability_50_plus, fill=probability_50_plus)) +
    geom_col() +
    geom_text(aes(label=percent(probability_50_plus, accuracy = 0.1)), vjust=-0.3) +
    scale_y_continuous(breaks=pretty_breaks(10), labels = scales::percent_format()) +
    #scale_fill_gradient2(low = 'red', mid = 'white', high = 'green', midpoint = 0.5) +
    scale_fill_gradient2(low = '#DF585C', mid = 'white', high = '#37B57F', midpoint = 0.5) +
    labs(title="Probability of Incumbent party's vote share > 50%",
         x="Economic Growth (% from previous year)",
         y="Probability of Incumbent party's vote share > 50%")
```

---

If we were to predict on several instances, we would get a column for each input value (i.e. growth of `-2` corresponds to column `1`)

```{r}
head(posterior_predict(object=model, newdata = data.frame(growth=-2:2)))
```

```{r}
all_predictions <- posterior_predict(object=model, newdata = data.frame(growth=-6:6))
head(all_predictions, 20)
```

```{r chapter_9_posterior_predict_points, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
all_predictions <- all_predictions %>%
    as.data.frame() %>%
    pivot_longer_all(names_to='growth', values_to='prediction') %>%
    mutate(growth = as.numeric(growth) - 6 - 1)

results %>%
    ggplot(aes(x=growth, y=prediction, color=stat.sig)) +
    geom_hline(yintercept = 50, color="#DF585C") +
    #geom_point() +
    geom_jitter(data=all_predictions, aes(x=growth, y=prediction, color=NULL), width=0.1, alpha=0.1) +
    #geom_errorbar(aes(ymin=conf.low.68, ymax=conf.high.68), width=0.5) +
    geom_errorbar(aes(ymin=conf.low.95, ymax=conf.high.95), width=0.7) +
    scale_x_continuous(breaks = pretty_breaks(10), labels = function(.x)paste0(.x, "%")) +
    scale_y_continuous(breaks = pretty_breaks(20), labels = function(.x)paste0(.x, "%")) +
    scale_color_manual(values=c("#7A7A7A", "#7AA9CF")) +
    labs(title="Prediction of Incumbent party's vote share, given various economic growth values.",
         subtitle="We can use this type of prediction to make statements about uncertainty about an individual",
         x="Economic Growth (% from previous year)",
         y="Prediction of Incumbent party's vote share.")
```

```{r include=FALSE}
rm(model)
rm(simulations)
rm(new_data)
rm(y_linpredict)
rm(simulate_election_results_linpred_conf_int)
rm(simulate_election_results_predict_conf_int)
rm(y_pred)
rm(results)
rm(all_predictions)
rm(num_coefficients)
```

## 9.3 Prior Information and Bayesian Synthesis

Prior estimate and standard deviation

```{r}
bayesian_updating <- function(.new_vote_count, .new_sample_size) {
 
    prior_estimate <- 0.524
    prior_estimate_sd <- 0.041
    
    (data_estimate <- .new_vote_count / .new_sample_size)
    (data_estimate_sd <- sqrt((.new_vote_count / .new_sample_size) * (1 - (.new_vote_count / .new_sample_size)) / .new_sample_size))
    
    (bayes_estimate <- ((prior_estimate / (prior_estimate_sd^2)) + (data_estimate / (data_estimate_sd^2))) / ((1 / (prior_estimate_sd^2)) + (1 / (data_estimate_sd^2))))
    (bayes_estimate_sd <- 1 / sqrt((1 / (prior_estimate_sd^2)) + (1 / (data_estimate_sd^2))))
    
    print(glue::glue("Prior: { prior_estimate }; Likelihood: { data_estimate }; Posterior: { round(bayes_estimate, 4) }; standard deviation: { round(bayes_estimate_sd, 4) }"))
    
    x <- seq(0, 1, length.out = 1000)
    data.frame(type="Prior Distribution", x=x, value=dnorm(x, prior_estimate, prior_estimate_sd)) %>%
        bind_rows(data.frame(type="Likelihood / Data Distribution", x=x, value=dnorm(x, data_estimate, data_estimate_sd))) %>%
        bind_rows(data.frame(type="Posterior", x=x, value=dnorm(x, bayes_estimate, bayes_estimate_sd))) %>%
        mutate(type = factor(type, levels=c('Prior Distribution', 'Likelihood / Data Distribution', 'Posterior'))) %>%
        ggplot(aes(x=x, y=value, color=type)) +
        geom_line() +
        coord_cartesian(xlim=c(0.4, 0.65)) +
        scale_x_continuous(breaks = pretty_breaks(10), labels = scales::percent_format(accuracy = 1)) +
        theme(axis.title.y=element_blank(),
              axis.text.y=element_blank(),
              axis.ticks.y=element_blank()) +
        labs(title='Bayesian Updating',
             x='% Voting Democrat')   
}
```

```{r chapter_9_bayesian_updating_1, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
bayesian_updating(.new_vote_count = 19, .new_sample_size = 40)
```

```{r chapter_9_bayesian_updating_2, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
bayesian_updating(.new_vote_count = 95, .new_sample_size = 200)
```

```{r chapter_9_bayesian_updating_3, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
bayesian_updating(.new_vote_count = 190, .new_sample_size = 400)
```

```{r include=FALSE}
rm(bayesian_updating)
```

## 9.5 Uniform, weakly informative, and informative priors in regression

### Uniform prior distribution

> We can run `stan_glm` with uniform prior densities for the scale parameter using the `NULL` option. (pg. 123)

```{r}
uniform_model <- stan_glm(vote ~ growth, data=hibbs,
                          refresh = 0,  # refresh = 0 supresses the default Stan sampling progress output
                          prior_intercept = NULL,
                          prior = NULL,
                          prior_aux = NULL)
print(uniform_model)
```

Compare to `lm`; close but not the same.

```{r}
lm_model <- lm(vote ~ growth, data=hibbs)

coef(lm_model)
coef(uniform_model)
```

> These numbers are summaries of a matrix of simulations representing different possible values of the parameters (intercept/slope/residual-standard-deviation). `pg. 113`

> We have a set of `posterior simulations` rather than a single point estimate because we have uncertainty about these parameters. `pg. 113`

```{r}
uniform_coef_simulations <- as.matrix(uniform_model)
head(uniform_coef_simulations)
```

```{r chapter_9_uniform_prior_simulations, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
uniform_coef_simulations %>%
    as.data.frame() %>%
    ggplot(aes(x=`(Intercept)`, y=growth)) +
    geom_point(alpha = 0.5) +
    geom_point(x=coef(uniform_model)['(Intercept)'], y=coef(uniform_model)['growth'], size=2, color='red') +
    scale_x_continuous(breaks = pretty_breaks(10)) +
    scale_y_continuous(breaks = pretty_breaks(10)) +
    labs(title = "Uniform Prior - 4000 posterior draws of the Intercept and growth.",
         subtitle = "(Posterior simulations of the coefficients); Red dot is point of model coefficients")
```

```{r chapter_9_uniform_prior_coefficients, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
uniform_coef_simulations %>%
    as.data.frame() %>%
    pivot_longer_all() %>%
    #mutate(stat.sig = factor(stat.sig, levels=c("TRUE", "FALSE"))) %>%
    #mutate(name = fct_reorder(name, coef_median)) %>%
    ggplot(aes(x=value, y=name)) +
    geom_density_ridges(alpha=0.5) +
    geom_vline(xintercept = 0, color='red')
```


```{r}
# Note that if we don't set.seed the predictions will change because they are being simulated
simulate_election_results_predict_conf_int <- function(.model, .growth) {

    predictions <- posterior_predict(object=.model, newdata = data.frame(growth=.growth))
    confidence_intervals_68 <- quantile(predictions, c(0.16, 1-0.16))
    confidence_intervals_95 <- quantile(predictions, c(0.025, 0.975))
    data.frame(prediction=median(predictions),
               conf.low.68=unname(confidence_intervals_68[1]),
               conf.high.68=unname(confidence_intervals_68[2]),
               conf.low.95=unname(confidence_intervals_95[1]),
               conf.high.95=unname(confidence_intervals_95[2]),
               probability_50_plus = mean(predictions > 50)) %>%
        mutate(stat.sig = !between(50, conf.low.95, conf.high.95))
}
set.seed(1)
simulate_election_results_predict_conf_int(.model=uniform_model, .growth=2)
```

```{r}
set.seed(1)
uniform_prediction_summary <- data.frame(growth = -6:6) %>%
    mutate(results = map(growth, ~simulate_election_results_predict_conf_int(.model=uniform_model, .growth=.))) %>%
    unnest(results) 
head(uniform_prediction_summary)
```

```{r chapter_9_uniform_prior_predictions, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
uniform_prediction_summary %>%
    ggplot(aes(x=growth, y=prediction)) +
    geom_hline(yintercept = 50, color="#DF585C") +
    #geom_point() +
    #geom_jitter(data=all_predictions, aes(x=growth, y=prediction, color=NULL), width=0.1, alpha=0.1) +
    #geom_errorbar(aes(ymin=conf.low.68, ymax=conf.high.68), width=0.5) +
    geom_errorbar(aes(ymin=conf.low.95, ymax=conf.high.95), width=0.7) +
    scale_x_continuous(breaks = pretty_breaks(10), labels = function(.x)paste0(.x, "%")) +
    scale_y_continuous(breaks = pretty_breaks(20), labels = function(.x)paste0(.x, "%")) +
    scale_color_manual(values=c("#7A7A7A", "#7AA9CF")) +
    labs(title="Prediction of Incumbent party's vote share, given various economic growth values.",
         subtitle="Using Uniform Prior",
         x="Economic Growth (% from previous year)",
         y="Prediction of Incumbent party's vote share.")
```


### Default Prior Distribution

> The default prior distribution can be specified implicitly (using the defaults): (pg. 124)

```{r}
set.seed(1)
default_prior_model <- stan_glm(vote ~ growth, data=hibbs, refresh=0)
print(default_prior_model)
```

> or explicitly, below. The particular choice of `2.5` as a scaling factor is somewhat arbitrary, chosen to provide some stability in estimation while have little influence on the coefficient estimate when data are even moderately informative.

```{r}
sd_x <- sd(hibbs$growth)
sd_y <- sd(hibbs$vote)
mean_y <- mean(hibbs$vote)


growth_prior_mean <- 0
growth_prior_sd <- 2.5 * sd_y / sd_x

set.seed(1)
default_prior_explicit_model <- stan_glm(vote ~ growth, data=hibbs,
                                         refresh = 0,  # refresh = 0 supresses the default Stan sampling progress output
                                         prior = rstanarm::normal(location = growth_prior_mean, scale = growth_prior_sd),
                                         prior_intercept = rstanarm::normal(location = mean_y, scale = 2.5 * sd_y),
                                         prior_aux = rstanarm::exponential(1 / sd_y))
print(default_prior_explicit_model)
```

> These two models are the same (need to set.seed) with the only difference being that for the latter we specified the priors directly.

```{r}
coef(default_prior_model)
coef(default_prior_explicit_model)
```

Not much difference in this case between the `uniform` and the `default`:

```{r}
coef(default_prior_model)
coef(uniform_model)
```

```{r chapter_9_growth_coefficient_bayesian, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
growth_estimate_default_prior <- default_prior_model %>%
    tidy() %>%
    filter(term == 'growth')

growth_estimate_uniform_prior <- uniform_model %>%
    tidy() %>%
    filter(term == 'growth')

x <- seq(-20, 20, length.out = 1000)
data.frame(type="Prior Distribution", x=x, value=dnorm(x, growth_prior_mean, growth_prior_sd)) %>%
    bind_rows(data.frame(type="Uniform Prior Posterior", x=x, value=dnorm(x, growth_estimate_uniform_prior$estimate, growth_estimate_uniform_prior$std.error))) %>%
    bind_rows(data.frame(type="Default Prior Posterior", x=x, value=dnorm(x, growth_estimate_default_prior$estimate, growth_estimate_default_prior$std.error))) %>%
    mutate(type = factor(type, levels=c('Prior Distribution', 'Uniform Prior Posterior', 'Default Prior Posterior'))) %>%
    ggplot(aes(x=x, y=value, color=type)) +
    geom_line() +
    #coord_cartesian(xlim=c(0.4, 0.65)) +
    #scale_x_continuous(breaks = pretty_breaks(10), labels = scales::percent_format(accuracy = 1)) +
    theme(axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) +
    labs(title='`growth` coefficient - bayesian updating',
         x='`growth` Coefficient Distribution')   
```

```{r chapter_9_model_bayesian, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hibbs %>%
    ggplot(aes(x=growth, y=vote)) +
    geom_hline(yintercept = 50, color="red", linetype="dashed") +
    geom_text(aes(label=year)) +
    #geom_smooth(method='lm') +
    geom_abline(intercept = coef(uniform_model)['(Intercept)'], slope = coef(uniform_model)['growth'], color='blue') +
    geom_abline(intercept = coef(default_prior_model)['(Intercept)'], slope = coef(default_prior_model)['growth'], color='orange') +
    scale_x_continuous(labels=function(.x)paste0(.x, '%')) +
    scale_y_continuous(labels=function(.x)paste0(.x, '%')) +
    labs(title="Forecasting the election from the economy",
         y="Incuming party's vote share",
         x="Average recent growth in personal",
         caption='Figure 1.1')
```

### Weekly informative prior distribution based on subject-matter knowledge

```{r include=FALSE}
rm(uniform_model)
rm(lm_model)
rm(uniform_coef_simulations)
rm(uniform_prediction_summary)
rm(default_prior_model)
rm(default_prior_explicit_model)
rm(sd_x)
rm(sd_y)
rm(mean_y)
rm(x)
rm(growth_prior_mean)
rm(growth_prior_sd)
rm(growth_estimate_default_prior)
rm(growth_estimate_uniform_prior)
```

# Chapter 10

## Example: uncertainty in predicting congressional elections

Example from pg 140

```{r}
head(congress)
```

`dem_vote_share` is the democratic share of the two-party vote in each district.

Each row is a single district.

Note that the uncontested elections (i.e. only 1 person running) have original vote shares of `0` or `1` (i.e. dems got 0% or 100% of the vote in the single-person race), and those values were changed to `0.25` and `0.75`. These are the approximate proportion of votes received by the Democratic candidate had the election actually been contested. You will see a cross of values at the `0.25` and `0.75` (more visible on the latter) on the x & y axis.

```{r}
create_election_results <- function(.current_dem_vote_share,
                                    .prev_dem_vote_share,
                                    .incumbency) {

    election_results <- data.frame(dem_vote_share=.current_dem_vote_share,
                                   prev_dem_vote_share=.prev_dem_vote_share,
                                   incumbency=.incumbency) %>%
        mutate(incumbency = case_when(
                    incumbency == -1 ~ "Rep running reelection",
                    incumbency == 0 ~ "Open", # neither of the two candidates was occupying the seat at the time.
                    incumbency == 1 ~ "Dem running reelection",
                    TRUE ~ 'error'),
               incumbency = factor(incumbency, levels=c("Dem running reelection", "Rep running reelection", "Open")))

    return (election_results)    
}
election_results_88 <- create_election_results(congress$v88_adj, congress$v86_adj, congress$inc88)
head(election_results_88)
```

```{r chapter_10_deom_vote_share_data, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
election_results_88 %>%
    ggplot(aes(x=prev_dem_vote_share, y=dem_vote_share, color=incumbency)) +
    geom_abline(intercept = 0, slope = 1) +
    geom_point(alpha=0.7) +
    geom_smooth(method='lm', se=FALSE) +
    geom_smooth(aes(color=NULL), method='lm', se=FALSE, color='#7A7A7A') +
    scale_x_continuous(breaks=pretty_breaks(10), labels=scales::percent_format(1)) +
    scale_y_continuous(breaks=pretty_breaks(10), labels=scales::percent_format(1)) +
    scale_color_manual(values=c("#0015BC", "#FF0000", "#1AAF54")) +
    coord_cartesian(xlim=c(0.1, 0.9), ylim=c(0.1, 0.9)) +
    labs(title="`88 Election results showing Democratic Vote Share vs Previous Democratic Vote Share",
         subtitle = 'Black line represents point at which previous and current Dem vote share is unchanged.\nGray line represents single regression if incumbency is not in model.',
         y="Democratic Vote Share",
         x="Previous Democratic Vote Share")
```

NOTE: In the graph above, the slopes of the colored regression lines are allowed to change with each incumbency group, but the slopes are very close; which suggests there there isn't much interaction effects; i.e. a model including interaction affects would be very close to a model not including interaction effects.

```{r}
set.seed(1)
model_88_interactions <- stan_glm(dem_vote_share ~ prev_dem_vote_share*incumbency, data=election_results_88, refresh=0)
print(model_88_interactions, digits=2)
```

NOte: the interaction coefficients are all close to 0 and have a large mad_sd.

```{r}
set.seed(1)
model_88 <- stan_glm(dem_vote_share ~ prev_dem_vote_share + incumbency, data=election_results_88, refresh=0)
print(model_88, digits=2)
```

```{r}
prediction_data_90 <- create_election_results(.current_dem_vote_share = NA,
                                               .prev_dem_vote_share = election_results_88$dem_vote_share,  # same as congress$v88_adj
                                               .incumbency = congress$inc90) %>%
    select(-dem_vote_share)
head(prediction_data_90)
```

```{r}
set.seed(1)
predictions_90 <- posterior_predict(model_88, newdata=prediction_data_90)
dim(predictions_90)
```

Predictions has `4000` simulations (rows) for each of the 435 predicted values, which corresponds to the 435 congressional districts where we are predicting the democratic party's vote share.

So, each row is a single simulation. 

`predictions_90[1, ]` gives the first simulation, and has the a simulated prediction for each district.

`predictions_90[1, 2]` gives the predicted value for the second district in the first simulation.

```{r}
first_simulation <- predictions_90[1, ]
length(first_simulation)
first_simulation[1:10]
```

We can get the simulated predictions for the first simulation by taking the values from the first row. THere are `435` values, each value representing the Dem vote share for that particular district.

Which means that, for this simulation, we can get the number of Democratic wins (i.e. number of districts where dems had greater than 50% of the vote share) by counting all of the instances where the prediction is greater than `0.5`.

```{r}
sum(first_simulation > 0.5)
```

We can get the outcome of each simulation by counting the number of Dem vote shares that are greater than 50%.

```{r}
dem_wins_per_sim <- rowSums(predictions_90 > 0.5)
dem_wins_per_sim[1:5]
```

For example:

- In the first simulation, democrats won in ``r dem_wins_per_sim[1]`` districts.
- In the second simulation, democrats won in ``r dem_wins_per_sim[2]`` districts.
- In the third simulation, democrats won in ``r dem_wins_per_sim[3]`` districts.
- and so on.

We can get an idea of uncertainty by plotting a histogram, or getting the prediction intervals.

```{r}
median(dem_wins_per_sim)
mad(dem_wins_per_sim)
```

```{r}
quantile(dem_wins_per_sim, c(0.025, 0.5, 0.975))
```

```{r chapter_10_deom_vote_share_data_outcome, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hist(dem_wins_per_sim)
```

To get the uncertainty for any individual district, we would look at specific columns.

To get the simulated predictions for district `100`, we look at the hundredth column. There are 4000 simulations.

```{r}
district_100_predictions <- predictions_90[, 100]
length(district_100_predictions)
```

We can get the expected outcome and uncertainty by plotting a histogram or getting the prediction intervals.

```{r}
median(district_100_predictions)
mad(district_100_predictions)
```

```{r}
quantile(district_100_predictions, c(0.025, 0.5, 0.975))
```

```{r chapter_10_deom_vote_share_data_district, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hist(district_100_predictions)
```

The probability that the democrat will win district 100 is:

```{r}
mean(district_100_predictions > 0.5)
```

We could have gotten the number od district wins first by getting the column medians (which is the expected outcome (dem vote share) for each each district), and then adding up the number greater than 50%.

```{r}
district_medians <- apply(predictions_90, 2, median)
length(district_medians)
district_medians[100]
```

The result matches our median for district 100 above.

Now we can count the number of districts above 50%

```{r}
sum(district_medians > 0.5)
```

Note this gives 260 rather than 261.

> NOTE: The model as constructed not allow for national partisan swings of the sort that happen from election to election. To put it another way, the intercept of the model predicting 1988 from 1986 is not the same as the intercept for predicting 1990 from 1988, and it would be a mistake to directly apply the first model to make predictions from the second dataset. (pg. 144)

```{r include=FALSE}
rm(election_results_88)
rm(model_88)
rm(model_88_interactions)
rm(prediction_data_90)
rm(predictions_90)
rm(first_simulation)
rm(dem_wins_per_sim)
rm(district_100_predictions)
rm(district_medians)
```

## Fitting the same model to many datasets

Example from pg. 148

```{r}
nes_local <- nes_data %>% 
    select(year, partyid7, real_ideo, race_adj, age_discrete, educ1, female, income) %>%
    na.omit()
head(nes_local)
```

> This approach is so easy and powerful but yet is rarely used as a data-analytic tool. We suspect that one reason for its rarity of use is that, once one acknowledges the time-series structure of a dataset, it is natural to want to take the next step and model that directly. In practice, however, there is a board range of problems for which a cross-sectional analysis is informative, and for which a time-series display is appropriate to give a sense of trends. 

As we see below, modeling the time-series directly would not have highlighted some of these changes in coefficient values over time.

```{r}
regress_year <- function (data, yr) {
  this_year <- data[data$year==yr,]
  set.seed(1)
  fit <- stan_glm(partyid7 ~ real_ideo + race_adj + factor(age_discrete) +
                      educ1 + female + income,
                  data=this_year, warmup = 500, iter = 1500, refresh = 0,
                  save_warmup = FALSE, cores = 1, open_progress = FALSE)
  coefs <- cbind(coef(fit),se(fit))
  return (coefs)
}
regress_year(nes_local, 1972)
```

```{r}
summary <- array (NA, c(9,2,8)) # 3d matrix
for (yr in seq(1972,2000,4)){
  i <- (yr-1968)/4
  summary[,,i] <- regress_year(nes_local, yr)
}
summary[,,1] # first year's results; same values as above (regress_year(nes_local, 1972))
```

```{r chapter_10_many_datasets, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
yrs <- seq(1972,2000,4)
coef_names <- c("Intercept", "Ideology", "Black", "Age_30_44", "Age_45_64", "Age_65_up", "Education", "Female", "Income")
par(mfrow=c(2,5), mar=c(2,3,2,2), tck=-.02, mgp=c(2,.7,0))
for (k in 1:9){
  plot(range(yrs), range(0,summary[k,1,]+.67*summary[k,2,],summary[k,1,]-.67*summary[k,2,]), type="n", xlab="", ylab="Coefficient", main=coef_names[k], mgp=c(1.2,.2,0), cex.main=1, cex.axis=1, cex.lab=1, tcl=-.1, bty="l", xaxt="n")
  axis(1, c(1972,1986,2000), mgp=c(.5,.3,0))
  abline(0,0, lty=2)
  points(yrs, summary[k,1,], pch=20)
  segments(yrs, summary[k,1,]-.67*summary[k,2,], yrs, summary[k,1,]+.67*summary[k,2,])
}
```

```{r include=FALSE}
rm(nes_local)
rm(regress_year)
rm(i)
rm(summary)
rm(yrs)
rm(yr)
rm(coef_names)
rm(k)
```

# Chapter 11

## Comparing data to replications from a fitted model

The example below uses the concepts from `pg. 163` but uses the `election_results_88` dataset from chapter 9.

```{r}
election_results_88 <- create_election_results(congress$v88_adj, congress$v86_adj, congress$inc88)
head(election_results_88)
```

```{r}
set.seed(1)
model_88 <- stan_glm(dem_vote_share ~ prev_dem_vote_share + incumbency, data=election_results_88, refresh=0)
fitted_88 <- posterior_predict(model_88)
dim(fitted_88)
```

`fitted_88` contains `4000` simulations (i.e. 'fitted values') of outcome variable (democratic vote share in the `435` districts)

`100` rows (i.e. simulations) were taken from the 4000 simulations in `fitted_88`. 

```{r}
density_check <- election_results_88 %>% 
    select(incumbency, dem_vote_share) %>%
    mutate(district = row_number()) %>%
    bind_cols(
        fitted_88[1:100,] %>% # 100 simulations (100 rows, 435 columns for 435 outcomes (districts))
            t() %>%  # transform so that each column is a simulation and each row 
            as.data.frame()
    ) %>%
    pivot_longer(-c(incumbency, district), names_to='simulation_index') %>%
    mutate(simulation_index = ifelse(simulation_index == 'dem_vote_share', 'Actual Dem Vote Share', str_replace(simulation_index, 'V', 'Sim ')))

head(density_check)
```

```{r chapter_10_ppc_dens_overlay, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
bayesplot::ppc_dens_overlay(election_results_88$dem_vote_share, fitted_88[1:100,])
```

---

Or we can manually create the graph, so that we can group predictions by variable. Each thin line in the graph below represents a single row of `fitted_88`. 

```{r chapter_10_dem_fit_check, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
density_check %>%
    filter(simulation_index != 'Actual Dem Vote Share') %>%
    #ggplot(aes(x=value, group=simulation_index)) + 
    ggplot(aes(x=value, color=incumbency, group=interaction(incumbency, simulation_index))) + 
    geom_density(size=0.1) +
    geom_density(data=density_check %>% filter(simulation_index == 'Actual Dem Vote Share'),
                 size=3) +
    scale_color_manual(values=c("#0015BC", "#FF0000", "#1AAF54")) +
    labs(title="Dem Vote Share Density - by Incumbency",
         subtitle = "Thick lines are actual densities from the outcome variable;\nthin lines are simulated values from the fitted data",
         x="Dem Vote Share")
```

```{r include=FALSE}
rm(create_election_results)
rm(election_results_88)
rm(model_88)
rm(fitted_88)
rm(density_check)
```

## 11.6 Residual Standard Deviation and Explained R^2

```{r}
set.seed(1)
model <- stan_glm(kid_score ~ mom_hs + mom_iq, data=kid_iq, refresh=0)
print(model)
```

### `loo()` & `Posterior Predictive Log Score`

```{r}
print(loo(model))
```

See pg 176 for description of metrics.

### `loo_compare`

```{r}
loo_compare(loo(stan_glm(kid_score ~ mom_hs + mom_iq, data=kid_iq, refresh=0)),
            loo(stan_glm(kid_score ~ mom_hs*mom_iq, data=kid_iq, refresh=0)))
```

Seems like I get positive 3.6 elpd_diff while book gets negative, but authors code has similar numbers to mine.

Different output between book and code makes it a little confusing, need to dig into explanation on pg 178.

See pg 178 for description

---

### loo `R^2` and other metrics

```{r}
R2(y = model$y, pred = model$fitted.values)
```

Bayesian R2

```{r}
length(bayes_R2(model))
quantile(bayes_R2(model), c(0.025, 0.5, 0.975))
```

Just for fun, Mean Absolute Error and Root Mean Squared Error

```{r}
mae <- function(.actual, .fitted) {
    .residuals <- .actual - .fitted
    mean(abs(.residuals))
}
mae(model$y, model$fitted.values)
```

```{r}
rmse = function(.actual, .fitted){
    .residuals <- .actual - .fitted
    sqrt(mean((.residuals)^2))
}
rmse(model$y, model$fitted.values)
```

## 11.8 Cross Validation

> `loo_predict()` computes mean of LOO predictive distribution.

```{r}
loo_fitted_values <- function(.model) {
    set.seed(1)
    loo_predict(.model)$value
}
loo_residuals <- function(.model) {
    .model$y - loo_fitted_values(.model)
}
kid_score_loo_residuals <- loo_residuals(model)
kid_score_residuals <- residuals(model)

length(kid_score_loo_residuals) == length(kid_score_residuals)
```

```{r chapter_11_in_sample_vs_loo, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
kid_iq_residuals <-  data.frame(kid_score=kid_iq$kid_score,
                                   `LOO Residuals`=kid_score_loo_residuals,
                                   `In Sample Residuals`=kid_score_residuals,
                                   check.names = FALSE)

kid_iq_residuals %>%
    pivot_longer(-kid_score) %>%
    ggplot(aes(x=kid_score, y=value, color=name)) +
    geom_hline(yintercept = 0) +
    geom_point() +
    geom_segment(data=kid_iq_residuals, aes(xend=kid_score, y=`In Sample Residuals`, yend=`LOO Residuals`, color=NULL)) +
    scale_color_manual(values=c("#DF585C", "#37B57F")) +
    coord_cartesian(xlim = c(75, 100), ylim = c(-3, 3)) +
    labs(title="In Sample vs LOO Residuals",
         subtitle="Graph zoomed in to see differences.",
         y="Residual Values")
```

---

In-Sample R2:

```{r}
R2(y = model$y, pred = model$fitted.values)
```

LOO R2:

```{r}
kids_iq_loo_fitted_values <- loo_fitted_values(model)
R2(y = model$y, pred = kids_iq_loo_fitted_values)
```

---

In-Sample Bayes R2:

```{r}
kids_iq_bayes_r2 <- bayes_R2(model)
length(kids_iq_bayes_r2)
quantile(kids_iq_bayes_r2, c(0.025, 0.5, 0.975))
```

LOO Bayes R2:

```{r}
kids_iq_bayes_loo_r2 <- loo_R2(model)
length(kids_iq_bayes_loo_r2)
quantile(kids_iq_bayes_loo_r2, c(0.025, 0.5, 0.975))
```

---

In Sample Mean Absolute Error:

```{r}
mae(model$y, model$fitted.values)
```

Loo Mean Absolute Error:

```{r}
mae(model$y, kids_iq_loo_fitted_values)
```

---

In Sample Root Mean Squared Error:

```{r}
rmse(model$y, model$fitted.values)
```

Loo Root Mean Squared Error:

```{r}
rmse(model$y, kids_iq_loo_fitted_values)
```

```{r}
rm(model)
rm(kid_score_loo_residuals)
rm(kid_score_residuals)
rm(kid_iq_residuals)
rm(kids_iq_bayes_r2)
rm(kids_iq_bayes_loo_r2)
rm(kids_iq_loo_fitted_values)
```

# Chapter 12 - Transformations and regression

## Centering & Scaling

### No transformations

#### `kid_score ~ mom_hs + mom_iq`

```{r}
head(kid_iq)
```

NOTE: most of this topic can be expressed simply with `lm()` rather than `stan_glm()`, which is actually more convenient because we want to see coefficients without simulations so we know when a coefficient changes because of a transformation vs simulation.

```{r chapter_12_kids_iq_no_interactions, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
kid_iq %>%
    mutate(mom_hs = ifelse(mom_hs == 1, TRUE, FALSE)) %>%
    ggplot(aes(x=mom_iq, y=kid_score, color=mom_hs,  group=mom_hs)) +
    geom_point() +
    # from regression coefficients below
    geom_abline(slope = 0.56391, intercept = 25.73154, color="#DF585C") +
    geom_abline(slope = 0.56391, intercept = 25.73154 + 5.95012, color="#37B57F") +
    geom_smooth(aes(group=NULL, color=NULL), method='lm', formula = y ~ x, se=FALSE) +
    scale_color_manual(values=c("#DF585C", "#37B57F")) +
    #geom_smooth(data= kid_iq %>% filter(mom_hs == 0), method='lm', formula = y ~ x, se=FALSE) +
    labs(title = "Kids IQ based on if Mom went to High School & Mom's IQ")
```

```{r}
kid_iq %>%
    lm(kid_score ~ mom_hs + mom_iq, data=.) %>%
    summary()
```

(see pg 134 for additional explanations of coefficient interpretations)

- `Intercept`: "If a child had a moether with an IQ of `0` and who did not complete high school, then we would predict this child's test score to be 26. This is not a useful prediction, since no mothers have IQs of `0`.
- `mom_hs`: This model assumes `mom_hs` lines have the same slope (i.e. no interaction terms; see graph above). The coefficient is `5.95012` which means that the IQ score of the group of kids whose mom had a high-school education are predicted to be, on average, `5.95012` points above the kids whose mom did not have a high-school education, at all levels of IQ (i.e. no interaction terms, same slope).`
- `mom_iq`: the coefficient is `0.56391`, which is the slope of both the lines above and means that, on average (and holding education constant), a `1` point increase in the IQ score is associated with a `5.95012` increase in IQ score for the child. Or, from pg 134: `"When comparing two children whose mothers have the same level of education, the child whose mother is x IQ points higher is predicted to have a test score that is 6x higher, on average."`

#### `kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq`

```{r chapter_12_kids_iq_interactions, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
kid_iq %>%
    mutate(mom_hs = ifelse(mom_hs == 1, TRUE, FALSE)) %>%
    ggplot(aes(x=mom_iq, y=kid_score, color=mom_hs,  group=mom_hs)) +
    geom_point() +
    geom_smooth(method='lm', formula = y ~ x, se=FALSE) +
    geom_smooth(aes(group=NULL, color=NULL), method='lm', formula = y ~ x, se=FALSE) +
    scale_color_manual(values=c("#DF585C", "#37B57F")) +
    labs(title = "Kids IQ based on if Mom went to High School & Mom's IQ")
```

```{r}
kid_iq %>%
    lm(kid_score ~ mom_hs*mom_iq, data=.) %>%
    summary()
```

- `Intercept`: represents the predicted test scores for children whose mothers did not complete high school and had IQs of 0, not a meaningful scenaro.
- coefficient of `mom_hs` is `51.2682`, which means that this is the predicted difference in IQ score for kids whose mom went to high-school vs no high-school **when mom's IQ is `0`**, which is meaningless.
- coefficient of `mom_iq` is `0.9689` which means, for kids whos mom **did not** go to high-school, each additional IQ point that mom has increases the predicted score, on average, by `0.9689` points. **It is not the average over the general population.**
- `mom_hs:mom_iq`: represents the *difference* in the **slope** for mom_iq, comparing children with mothers who did and did not complete high school: that is, the difference between the slopes of the red and green lines in the graph above. **NOTICE that we can recreate the graph above by using `geom_abline()` instead of `geom_smooth()` and supplying the intercepts and slopes manually; for the slope of the line for children whose mom did complete high school, we add (in this case a negative number, so subtract) the interaction term. 

```{r chapter_12_kids_iq_interactions_manual, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
kid_iq %>%
    mutate(mom_hs = ifelse(mom_hs == 1, TRUE, FALSE)) %>%
    ggplot(aes(x=mom_iq, y=kid_score, color=mom_hs,  group=mom_hs)) +
    geom_point() +
    # regression line for children whose mom did not complete high school
    geom_abline(slope = 0.9689, intercept = -11.4820,
                color="#DF585C", size=1.2) +
    # regression line for children whose mom did complete high school
    geom_abline(slope = 0.9689 - 0.4843, intercept = -11.4820 + 51.2682,
                color="#37B57F", size=1.2) +
    #geom_smooth(aes(group=NULL, color=NULL), method='lm', formula = y ~ x, se=FALSE) +
    scale_color_manual(values=c("#DF585C", "#37B57F")) +
    #geom_smooth(data= kid_iq %>% filter(mom_hs == 0), method='lm', formula = y ~ x, se=FALSE) +
    labs(title = "Kids IQ based on if Mom went to High School & Mom's IQ")
```

### Centering

#### `kid_score ~ mom_hs + mom_iq`

```{r}
kid_iq %>%
    mutate(centered_mom_hs = mom_hs - mean(mom_hs),
           centered_mom_iq = mom_iq - mean(mom_iq)) %>%
    lm(kid_score ~ centered_mom_hs+centered_mom_iq, data=.) %>%
    summary()
```

Notice that, after centering, the value of the intercept is the average `kid_score` (but that's only the case if we don't have any interactions, which makes sense):

```{r}
mean(kid_iq$kid_score)
```

Also notice that centering did **not** change either of the coefficients for `mom_hs` or `mom_iq` compared with the original model with no interactions. **This too, is only in the case we do not have interactions.**

**When we do not have interaction terms, there doesn't really seem to be a benefit from centering the variables, with regards to in interpretation (other than the intercept).**

#### `kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq`

```{r}
kid_iq %>%
    mutate(centered_mom_hs = mom_hs - mean(mom_hs),
           centered_mom_iq = mom_iq - mean(mom_iq)) %>%
    lm(kid_score ~ centered_mom_hs*centered_mom_iq, data=.) %>%
    summary()
```

NOTE: now the interaction term `centered_mom_hs:centered_mom_iq` didn't change; this makes sense since the difference in slopes of the lines shouldn't have changed

> Each main effect now corresponds to a predictive difference with the other input at its average value (pg 185)

```{r}
model <- lm(kid_score ~ mom_hs*mom_iq, data=kid_iq)

average_mom_iq <- mean(kid_iq$mom_iq)
predict(model, newdata = data.frame(mom_hs=1, mom_iq = average_mom_iq)) - predict(model, newdata = data.frame(mom_hs=0, mom_iq = average_mom_iq))
```

```{r}
# in this case, since it is a boolean, the average is the percent of moms with high-school education
# i.e. 78.57% of moms have a high school education
average_mom_hs <- mean(kid_iq$mom_hs)
predict(model, newdata = data.frame(mom_hs=average_mom_hs, mom_iq = 120)) - predict(model, newdata = data.frame(mom_hs=average_mom_hs, mom_iq = 119))
predict(model, newdata = data.frame(mom_hs=average_mom_hs, mom_iq = 88)) - predict(model, newdata = data.frame(mom_hs=average_mom_hs, mom_iq = 87))
```

NOTE: it may not make sense to center `mom_hs`, it's probably more intuitive to leave as is.

### standardizing via z-score

```{r}
kid_iq %>%
    mutate(centered_scaled_mom_hs = (mom_hs - mean(mom_hs)) / sd(mom_hs),
           centered_scaled_mom_iq = (mom_iq - mean(mom_iq)) / sd(mom_iq)) %>%
    lm(kid_score ~ centered_scaled_mom_hs*centered_scaled_mom_iq, data=.) %>%
    summary()
```

### Center/Scaling using 2 standard deviations

```{r}
kid_iq %>%
    mutate(centered_scaled_mom_hs = (mom_hs - mean(mom_hs)) / (2 * sd(mom_hs)),
           centered_scaled_mom_iq = (mom_iq - mean(mom_iq)) / (2 * sd(mom_iq))) %>%
    lm(kid_score ~ centered_scaled_mom_hs*centered_scaled_mom_iq, data=.) %>%
    summary()
```

```{r include=FALSE}
rm(model)
rm(average_mom_iq)
rm(average_mom_hs)
```

## Example

```{r}
head(mesquite)
```

```{r}
SEED <- 4587
fit_2 <- stan_glm(log(weight) ~ log(diam1) + log(diam2) + log(canopy_height) + log(total_height) + log(density) + group,
                  data=mesquite, seed=SEED, refresh=0)
```

---

```{r chapter_12_mesquite_posterior_density, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
posterior_predictions <- posterior_predict(fit_2)
n_sims <- nrow(posterior_predictions)
sims_display <- sample(n_sims, 100)
ppc_dens_overlay(log(mesquite$weight), posterior_predictions[sims_display,]) +
    theme(axis.line.y = element_blank()) +
    labs(title="Density of Outcome vs. Posterior (fitted) Predictions")
```

---

```{r chapter_12_mesquite_coefficient_density, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_areas(as.matrix(fit_2), regex_pars = "^log|^gro") +
    labs(title = "Coefficient Densities")
```

---

```{r chapter_12_mesquite_coefficient_ridges, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_areas_ridges(as.matrix(fit_2), regex_pars = "^log|^gro") +
    labs(title = "Coefficient Densities")
```

---

```{r chapter_12_mesquite_coefficient_intervals, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_intervals(as.matrix(fit_2), regex_pars = "^log|^gro",
                          prob = 0.5, prob_outer = 0.9) +
    labs(title = "Coefficient Intervals")
```

---

```{r chapter_12_mesquite_joint_density, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_scatter(as.matrix(fit_2),
                        pars = c("log(canopy_height)", "log(total_height)"),
                        size = 1, alpha = 0.5) +
    geom_vline(xintercept=0) +
    geom_hline(yintercept=0) +
    labs(title="Joint Density",
         x="coef of log(canopy_height)",
         y="coef of log(total_height)")
```

---

```{r chapter_12_mesquite_coefficient_pairs, fig.height=9, fig.width=9, message=FALSE}
bayesplot::mcmc_pairs(as.matrix(fit_2), regex_pars = "^log|^gro")
```

---

Bayesian R^2

```{r chapter_12_mesquite_bayes_r2, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_dens(data.frame(bayes_R2(fit_2))) +
    xlab('Bayesian R^2')
```

```{r chapter_12_mesquite_bayes_loo_r2, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
fit_2_r2 <- data.frame(`Bayes R2`= bayes_R2(fit_2),
           `LOO R2`= loo_R2(fit_2),
           check.names = FALSE)

median_bayes_r2 <- median(fit_2_r2$`Bayes R2`)
median_loo_r2 <- median(fit_2_r2$`LOO R2`)

fit_2_r2 %>%
    pivot_longer_all() %>%
    ggplot(aes(x=value, color=name)) +
    geom_density() +
    annotate("text", median_bayes_r2, x=median_bayes_r2, y=0, label=round(median_bayes_r2, 2), color='purple', hjust=1) +
    annotate("text", median_loo_r2, x=median_loo_r2, y=0, label=round(median_loo_r2, 2), color='orange', hjust=1) +
    geom_vline(xintercept = median_bayes_r2, color='purple') +
    geom_vline(xintercept = median_loo_r2, color='orange') +
    scale_color_manual(values=c('purple', 'orange')) +
    scale_x_continuous(breaks = pretty_breaks(10)) +
    labs(title="Bayes R^2 vs. LOO R^2 Density",
         subtitle = "Vertical lines represent median R^2 values",
         x="R^2")
```

---


```{r include=FALSE}
rm(SEED)
rm(fit_2)
rm(fit_2_r2)
rm(posterior_predictions)
rm(n_sims)
rm(sims_display)
rm(median_loo_r2)
rm(median_bayes_r2)
```

## Regularized horseshoe prior

Example starting on page 206 and author's code [here](https://avehtari.github.io/ROS-Examples/Student/student.html).

### Default weak prior on original coefficients

```{r}
students[1:5, c(1:5)]
```

```{r}
SEED <- 2132
model_weak_prior <- stan_glm(math_score ~ ., data = students, seed = SEED, refresh=0)
print(model_weak_prior)
```

```{r chapter_12_students_weak_prior, fig.height=6, fig.width=6, message=FALSE}
ordered_levels <- as.matrix(model_weak_prior) %>%
    as.data.frame() %>%
    select(-`(Intercept)`, -sigma) %>%
    pivot_longer_all() %>%
    mutate(name = fct_reorder(name, value)) %>%
    pull(name) %>%
    levels()
    
bayesplot::mcmc_intervals(as.matrix(model_weak_prior),
                          pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95) +
    scale_y_discrete(limits = ordered_levels) +
    geom_vline(xintercept = 0, color='red')
```

> The plot above shows coefficient distributions without standardization of predictors; it looks liek there is a different amount of uncertainty on the relevance of the predictors. For example, it looks like `absences` has a really small reevance and high certainty.

---

### Default weak prior on scaled predictors

```{r}
scaled_students <- students %>%
    select(-math_score) %>%
    scale() %>%
    as.data.frame() %>%
    mutate(math_score = students$math_score) %>%
    select(math_score, everything())
model_weak_prior_standardized <- stan_glm(math_score ~ ., data = scaled_students, seed = SEED, refresh=0)
print(model_weak_prior_standardized)
```

```{r chapter_12_students_weak_prior_standardized, fig.height=6, fig.width=6, message=FALSE}
ordered_levels <- as.matrix(model_weak_prior_standardized) %>%
    as.data.frame() %>%
    select(-`(Intercept)`, -sigma) %>%
    pivot_longer_all() %>%
    mutate(name = fct_reorder(name, value)) %>%
    pull(name) %>%
    levels()
    
bayesplot::mcmc_intervals(as.matrix(model_weak_prior_standardized),
                          pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95) +
    scale_y_discrete(limits = ordered_levels) +
    geom_vline(xintercept = 0, color='red')
```

I actually think standardization makes the coefficients themselves less interpretable, but does give a better comparison of relevance across predictors.

---

### Compare Bayesian R^2 and LOO R^2

```{r}
round(median(bayes_R2(model_weak_prior)), 2)
round(median(bayes_R2(model_weak_prior_standardized)), 2)
```

```{r}
round(median(loo_R2(model_weak_prior)), 2)
round(median(loo_R2(model_weak_prior_standardized)), 2)
```

```{r}
(loo1 <- loo(model_weak_prior))
```

> Medians of Bayesian R2 and LOO R2 are quite different, and p_loo is approximately 26, which indicates that the model is fitting to all predictors.

---

### Fit a regression model with a weakly informative prior scaled with the number of covariates

```{r}
model_weakly_informed_prior <- stan_glm(math_score ~ ., data = scaled_students, seed = SEED,
                 prior=normal(scale=sd(scaled_students$math_score)/sqrt(26)*sqrt(0.3),
                              autoscale=FALSE),
                 refresh=0)
```

```{r}
round(median(bayes_R2(model_weakly_informed_prior)), 2)
```

```{r}
round(median(loo_R2(model_weakly_informed_prior)), 2)
```

```{r}
(loo2 <- loo(model_weakly_informed_prior))
```

```{r}
loo_compare(loo1,loo2)
```

```{r chapter_12_students_weakly_informed_prior, fig.height=6, fig.width=6, message=FALSE}
ordered_levels <- as.matrix(model_weakly_informed_prior) %>%
    as.data.frame() %>%
    select(-`(Intercept)`, -sigma) %>%
    pivot_longer_all() %>%
    mutate(name = fct_reorder(name, value)) %>%
    pull(name) %>%
    levels()

bayesplot::mcmc_intervals(as.matrix(model_weakly_informed_prior),
                          pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95) +
    scale_y_discrete(limits = ordered_levels) +
    geom_vline(xintercept = 0, color='red')
```

```{r}
p <- ncol(scaled_students) - 1
p0 <- 6
slab_scale <- sd(scaled_students$math_score)/sqrt(p0)*sqrt(0.3)
# global scale without sigma, as the scaling by sigma happens in stan_glm
global_scale <- p0 / (p - p0) / sqrt(nrow(scaled_students))
model_horseshoe_prior <- stan_glm(math_score ~ ., data = scaled_students, seed = SEED,
                                  prior=hs(global_scale=global_scale, slab_scale=slab_scale),
                                  refresh=0)
```

```{r}
round(median(bayes_R2(model_horseshoe_prior)), 2)
```

```{r}
round(median(loo_R2(model_horseshoe_prior)), 2)
```

```{r}
(loo3 <- loo(model_horseshoe_prior))
```

```{r}
loo_compare(loo2,loo3)
```

```{r chapter_14_mcmc_intervals_horseshoe_prior, fig.height=6, fig.width=6, message=FALSE}
ordered_levels <- as.matrix(model_horseshoe_prior) %>%
    as.data.frame() %>%
    select(-`(Intercept)`, -sigma) %>%
    pivot_longer_all() %>%
    mutate(name = fct_reorder(name, value)) %>%
    pull(name) %>%
    levels()

bayesplot::mcmc_intervals(as.matrix(model_horseshoe_prior),
                          pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95) +
    scale_y_discrete(limits = ordered_levels) +
    geom_vline(xintercept = 0, color='red')
```

```{r include=FALSE}
rm(SEED)
rm(model_weak_prior)
rm(model_weak_prior_standardized)
rm(ordered_levels)
rm(scaled_students)
rm(loo1)
rm(model_weakly_informed_prior)
rm(p0)
rm(p)
rm(slab_scale)
rm(global_scale)
rm(model_horseshoe_prior)
rm(loo2)
rm(loo3)
```

# Chapter 13 - Logistic Regression

## Logit & Inverse Logit distributions

```{r}
logit(0)
logit(0.5)
logit(1)
```

---

```{r chapter_13_logit_distribution, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
curve(rstanarm::logit(x), 0, 1, main="Logit Distribution")
abline(h=0, col="red", lty=2, lwd=3)
abline(v=0.5, col="red", lty=2, lwd=3)
```

---

```{r chapter_13_inv_logit_distribution, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
curve(rstanarm::invlogit(x), -6, 6, main="Inverse Logit Distribution")
abline(v=rstanarm::logit(0.5), col="red", lty=2, lwd=3)
abline(h=0.5, col="red", lty=2, lwd=3)
```

---

Now, lets map the `x` values associated with line/model for `-1.4 + 0.3x` to values `0-1`.

Above, `x=0` is the value that results at the midway-point (`0.5`) of the curve.

What is the value of `x` in equation `-1.4 + 0.3x` associated with the mid-point of our Inverse Logit Distribution?

There might be a better way, but we will cheat with `optimize`. Basically, we give it a function and a range of values to test, and it will tell us which value of `x` is associated with the smallest value (i.e. the value we are "optimizing" for). In this case, the destribution doesn't have a natural maximum or minimum that we want to optimize (e.g. like the normal distribution) so we have to modify the function to subtract `0.5` from the result from invlogit function and take the absolute value, since that will result in very small numbers of `x` that are close to the value of 0.5 on the y-axis.

```{r}
abs(1-0.5) # not close to 0.5
abs(0-0.5) # not close to 0.5
abs(0.51 - 0.5) # fairly close to 0.5
abs(0.49 - 0.5) # fairly close to 0.5
```

```{r}
xmin <- optimize(function (x) abs(rstanarm::invlogit( -1.4 + 0.33*x ) - 0.5),
                 c(0, 10), # search `x` values 0 through 10
                 tol = 0.0001)
xmin
```

```{r}
rstanarm::invlogit( -1.4 + 0.33*xmin$minimum )
```

```{r chapter_13_inv_logit_distribution_function, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
curve(rstanarm::invlogit( -1.4 + 0.33*x ), -10, 20, main="Inverse Logit Distribution")
abline(v=xmin$minimum, col="red", lty=2, lwd=3)
abline(h=0.5, col="red", lty=2, lwd=3)
```

---

## Example

### Data

```{r}
nes_1992 <- nes_data %>%
    filter(year == 1992 & !is.na(rvote) & !is.na(dvote) & (rvote==1 | dvote==1))

rownames(nes_1992) <- NULL

nes_1992 %>%
    select(rvote, dvote, age, gender, race, income)
```

### Model

```{r}
set.seed(1)
model_1 <- stan_glm(rvote ~ income, family=binomial(link="logit"), data=nes_1992, refresh=0)
print(model_1)
```

> We fitted the model as `Pr(y_i = 1) = logit^-1(-1.4 + 0.3 * income) (pg 218)

> NOTE: Unlike with linear regression, there is no `sigma` in the output. Logistic regression has no separate variance term: its uncertainty comes from its probabilitic prediction of binary outcomes. (pg 219)

### Coefficient Uncertainties

```{r}
coefficient_simulations <- as.matrix(model_1)
head(coefficient_simulations)
```

```{r chapter_12_mesquite_coefficient_density, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_areas(as.matrix(model_1)) +
                      #regex_pars = "^log|^gro") +
    labs(title = "Coefficient Densities")
```

```{r chapter_13_logistic_coef_densities, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
bayesplot::mcmc_intervals(as.matrix(model_1), 
                          #regex_pars = "^log|^gro",
                          prob = 0.75, prob_outer = 0.99) +
    labs(title = "Coefficient Intervals")
```

## `posterior_epred()`

We can get the probabilities i.e. expected outcome via `posterior_epred`:

```{r}
new_data <- data.frame(income=1:5)
population_simulations_across_income <- posterior_epred(object=model_1, newdata = new_data)
head(population_simulations_across_income)
```

`This gives the same values as `invlogit(posterior_linpred(...))` (i.e. `posterior_epred` & posterior_predict are doing the `invlogit` for us):

```{r}
head(invlogit(posterior_linpred(object=model_1, newdata = new_data)))
```

---

We can use this type of prediction (above) to make statements about uncertainty about the **population** (pg 224)

```{r}
mean(population_simulations_across_income[, 5])
sd(population_simulations_across_income[, 5])
```
 
 > According to our fitted model, the percentage of Bush supporters in the **population**  at that time, among people with income `level 5`, was probably in the range of `56%` (the same as computed using `predict()`) +- `%3` (pg 223)

---

What is the posterior probability `that Bush was more popular among people with income level 5 than among people with income level 4`? (Again, a question about the population, not about individuals.)

```{r}
mean(population_simulations_across_income[, 5] > population_simulations_across_income[, 4])
```

---

> 99% posterior distribution for the **difference** in support for Bush, comparing people in the richest to the second-richest category

```{r}
quantile(population_simulations_across_income[,5] - population_simulations_across_income[,4], c(0.005, 0.025, 0.5, 0.975, 0.995))
```

```{r}
median(population_simulations_across_income[, 5])
median(population_simulations_across_income[, 4])
```

```{r}
median(population_simulations_across_income[, 5]) - median(population_simulations_across_income[, 4])
```

---

```{r}
# the function is similar to what we used above, except using the model directly to get the probability
# and subtracting 0.5 and then taking the absolute number
# the result is that the smallest numbers will be the closes to the midpoint of the curve where the probability
# is 0.5
# rather than hard-code the equation; this finds the midpoint for any model (have to adjust the `interval` parameter)
xmin <- optimize(f=function (.x) abs(predict(object=model_1, newdata = data.frame(income=.x), type='response') - 0.5),
                 interval=c(0, 10), # search `x` values 0 through 10
                 tol = 0.0001)
xmin
```

```{r chapter_12_nes_1992_predictions, fig.height=6, fig.width=plot_width_height_6, message=FALSE}
t(apply(population_simulations_across_income, 2, function(.x) quantile(.x, c(0, 0.025, 0.25, 0.5, 0.75, 0.975, 1)))) %>%
    as.data.frame() %>%
    mutate(income=row_number()) %>%
    #pivot_longer(-income)
    ggplot(aes(x=income, y=`50%`)) +
    geom_point(size=4) +
    geom_errorbar(aes(ymin=`25%`, ymax=`75%`), width=0, size=2.5) +
    geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`), width=0, size=1) +
    geom_errorbar(aes(ymin=`0%`, ymax=`100%`), width=0.1, size=0.25) +

    geom_hline(yintercept = 0.5, color='red') +
    geom_vline(xintercept = xmin$minimum, color='red') +
    geom_line(data=data.frame(x=seq(0, 6, by=0.1)) %>%
                  mutate(y= map_dbl(x, ~rstanarm::invlogit( coef(model_1)['(Intercept)'] + coef(model_1)['income'] * .))),
              aes(x=x, y=y), color='purple') +
    scale_x_continuous(breaks = pretty_breaks(10)) +
    scale_y_continuous(breaks = pretty_breaks(10), labels = percent_format(accuracy = 1)) +
    labs(title="Population Uncertainty that Vote is Republican, given Income.",
         subtitle="Purple line is logit function for model; red lines are 50% point",
         x="Income Level",
         y="Prediction of Probability")
```

---

## `posterior_predict()`

> To get the uncertainty for a **single voter** corresponding to some **invidual data point**, we use the posterior prediction (pg 224):

Which has values of `0`, `1`, `1`, `0`, etc., corresponding to possible values of `vote` among people with `income=1`, `2`, etc.. 

```{r}
set.seed(1)
individual_simulations_across_income <- posterior_predict(object=model_1, newdata = new_data)
head(individual_simulations_across_income)
```

> We can use this type of prediction (above) to make statements about uncertainty about **individual people** (pg 224).

> Taking the average of these simulations of column 5 gives `0.56`, which is equivalent to the point prediction.

```{r}
mean(individual_simulations_across_income[, 5])
```

```{r}
colMeans(individual_simulations_across_income)
```

---

Note that we can the "same" (although not exact) numbers if we use `predict()`.

`predict()` by default gives us back the linear prediction for `B*X`. If we want the probability, we need to use `type='response'`.

> Alternatively, we could directly compute the predcition using the point esimtatewith `invlogit()`. Because the logistic transformation is nonlinear, the result from performing `invlogit` on the fitted linear predictor is not quite identical to what is obtained from `predict`, but in this case, the difference is tiny ... pg 223

```{r}
predict(object=model_1, newdata = new_data)
predict(object=model_1, newdata = new_data, type='response')
rstanarm::invlogit(predict(object=model_1, newdata = new_data))
```

---

`rowSums` should take each simulation and give the number of people who voted republican (1 for each of the 5 income levels). The result is a vector of 4000 integers.

If we take the average of that number, we should be calculating how many people, on average, vote republican of a random sample of 1 person from each income level..

```{r}
mean(rowSums(individual_simulations_across_income))
```

```{r chapter-12-misc}
hist(rowSums(individual_simulations_across_income))
```

```{r include=FALSE}
rm(xmin)
rm(nes_1992)
rm(model_1)
rm(new_data)
rm(population_simulations_across_income)
rm(individual_simulations_across_income)
rm(coefficient_simulations)
```

# Chapter 14

```{r}
head(wells)
```

---

```{r}
set.seed(2)
model <- stan_glm(switch ~ dist100*arsenic, family=binomial(link="logit"), data=wells, refresh=0)
print(model, digits = 2)
```

---

> Intercept is the estimated probabilty of switching, if the distance to the nearest safe well is `0` and the arsenic level of the current well is `0`. This is impossible and we do not try to interpret. (pg 243)

```{r}
rstanarm::invlogit(coef(model)['(Intercept)'])
```

> Instead, we can evaluate the prediction at the average values of `dist100` and `arsenic`

```{r}
rstanarm::invlogit(predict(model, newdata = data.frame(dist100=mean(wells$dist100), arsenic=mean(wells$arsenic))))
predict(model, newdata = data.frame(dist100=mean(wells$dist100), arsenic=mean(wells$arsenic)),
        type='response')
```

> The probability of switching at average values of the data is `59%` (pg. 243)

---

Coefficient for `distance`:

The coefficient for `dist100` corresponds to comparing two wells that different by 1 in dist100 (i.e. 100 meters) **if the arsenic level is 0 for both wells**.

We can calculate what is called `Average Partial Effect` which is described in Intro Econometrics pg 194; and also used in Business Data Analysis in the price elasticity example on pg 49).

We want to calculate what the coefficient of `dist100` will be for the average value of `arsenic` (i.e. the Average Partial Effect of `dist100` on `switch` at the average arsenic values).

Then we use the `divide by 4` trick to approximate predictive differences on the probability scale.

```{r}
(coef(model)['dist100'] + (coef(model)['dist100:arsenic'] * mean(wells$arsenic))) / 4
```

We can check this by plugging in values and using the `predict` function to get the point prediction for average the average arsenic value where A) `dist100` is at it's average value and B) `dist100` is increased by 1 unit (which corresponds to an increase `100` meters.)

```{r}
predict_switch <- function(.dist100, .arsenic) {
    predict(model,
            newdata = data.frame(dist100=.dist100, arsenic=.arsenic),
            type='response')
}

(average_distance <- mean(wells$dist100))
(average_arsenic <- mean(wells$arsenic))
```

```{r}
predict_switch(.dist100 = average_distance+1, .arsenic = average_arsenic)
predict_switch(.dist100 = average_distance, .arsenic = average_arsenic)
```

```{r}
predict_switch(.dist100 = average_distance+1, .arsenic = average_arsenic) -
predict_switch(.dist100 = average_distance, .arsenic = average_arsenic)
```

Pretty close.

---

Coefficient for `arsenic`.

The coefficient for `arsenic` corresponds to comparing two wells that different by 1 in in `arsenic` **if the distance to the closest clean well is 0 for both wells**.

We can calculate what is called `Average Partial Effect` which is described in Intro Econometrics pg 194; and also used in Business Data Analysis in the price elasticity example on pg 49).

We want to calculate what the coefficient of `arsenic` will be for the average value of `dist100` (i.e. the Average Partial Effect of `arsenic` on `switch` at the average dist100 values).

Then we use the `divide by 4` trick to approximate predictive differences on the probability scale.

```{r}
(coef(model)['arsenic'] + (coef(model)['dist100:arsenic'] * mean(wells$dist100))) / 4
```

```{r}
predict_switch(.dist100 = average_distance, .arsenic = average_arsenic+1) -
predict_switch(.dist100 = average_distance, .arsenic = average_arsenic)
```

## Centered Model

```{r}
set.seed(2)
model_centered <- stan_glm(switch ~ c_dist100*c_arsenic, family=binomial(link="logit"),
                           data = wells %>%
                               mutate(c_dist100 = dist100 - mean(dist100),
                                      c_arsenic = arsenic - mean(arsenic)),
                           refresh=0)
print(model_centered, digits = 2)
```

```{r chapter_14_wells_model_centered, fig.height=4, fig.width=4, message=FALSE}
ordered_levels <- as.matrix(model_centered) %>%
    as.data.frame() %>%
    select(-`(Intercept)`) %>%
    pivot_longer_all() %>%
    mutate(name = fct_reorder(name, value)) %>%
    pull(name) %>%
    levels()
    
bayesplot::mcmc_intervals(as.matrix(model_centered),
                          pars=vars(-'(Intercept)'),
                          prob=0.50,
                          prob_outer=0.95) +
    scale_y_discrete(limits = ordered_levels) +
    geom_vline(xintercept = 0, color='red')
```

Same values that we manually calculated above

```{r}
rstanarm::invlogit(coef(model_centered)['(Intercept)'])
```

```{r message=FALSE, warning=FALSE}
coef(model_centered) %>%
    tidy() %>%
    filter(names %in% c('c_dist100', 'c_arsenic')) %>%
    mutate(probability = x / 4)
```

Interaction didn't change from by centering.

```{r}
coef(model)['dist100:arsenic']
coef(model_centered)['c_dist100:c_arsenic']
```

---

## Interactions

> Note that the interaction term is not quite "statistically significant" in that the estimate is not quite two standard errors away from zero.

We can visualize the interactions.

Note: the book uses `invlogit(cbind(1, x/100, 0.5, 0.5*x/100) %*% coef(model))` where `x` are values of `dist` and then use `%*%` to multiply the values against the corresponding coefficients, and then sum to get the linear prediction, and then `invlogit` to get the probability prediction.

While this might be faster, or explain what is actually happening 'under the hood', at a glance it is hard to tell what is going on, and the code isn't explained.

We can just use `predict()` to get the same thing, which I already wrapped in a helper function, `predict_switch`, above.

```{r}
dist_x <- 100

invlogit(cbind(1, dist_x/100, 0.5, 0.5*dist_x/100) %*% coef(model))
predict_switch(.dist100 = dist_x/100, .arsenic = 0.5)
```

Also, they are using an arsenic level of `0.5` which is actually below (although slightly) the minimum arsenic level in the data, and comparing that to an arsenic value of 1, which is less than the median. The range only captures around the lowest 35% of arsenic levels. Seems like weird and arbitrary references. I'll use min/medium/max values.

```{r}
quantile(wells$arsenic, c(0, .5, 1))
```

```{r chapter_14_interaction_plots, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
set.seed(2)
model_interaction <- stan_glm(switch ~ dist100*arsenic, family=binomial(link="logit"), data=wells, refresh=0)
model_no_interaction <- stan_glm(switch ~ dist100+arsenic, family=binomial(link="logit"), data=wells, refresh=0)

jitter_binary <- function(a, jitt=.05){
  a + (1-2*a)*runif(length(a),0,jitt)
}

predict_switch <- function(.model, .dist100, .arsenic) {
    predict(.model,
            newdata = data.frame(dist100=.dist100, arsenic=.arsenic),
            type='response')
}

plot_prob_swiching_given_distance <- function(.model) {

    plot(wells$dist, jitter_binary(wells$switch), xlim=c(0, max(wells$dist)))
    for(arsenic_quantile in c(0, 0.5, 0.95, 1)) {
        arsenic_value <- quantile(wells$arsenic, arsenic_quantile)
        curve(predict_switch(.model, .dist100=x/100, .arsenic = arsenic_value), add=TRUE, col='red')
        
        text_y <- predict_switch(.model, .dist100=0, .arsenic = arsenic_value)
        text(.25, text_y, paste("if arsenic = ", arsenic_value), adj=0, cex=.8, col = 'red')
    }
    
}
plot_prob_swiching_given_distance(model_no_interaction)
plot_prob_swiching_given_distance(model_interaction)
```

## Average Predictive Difference in Probability (of `switching`)

See pg 251

```{r}
set.seed(1)
model_full <- stan_glm(switch ~ dist100 + arsenic + educ4 + dist100:educ4 + arsenic:educ4, family=binomial(link="logit"), data=wells, refresh=0)
#model_full <- stan_glm(switch ~ dist100*arsenic*educ4, family=binomial(link="logit"), data=wells, refresh=0)
#model_full <- stan_glm(switch ~ dist100 + arsenic + educ4, family=binomial(link="logit"), data=wells, refresh=0)
print(model_full, 2)
```

Note: `educ4` has a negative coefficient. But `educ4` in general has a positive impact in general. **This highlights the fact that you should not only look at the main coefficient when there are interactions.**

This is because this is the value if dist100 and arsenic are both `0`, the latter being impossible. We'll see below that, on average, education has a positive impact.

```{r chapter_14_wells_model_full, fig.height=4, fig.width=4, message=FALSE}
ordered_levels <- as.matrix(model_full) %>%
    as.data.frame() %>%
    select(-`(Intercept)`) %>%
    pivot_longer_all() %>%
    mutate(name = fct_reorder(name, value)) %>%
    pull(name) %>%
    levels()
    
bayesplot::mcmc_intervals(as.matrix(model_full),
                          pars=vars(-'(Intercept)'),
                          prob=0.50,
                          prob_outer=0.95) +
    scale_y_discrete(limits = ordered_levels) +
    geom_vline(xintercept = 0, color='red')
```

```{r}
new_data_dist_100_0 <- wells %>% select(arsenic, educ4) %>% mutate(dist100=0)
new_data_dist_100_1 <- wells %>% select(arsenic, educ4) %>% mutate(dist100=1)

head(new_data_dist_100_0)
head(new_data_dist_100_1)
```


```{r}
new_data_dist_100_0$prediction <- predict(model_full, newdata = new_data_dist_100_0, type='response')
new_data_dist_100_1$prediction <- predict(model_full, newdata = new_data_dist_100_1, type='response')
delta <- wells %>% select(arsenic, educ4) %>% mutate(delta = new_data_dist_100_1$prediction - new_data_dist_100_0$prediction)
head(delta)
```

> The result is `-0.21`, implying that, on average in the data, households that are 100 meters from the nearest safe well are `21%` less likely to switch, compared to households that are right next to the nearest safe well, 

```{r}
mean(new_data_dist_100_1$prediction - new_data_dist_100_0$prediction)
mean(delta$delta)
```

```{r}
quantile(delta$delta, c(0, 0.25, .5, .75, 1))
```

```{r chapter_14_average_predictive_diff_delta, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
hist(delta$delta)
```

```{r chapter_14_average_predictive_diff_expanded, fig.height=6, fig.width=plot_width_height_6, message=FALSE, warning=FALSE}
new_data_dist_100_0 %>%
    bind_rows(new_data_dist_100_1) %>%
    mutate(dist100 = paste(dist100 * 100, 'meters')) %>%
    ggplot(aes(x=arsenic, y=prediction, color=dist100, group=dist100, size=educ4)) +
    geom_point(alpha=0.4, shape=1) +
    geom_smooth(formula = y~x, aes(color=NULL), method='loess', se=FALSE) +
    coord_cartesian(xlim=c(0, 10)) +
    scale_x_continuous(breaks=pretty_breaks(10)) +
    scale_y_continuous(breaks=pretty_breaks(10), labels=percent_format(accuracy = 1)) +
    labs(title="Predictive Difference in Probability of Switching",
         subtitle="Comparing households 100 meters away from nearest safe well to those right next to one;\nacross all levels of arsenic/educ4",
         y="Probability of Switching (prediction)")
```

## Residuals

```{r}
residuals_model_full <- residuals(model_full)

wells$switch[1:5] - model_full$fitted.values[1:5]
model_full$y[1:5] - model_full$fitted.values[1:5]
residuals_model_full[1:5]
```

```{r chapter_14_residuals_plain, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
plot(x=model_full$fitted.values, y=residuals_model_full)
```

From https://avehtari.github.io/ROS-Examples/Arsenic/arsenic_logistic_residuals.html

```{r}
binned_resids <- function (x, y, nclass=sqrt(length(x))){
  breaks.index <- floor(length(x)*(1:(nclass-1))/nclass)
  breaks <- c (-Inf, sort(x)[breaks.index], Inf)
  output <- NULL
  xbreaks <- NULL
  x.binned <- as.numeric (cut (x, breaks))
  for (i in 1:nclass){
    items <- (1:length(x))[x.binned==i]
    x.range <- range(x[items])
    xbar <- mean(x[items])
    ybar <- mean(y[items])
    n <- length(items)
    sdev <- sd(y[items])
    output <- rbind (output, c(xbar, ybar, n, x.range, 2*sdev/sqrt(n)))
  }
  colnames (output) <- c ("xbar", "ybar", "n", "x.lo", "x.hi", "2se")
  return (list (binned=output, xbreaks=xbreaks))
}
br8 <- binned_resids(model_full$fitted.values, residuals_model_full, nclass=40)$binned
head(br8)
```

NOTE: the downside (if it is much of one) is that a point might appear, for example, at `x=0.2445056` (x coordinate for first row, and left-most point in graph below); but this is representing all the residuals associated with the estimated probability of switching (`model_full$fitted.values`) in the range of `0.03686552 to 0.3190301`.

```{r chapter_14_residuals_binned, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
plot(range(br8[,'xbar']), range(br8[,'ybar'],br8[,'2se'],-br8[,'2se']),
     xlab="Estimated  Pr (switching)", ylab="Average residual",
     type="n", main="Binned residual plot", mgp=c(2,.5,0))
abline(0,0, col="gray", lwd=.5)
lines(br8[,'xbar'], br8[,'2se'], col="gray", lwd=.5)
lines(br8[,'xbar'], -br8[,'2se'], col="gray", lwd=.5)
points(br8[,'xbar'], br8[,'ybar'], pch=20, cex=.5)
```

NOTE: we can use the `binnedplot` function from the `arm` package.

```{r chapter_14_residuals_binned_arm, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
arm::binnedplot(x=model_full$fitted.values, y=residuals_model_full)
```

```{r chapter_14_residuals_actual_vs_fitted, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
data.frame(fitted=model_full$fitted.values,
           actual=model_full$y,
           residuals=residuals(model_full)) %>%
    mutate(actual = factor(actual)) %>%
    ggplot(aes(fitted, color=actual, group=actual)) +
    geom_boxplot() +
    coord_flip()
    
```

```{r chapter_14_roc, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
library(pROC); library(plotROC)
plot(roc(model_full$y, fitted(model_full)), print.thres = c(.1, .5), col = "red", print.auc = T)
```

Note the `specificity` (`0.339`) and `sensitivity` (`0.832`) at the `0.5` threshold (which is what I use via `round` below) matches the numbers below in the confusion matrix.

```{r}
caret::confusionMatrix(factor(round(model_full$fitted.values)), factor(model_full$y), positive='1')
```

```{r include=FALSE}
rm(model)
rm(model_full)
rm(model_centered)
rm(model_interaction)
rm(model_no_interaction)
rm(predict_switch)
rm(average_distance)
rm(average_arsenic)
rm(jitter_binary)
rm(arsenic_quantile)
rm(arsenic_value)
rm(dist_x)
rm(text_y)
rm(ordered_levels)
rm(new_data_dist_100_0)
rm(new_data_dist_100_1)
rm(delta)
rm(residuals_model_full)
rm(binned_resids)
rm(br8)
rm(plot_prob_swiching_given_distance)
```

