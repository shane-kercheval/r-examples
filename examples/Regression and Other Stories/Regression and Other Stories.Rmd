---
title: "Regression and Other Stories"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
#library(rtools)
# library(stringr)
# library(ggrepel)
# library(forecast)
#library(scales)
#library(lubridate)

library(knitr)

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_5 <- calculate_plot_width(5)
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)
```

```{r downloading_data, eval=FALSE, include=FALSE}
download.file(url='https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat',
              destfile = 'data/hibbs.dat', quiet = TRUE)
```


# Overview

This document includes examples and exercises from `Regression and Other Stories` by Andrew Gelman, Jennifer Hill and Aki Vehtari, 2020, first edition.

# Resources

[Errata](https://avehtari.github.io/ROS-Examples/errata.html)

[Authors' Code](https://avehtari.github.io/ROS-Examples/examples.html#Examples_by_chapters)

# Packages

## statistical packages

```{r message=FALSE, warning=FALSE}
library(rstan)
library(rstanarm)
library(bayesplot)
library(loo)
```

## base packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(scales)
```

## Settings

```{r}
theme_set(theme_light())
options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)
```

# Chapter 1

## Simple Example with `stan_glm`

Load in the data:

```{r}
hibbs <- read.table('data/hibbs.dat', header = TRUE)
head(hibbs)
```

---

Graph data:

```{r figure.1.1.a, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hibbs %>%
    ggplot(aes(x=growth, y=vote)) +
    geom_hline(yintercept = 50, color="red", linetype="dashed") +
    geom_text(aes(label=year)) +
    geom_smooth(method='lm') +
    scale_x_continuous(labels=function(.x)paste0(.x, '%')) +
    scale_y_continuous(labels=function(.x)paste0(.x, '%')) +
    labs(title="Forecasting the election from the economy",
         y="Incuming party's vote share",
         x="Average recent growth in personal",
         caption='Figure 1.1')
```

---

Build Model:

```{r}
model <- stan_glm(vote ~ growth, data=hibbs)
```

---

Model Summary:

```{r}
summary(model)
```

---

Print()

```{r}
print(model)
```

---

Model Coefficients:

```{r}
coef(model)
```

---

Compare to `lm()`:

```{r}
summary(lm(vote ~ growth, data=hibbs))
```

(pretty close)

---

Graph using model coefficients rather than `geom_smooth(method='lm')`

```{r figure.1.1.a2, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
hibbs %>%
    ggplot(aes(x=growth, y=vote)) +
    geom_hline(yintercept = 50, color="red", linetype="dashed") +
    geom_text(aes(label=year)) +
    #geom_smooth(method='lm') +
    geom_abline(intercept = coef(model)['(Intercept)'], slope = coef(model)['growth']) +
    scale_x_continuous(labels=function(.x)paste0(.x, '%')) +
    scale_y_continuous(labels=function(.x)paste0(.x, '%')) +
    labs(title="Forecasting the election from the economy",
         y="Incuming party's vote share",
         x="Average recent growth in personal",
         caption='Figure 1.1')
```

---

# Chapter 3

## Log-Log Interpretation

### Example Model

```{r message=FALSE, warning=FALSE}
metabolic_rate <- read_csv("data/Primate Body Mass and Basal Metabolic Rate.csv")
head(metabolic_rate)
```

```{r chapter_3_log_log_not_scaled, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
metabolic_rate %>%
    ggplot(aes(x=`Primate Mass`, y=`Metabolic Rate`)) +
    geom_point() +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "(Not Log Scaled)")
```

```{r chapter_3_log_log_scaled, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
metabolic_rate %>%
    ggplot(aes(x=`Primate Mass`, y=`Metabolic Rate`)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "Log Scaled")
```

```{r}
model <- lm(log(`Metabolic Rate`) ~ log(`Primate Mass`), data=metabolic_rate)
summary(model)
```

```{r}
coef(model)
```

### Predictions

According to `Introductory Econometrics, Wooldridge (pg. 206), 

> Because the exponential undoes the log, our first guess for predicting y [when dependent variable is log(y)] is to simply exponentiate the predicted value for `log(y): y_predicted = exp(log y_predicted)). **This does not work; in fact, it will systematically underestimate the expected value of y**.

One adjustment that the author describes is the `Duan smearing estimate.

> Given an esimtate a0, we can predict y as 
>
> `y_predicted = a0 * exp(log y_predicted)`

where, 

> `a0 = sum(exp(residuals)) / length(residuals)`

The code below implements the `Duan smearing estimate.

```{r}
duan_smearing_adjustment <- function(.model) {
    model_residuals <- residuals(.model)
    duan_smearing_coefficient <- sum(exp(model_residuals)) / length(model_residuals)
    
    return (duan_smearing_coefficient)
}

log_log_predict <- function(.model, .newdata) {
    
    duan_smearing_coefficient <- duan_smearing_adjustment(.model)
    .predictions <- predict(.model, newdata=.newdata)
    .predictions <- duan_smearing_coefficient*exp(.predictions)

    return (.predictions)
}
```

Now, let's create predictions for all `Primate Masses` between `50` and `80000`, which is around the interval of our data.

```{r}
all_predictions <- data.frame(`Primate Mass`=seq(50, 80000), check.names = FALSE)
log_y_predictions <- predict(model, newdata=all_predictions)

# this simple adjustment is also described on pg 206 but assumes normal residuals
all_predictions$`Predictions - Unadjusted` <- exp(log_y_predictions)
all_predictions$`Predictions - Adjusted - Simple` <- exp(((summary(model)$sigma)^2)/2)*exp(log_y_predictions)
all_predictions$`Predictions - Adjusted - Duan` <- log_log_predict(.model=model, .newdata=all_predictions)
head(all_predictions)
```


```{r chapter_3_log_log_scaled_regression, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
predictions_long <- all_predictions %>% 
    # get the original values, where applicable
    #left_join(metabolic_rate, by = 'Primate Mass') %>%
    pivot_longer(-`Primate Mass`, names_to = 'Prediction Type', values_to='Metabolic Rate')

metabolic_rate_plot <- predictions_long %>%
    ggplot(aes(x=`Primate Mass`, y=`Metabolic Rate`, color=`Prediction Type`)) +
    geom_point(data=metabolic_rate, aes(x=`Primate Mass`, y=`Metabolic Rate`, color=NULL)) +
    geom_text(data=metabolic_rate, 
              aes(x=`Primate Mass`, y=`Metabolic Rate`,
                  label=glue::glue("({ `Primate Mass` }, { `Metabolic Rate` })"),
                  color=NULL),
              vjust=-0.5, check_overlap = TRUE) +
    geom_line()

metabolic_rate_plot +
    scale_x_log10() +
    scale_y_log10() +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "Log Scaled")
```

```{r chapter_3_log_log_unscaled_regression, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
metabolic_rate_plot +
    labs(title="Primate Body Mass And Basal Metabolic Rate",
         subtitle = "Unscaled")
```

> "A one-unit difference in `log x` corresponds to an additive difference of `b` in `log y`. (R&OS pg. 39)

This is essentially "elasticity" and is interpreted as a `1%` change in x has a `b`% change in `y` (Intro Econometrics pg. 39), although this does not hold for large changes in `y` (Intro Econometrics pg. 186).

So, the interpretation of the `log(Primate Mass)` coefficient is that a `1%` change in `Primate Mass` results in a 

``r paste0(round(coef(model)["log(`Primate Mass`)"], 2), "%")`` change in `Metabolic Rate`.

`r `

Let's see if this is true.

```{r}
(prediction_105 <- predict(model, newdata = data.frame(`Primate Mass`=105, check.names = FALSE)))
```

If the `Primate Mass` is `105` then our prediction of `log(Metabolic Rate`) is ``r prediction_105``

```{r}
exp(prediction_105)
```

Which means our prediction of `Metabolic Rate` is ``r exp(prediction_105)``, which matches the point on the graph.

```{r}
(one_percent_change <- (0.01 * 105) + 105)
(one_percent_change_prediction <- predict(model, newdata = data.frame(`Primate Mass`=one_percent_change, check.names = FALSE)))
one_percent_change_prediction <- exp(one_percent_change_prediction)
```

```{r}
format_percent <- function(.x) { paste0(round(.x * 100, 2), '%')}
format_percent((one_percent_change_prediction - exp(prediction_105)) / exp(prediction_105))
```

```{r}
coef(model)["log(`Primate Mass`)"]
```

Yep, this works. A `1%` (from `105` to ``r one_percent_change``) change in `Primate Mass` resulted in a `0.74%` change in the predicted `Metabolic Rate`

But lets try with a larger change.

Lets predict `Metabolic Change` for a `Primate Mass` of `9,500`

```{r}
(percent_change <- (9500 - 105) / 105)
```

The percent-change in `Primate Mass` between these two values is ``r format_percent(percent_change)``.

```{r}
(expected_percent_change_y <- as.numeric(coef(model)["log(`Primate Mass`)"]) * percent_change)
```

Therefore, the **expected** percent change in `Metabolic Rate` should be ``r format_percent(expected_percent_change_y)``.

But the actual percent change is:

```{r}
(prediction_9500 <- predict(model, newdata = data.frame(`Primate Mass`=9500, check.names = FALSE)))
format_percent((exp(prediction_9500) - exp(prediction_105)) / exp(prediction_105))
```

Which is quite lower.

So this rule of thumb doesn't work across large changes in `y`.

---

### Simulation of Log-Log Predictions

`log y = b*log x + random-noise`

```{r}
set.seed(3)
simulated_x <- rexp(100000, rate = 0.5) + 4
noise <- rnorm(100000, sd = 0.5)
b_coefficient <- 3
actual_log_y <- b_coefficient * log(simulated_x)
simulated_log_y <- actual_log_y + noise
simulated_y <- exp(simulated_log_y)

simulated_data <- data.frame(simulated_x, simulated_y)

simulated_data %>%
    ggplot(aes(x=simulated_x, y=simulated_y)) +
    geom_point(alpha=0.1) +
    scale_x_log10() +
    scale_y_log10()
```

```{r}
training_indices <- sample.int(n=nrow(simulated_data), size = 0.7*nrow(simulated_data), replace = FALSE)
training_data <- simulated_data[training_indices,]
test_data <- simulated_data[-training_indices,]
model <- lm(log(simulated_y) ~ log(simulated_x), data=training_data)
summary(model)
```

```{r}
log_y_predictions <- predict(model, newdata=test_data)

# this simple adjustment is also described on pg 206 but assumes normal residuals
test_data$`Predictions - Unadjusted` <- exp(log_y_predictions)
test_data$`Predictions - Adjusted - Simple` <- exp(((summary(model)$sigma)^2)/2)*exp(log_y_predictions)
test_data$`Predictions - Adjusted - Duan` <- log_log_predict(.model=model, .newdata=test_data)
head(test_data)
```

```{r chapter_3_log_log_scaled_simulation, fig.height=5, fig.width=plot_width_height_5, message=FALSE, warning=FALSE}
predictions_long <- test_data %>%
    select(-simulated_y) %>%
    # get the original values, where applicable
    #left_join(metabolic_rate, by = 'Primate Mass') %>%
    pivot_longer(-simulated_x, names_to = 'Prediction Type', values_to='Predicted Y')

simulated_plot <- predictions_long %>%
    ggplot(aes(x=simulated_x, y=`Predicted Y`, color=`Prediction Type`)) +
    geom_point(data=test_data, aes(x=simulated_x, y=simulated_y, color=NULL)) +
    geom_line()

simulated_plot +
    scale_x_log10() +
    scale_y_log10() +
    labs(title="Simulated Data & Predictions",
         subtitle = "Log Scaled",
         caption="The 'Simple' and 'Duan' adjustments are slow close that they overlap, and the Duan is being hidden behind the Simple.")
```

```{r chapter_3_log_log_unscaled_simulation, fig.height=5, fig.width=plot_width_height_5, message=FALSE}
simulated_plot +
    labs(title="Simulated Data & Predictions",
         subtitle = "Unscaled",
         caption="The 'Simple' and 'Duan' adjustments are slow close that they overlap, and the Duan is being hidden behind the Simple.")
```

The adjustments seem to make sense, visualy; but we can check the `Root Mean Square Error` (`RMSE`) to see if it actually results in a better out-of-sample fit, below (which it appears is the case).

```{r}
rmse = function(fitted, actual){
  sqrt(mean((fitted - actual)^2))
}

rmse(fitted=test_data$`Predictions - Unadjusted`, actual=test_data$simulated_y)
rmse(fitted=test_data$`Predictions - Adjusted - Simple`, actual=test_data$simulated_y)
rmse(fitted=test_data$`Predictions - Adjusted - Duan`, actual=test_data$simulated_y)
```
