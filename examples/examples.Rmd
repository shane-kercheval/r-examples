---
title: ""
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
#library(rtools)
# library(stringr)
# library(ggrepel)
# library(forecast)
library(scales)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(knitr)

options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)

theme_set(theme_light())

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)
```

# Packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(janitor)
library(ggplot2)
library(tidytext)
```

# Data Cleaning

---

```{r eval=FALSE, include=FALSE}
CO2
starwars
storms
```

## General

### `clean_names()`

```{r}
iris %>% colnames()
```

```{r}
iris %>% janitor::clean_names() %>% colnames()
```

---

### `extract()`

Turn `1` column into `x` columns based on regex

`convert=TRUE` converts them to numeric

```{r}
.df <- data.frame(season_epison = paste0('S', c(1, 1, 1, 2, 2, 2, 3, 3, 3),
                                         'E', c(1, 2, 3, 1, 2, 3, 1, 2, 3)))

.df %>% extract(season_epison, c('season', 'episode'), 'S(.*)E(.*)', convert = TRUE, remove = FALSE)
```

---

### regex_left_join

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=1715)

Join data.frames based on matching regex.

In this example, we will create categories and add them to our data.frame based on regex.

```{r message=FALSE, warning=FALSE}
#install.packages('fuzzyjoin')
library(fuzzyjoin)

cetaceans_raw <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-12-18/allCetaceanData.csv') %>% 
    select(-X1) %>%
    mutate(birthYear = as.numeric(birthYear))
cetaceans <- cetaceans_raw %>% select(species, originLocation)
head(cetaceans)
```

```{r}

regexes <- tribble(
  ~ regex, ~ category,
  "Unknown", "Unknown",
  "Gulf of Mexico", "Gulf of Mexico",
  "Florida|FL", "Florida",
  "Texas|TX", "Texas",
  "SeaWorld", "SeaWorld",
  "Pacific", "Pacific Ocean",
  "Atlantic", "Atlantic Ocean"
)

cetaceans %>%
    # left-join will create multiple rows if there are muliple matches
    # so create a row number to track unique rows
    mutate(unique_id = row_number()) %>%
    # join on regexes, based on regex
    regex_left_join(regexes, c(originLocation = "regex")) %>%
    # only keep the unique/distinct rows from the data.frame
    # If there are multiple rows for a given combination of inputs, only the first row will be preserved.
    # If omitted, will use all variables.
    # .keep_all:  If TRUE, keep all variables in .data.
    distinct(unique_id, .keep_all = TRUE) %>%
    # coalesce gets the first value that is not NA
    # so if category is not NA then use category, else use originLocation
    mutate(category = coalesce(category, originLocation)) %>%
    head(20)
```

---

## Aggregation

### `group_by() & which.max()`

```{r}
which.max(c(2, 1, 4, 3))
which.max(c(2, 4, 4, 3))
```

`first(name[which.max(height)])`

```{r}
starwars %>%
    group_by(gender) %>%
    summarise(n = n(),
              tallest_person = first(name[which.max(height)]),
              tallest_height = max(height, na.rm = TRUE),
              oldest_person = first(name[which.min(birth_year)]))
```

[Tidy Tuesday screencast: analyzing franchise revenue - YouTube](https://youtu.be/1xsbTs9-a50?t=365)

---

### `group_by()` & `top_n()`

This gets the `N` rows associated with the top `N` values for each category being grouped

`top_n(3, height)`

```{r}
starwars %>%
    group_by(gender) %>%
    top_n(3, height) %>%
    select(gender, name, height) %>%
    arrange(gender, height) %>%
    ungroup()
```

---

### `summarise()` & `across`

* Summarize multiple columns with multiple functions
* name columns with `glue` style convention

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(across(starts_with("d"),
                   list(mean = mean,
						   sd = sd),
                   .names = "{col}_{fn}"))
```

---

same as above using formulas

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(across(starts_with("d"), 
                   list(minus_sd = ~ (mean(.x) - sd(.x)), 
                        mean = mean, 
                        plus_sd = ~ (mean(.x) + sd(.x)))
                   ))
```

---

### `summarise_at()`

```{r}
iris %>% group_by(Species) %>% summarise_at(vars(Sepal.Length, Sepal.Width), sum, na.rm = TRUE)
```

---

### `add_count`

```{r}
iris %>% add_count(Species, name = 'num_species') %>% head()
```

`add_count(Species, name = 'num_species')` is equivalent to `group_by(Species) %>% mutate(num_species = n()) %>% ungroup()`

---

# dplyr/tidyverse 

## `join` `suffix`

```{r}
band_members %>%
    inner_join(band_members, by = 'name',
               suffix = c('.x', '.y'))
```


## `semi-join`

```{r}
band_members
```

```{r}
band_instruments
```

`x %>% semi_join(y)` is an `inner_join(y)` that returns only `x` (doesn't include any columns from `y`)

```{r}
band_members %>% semi_join(band_instruments, by = 'name')
```

```{r}
band_members %>% inner_join(band_instruments, by = 'name') %>% select(name, band)
```

---

## Indirection

Create a dplyr-like function that uses the column names of the dataframe rather than strings or the object directly.

https://dplyr.tidyverse.org/articles/programming.html

### dplyr-like function

```{r}
var_summary <- function(data, var) {
    # note the {{ var }}
    data %>%
        summarise(n = n(),
                  min = min({{ var }}),
                  max = max({{ var }}))
}

mtcars %>% 
    group_by(cyl) %>% 
    var_summary(mpg)
```

> If you want the user to provide a set of data-variables that are then transformed, use across():

```{r}
var_summary <- function(data, .group_by, .var) {
    # note the {{ var }}
    data %>%
        group_by(across({{ .group_by }})) %>%
        summarise(n = n(),
                  min = min({{ .var }}),
                  max = max({{ .var }}))
}

mtcars %>% var_summary(.group_by=cyl, .var=mpg)
```

```{r}
mtcars %>% var_summary(.group_by=c(cyl, vs), .var=mpg)
```

---

> Use the .names argument to across() to control the names of the output.

(Note the vignette uses `.col` instead of `col`, which fails.)

```{r}
my_summarise <- function(data, .group_by, .summarise_vars) {
  data %>%
    group_by(across({{ .group_by }})) %>% 
    summarise(across({{ .summarise_vars }},
                     mean,
                     .names = "mean_{col}"))
}
mtcars %>% my_summarise(cyl, mpg)
```

```{r}
mtcars %>% my_summarise(c(cyl, vs), .summarise_vars=c(mpg, hp))
```

---

### .data

> "Note that `.data` is not a data frame; it’s a special construct, a pronoun, that allows you to access the current variables either directly, with `.data$x` or indirectly with `.data[[var]]`. Don’t expect other functions to work with it."

```{r}
var <- 'cyl'
mtcars %>% count(.data[[var]])
```

```{r}
# ROW_NUMBER() OVER (PARITION BY col_x ORDER BY col_y) AS index
row_number_over_partition_by <- function(data, .partition_by, .order_by) {
    data %>%
        group_by(across({{ .partition_by }})) %>%
        mutate(index = row_number({{ .order_by }})) %>%
        ungroup()
}
mtcars %>% 
    select(cyl, mpg) %>%
    row_number_over_partition_by(cyl, mpg) %>%
    arrange(cyl, mpg)
```

```{r}
mtcars %>% 
    select(cyl, mpg) %>%
    row_number_over_partition_by(cyl, desc(mpg)) %>%
    arrange(cyl, mpg)
```

### eval_tidy

```{r}
with_data <- function(data, .x) {
  expr <- rlang::enquo(.x)
  print(expr)
  rlang::eval_tidy(expr, data = data)
}
mtcars %>% with_data(.x=mean(cyl) * 10)
```

### other examples

```{r}
# this is a hack because row_number only takes one column
#.partition_by, 
row_number_over_partition_by <- function(.x, .col_name, .partition_by, ...) {
    .result <- .x %>%
        mutate(temp______original_order = row_number()) %>%
        group_by(across({{ .partition_by }})) %>%
        arrange( ... , .by_group = TRUE) %>%
        mutate(grouped_index = row_number()) %>%
        ungroup() %>%
        arrange(temp______original_order) %>%
        pull(grouped_index)

    .x[deparse(substitute(.col_name))] <- .result
    
    return (.x)
}
mtcars %>% 
    select(cyl, am, vs, mpg) %>%
    row_number_over_partition_by(
        # new column name
        .col_name = my_index,
        # "partition by"
        .partition_by = c(cyl, am),
        # "order by" i.e. sort index in order of these variable
        desc(vs), mpg
    ) %>%
    arrange(desc(cyl), am, desc(vs), mpg)
```

```{r}
filter_over_partition_by <- function(.x, .partition_by, ...) {
    .x %>%
        mutate(temp______original_order = row_number()) %>%
        group_by(across({{ .partition_by }})) %>%
        arrange( ... , .by_group = TRUE) %>%
        filter(row_number() == 1) %>%
        ungroup() %>%
        arrange(temp______original_order) %>%
        select(-temp______original_order)
}
mtcars %>% 
    select(cyl, am, vs, mpg) %>%
    filter_over_partition_by(
        # "partition by"
        .partition_by = c(cyl, am),
        # "order by" i.e. sort index in order of these variable
        desc(vs), mpg
    ) %>%
    arrange(desc(cyl), am, desc(vs), mpg)
```

# ggplot

### `reorder_within()`

Reorder sub-categories within category e.g. for faceting.

For example, the order of `setosa`, `versicolor`, `virginica` is allowed to change for each measurement (e.g. `Petal.Length`, `Sepal.Width`)

other examples: https://juliasilge.com/blog/reorder-within/

```{r}
iris_gathered <- iris %>% pivot_longer(-Species, names_to = 'metric', values_to = 'value')
iris_gathered %>%
    ggplot(aes(x=tidytext::reorder_within(x=Species, by=value, within=metric),
               y=value)) +
    geom_boxplot() +
    # this is necessary to undo the transformations to the values that reorder_within did
    tidytext::scale_x_reordered() +
    facet_wrap(~ metric, scales = 'free_x')
```

---

### `scale_x_log10()` with seconds

	* scales the x-axis ticks but keeps the actual values the same
	* this works when viewing time (in seconds)

```{r}
data.frame(value = abs(rnorm(1000, sd = (60^4) / 3))) %>%
ggplot(aes(x='', y=value)) +
    geom_point() +
    scale_y_log10(breaks = c(0, 60 ^ (0:4)),
                  labels = c("0", "Second", "Minute", "Hour", "2.5 Days", "120 Days"))
```

---

### ggplot2 with `interaction()`

group by two columns in ggplot2

```{r}
# Data frame with two continuous variables and two factors 
set.seed(0)
x <- rep(1:10, 4)
y <- c(rep(1:10, 2)+rnorm(20)/5, rep(6:15, 2) + rnorm(20)/5)
treatment <- gl(2, 20, 40, labels=letters[1:2])
replicate <- gl(2, 10, 40)
d <- data.frame(x=x, y=y, treatment=treatment, replicate=replicate)
d %>%
    ggplot(aes(x=x, y=y, colour=treatment, shape = replicate,
               group=interaction(treatment, replicate))) +
    geom_point() +
    geom_line()
```

---

### spinogram

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=1244)

We will use `geom_area` but need to fill in the missing data with `complete`

First, here is what it would look like if we didn't use `complete`.

Notice the missing gaps. That is because we have missing years.

```{r message=FALSE, warning=FALSE}
cetaceans <- cetaceans_raw

cateaceans_acquisition_by_decade <- cetaceans %>%
  filter(originDate >= "1960-01-01") %>%
  count(acquisition,
        decade = 5 * (year(originDate) %/% 5))

cateaceans_acquisition_by_decade %>%
    mutate(acquisition = fct_reorder(acquisition, n, sum)) %>%
    group_by(decade) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(decade, percent, fill = acquisition)) +
    geom_area() +
    scale_y_continuous(labels = percent_format()) +
    scale_fill_manual(values=rtools::rt_colors()) +
    theme_minimal() +
    labs(x = "year",
       y = "% of dolphins recorded")
```

This shows everything in `cateaceans_acquisition_by_decade_complete` that is not in `cateaceans_acquisition_by_decade`. `complete` filled in the gabs.

```{r}
cateaceans_acquisition_by_decade_complete <- cateaceans_acquisition_by_decade %>%
  complete(acquisition, decade, fill = list(n = 0))

cateaceans_acquisition_by_decade_complete %>%
    anti_join(cateaceans_acquisition_by_decade, by = c("acquisition", "decade", "n")) %>%
    head(10)
```

```{r}
cateaceans_acquisition_by_decade_complete %>%
    mutate(acquisition = fct_reorder(acquisition, n, sum)) %>%
    group_by(decade) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(decade, percent, fill = acquisition)) +
    geom_area() +
    scale_y_continuous(labels = percent_format()) +
    scale_fill_manual(values=rtools::rt_colors()) +
    theme_minimal() +
    labs(x = "year",
       y = "% of dolphins recorded")
```


# Advanced

## Confidence Intervals w/ t-tests

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/em4FXPf4H-Y?t=1783)

```{r eval=FALSE, include=FALSE}
restaurant_inspections_raw <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")
restaurant_inspections <- restaurant_inspections_raw %>% 
  janitor::clean_names() %>%
  select(-phone, -grade_date, -record_date, -building, -street) %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  separate(inspection_type, c("inspection_program", "inspection_type"), sep = " / ")

inspections <- restaurant_inspections %>%
  group_by(camis,
           dba,
           boro,
           zipcode,
           cuisine_description,
           inspection_date,
           action,
           score,
           grade,
           inspection_type,
           inspection_program) %>%
  summarize(critical_violations = sum(critical_flag == "Critical", na.rm = TRUE),
            non_critical_violations = sum(critical_flag == "Not Critical", na.rm = TRUE)) %>%
  ungroup()

most_recent_cycle_inspection <- inspections %>%
  filter(inspection_program == "Cycle Inspection",
         inspection_type == "Initial Inspection") %>%
  arrange(desc(inspection_date)) %>%
  distinct(camis, .keep_all = TRUE)

restaurant_inspections_by_dba <- most_recent_cycle_inspection %>%
  group_by(dba, cuisine = cuisine_description) %>%
  summarize(locations = n(),
            avg_score = mean(score),
            median_score = median(score)) %>%
  ungroup() %>%
  arrange(desc(locations))

saveRDS(restaurant_inspections_by_dba, 'data/restaurant_inspections_by_dba.RDS')
```

```{r}
restaurant_inspections_by_dba <- readRDS('data/restaurant_inspections_by_dba.RDS')
head(restaurant_inspections_by_dba, 109)
```

`nest(data=-cuisine)` groups by cuisine and creates a data-frame (tibble) out of all of the rest of the columns. That is, it `nests` the all of the data (a data.frame) with each row corresponding to a cuisine.

Then we take each of those data.frames, and run a `t.test` of `ave_score`.

```{r}
library(broom)
cuisine_conf_ints <- restaurant_inspections_by_dba %>%
    add_count(cuisine) %>%
    filter(n > 100) %>%
    nest(data=-cuisine) %>%
    mutate(model = map(data, ~ t.test(.$avg_score))) %>%
    mutate(model = map(model, ~ tidy(.))) %>%
    unnest(model)
head(cuisine_conf_ints, 10)
```

```{r}
cuisine_conf_ints %>%
  mutate(cuisine = str_remove(cuisine, " \\(.*"),
         cuisine = fct_reorder(cuisine, estimate)) %>%
  ggplot(aes(estimate, cuisine)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high)) +
  labs(x = "Average inspection score (higher means more violations)",
       y = "Type of cuisine",
       title = "Average inspection score by type of cuisine in NYC",
       subtitle = "Each restaurant chain was counted once based on its average score")
```

## Survival Analysis

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=2424)

Context is dolphins. Are some dolphins living longer than they used to? This is hard because some dolphins in are dataset are still alive.

```{r}
library(survival)
cetaceans <- cetaceans_raw
dolphin_survival <- cetaceans %>%
  filter(status %in% c("Alive", "Died")) %>%
  mutate(deathYear = ifelse(status == "Alive", 2017, year(statusDate)),
         status = ifelse(status == "Alive", 0, 1),  # note: alive == 0
         age = deathYear - birthYear) %>%
  filter(!is.na(deathYear)) %>%
  select(birthYear, deathYear, status, sex, age, acquisition, species) %>%
  filter(deathYear >= birthYear) %>%
  filter(sex != "U")
head(dolphin_survival)
```

`status` is `0` if `alive`, `1` if `died`

So the followin gives the median age of death, by sex, with confidence intervals.

```{r}
model <- survival::survfit(Surv(age, status) ~ sex, dolphin_survival)
model
```

```{r}
broom::tidy(model) %>%
  ggplot(aes(time, estimate, color = strata)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```

How can we tell if sex is actually meaningful (i.e. the survival rate is actual different and not due to chance?)

We can use a `Cox proportional hazards regression model`

```{r}
survival::coxph(Surv(age, status) ~ sex, dolphin_survival) %>%
  tidy()
```

p.value is not statistically significant (confience intervals include 0) so we can say that there is an actual difference.

We can do the same thing for acquisition.

```{r}
model <- survival::survfit(Surv(age, status) ~ acquisition, dolphin_survival)
broom::tidy(model) %>%
  filter(strata != "acquisition=Unknown") %>%
  ggplot(aes(time, estimate, color = strata)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```

If we do `coxph` it looks like the holdout group is `Born` and each category is being compared to that. `Capture` is not statistically significant, but `Rescue` and `Unknown` are.

```{r}
survival::coxph(Surv(age, status) ~ acquisition, dolphin_survival) %>%
  tidy()
```

## Pairwise Correlation

```{r message=FALSE, warning=FALSE}
library(gapminder)
library(widyr)

life_expectency_pairwise_cor <- gapminder %>% 
         filter(continent == 'Americas') %>%
         pairwise_cor(country, year, lifeExp, sort = TRUE)

head(life_expectency_pairwise_cor)
```

```{r fig.height=7, fig.width=7}
life_expectency_pairwise_cor %>%
    mutate(item1=factor(item1, ordered = TRUE),
           item2=factor(item2, ordered = TRUE)) %>%
    filter(item1 < item2) %>%
    ggplot(aes(item1, item2, fill= correlation)) +
    geom_tile() +
    scale_fill_gradient(low="white", high="blue") +
    theme(axis.text.x=element_text(angle=90, hjust=1)) +
    labs(title='Which countries tend to have similar life-expectencies, over time?',
         subtitle = '(Americas)',
         x=NULL,
         y=NULL)
```

```{r}
life_expectency_pairwise_cor <- life_expectency_pairwise_cor %>%
    mutate(item1=factor(item1),
           item2=factor(item2)) %>%
    arrange(item2) %>%
    pivot_wider(names_from = item2, values_from = correlation) %>%
    arrange(item1)

life_expectency_pairwise_cor[1:5, 1:5]
```

## `ebbr` package: Empirical Bayes on the Binomial in R

> Methods for empirical Bayes shrinkage and estimation on data with many observations of success/total counts.

https://github.com/dgrtwo/ebbr

Examples from `Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.`

```{r example_11.1, echo=FALSE}
#devtools::install_github("dgrtwo/ebbr")
#install.packages('Lahman')
library(Lahman)

# Grab career batting average of non-pitchers
# (allow players that have pitched <= 3 games, like Ty Cobb)
pitchers <- Pitching %>%
    group_by(playerID) %>%
    summarize(gamesPitched = sum(G)) %>%
    filter(gamesPitched > 3)

# Add player names
player_names <- Master %>%
    tibble::as_tibble() %>%
    dplyr::select(playerID, nameFirst, nameLast, bats) %>%
    unite(name, nameFirst, nameLast, sep = " ")

# include the "bats" (handedness) and "year" column for later
career_full <- Batting %>%
    filter(AB > 0) %>%
    # remove pitchers
    anti_join(pitchers, by = "playerID") %>%
    group_by(playerID) %>%
    summarize(H = sum(H),
              AB = sum(AB),
              year = mean(yearID)) %>%
    ungroup() %>%
    inner_join(player_names, by = "playerID") %>%
    filter(!is.na(bats)) %>%
    rename(hits=H, at_bats=AB) %>%
    mutate(batting_average = hits / at_bats)
# We don't need all this data for every step
career <- career_full %>% 
    select(playerID, name, hits, at_bats, batting_average)

head(career)
```

> In Chapter 3, we noticed that the distribution of player batting averages looked roughly like a beta distribution (Figure 11.1). We thus wanted to estimate the beta prior for the overall dataset, which is the first step of empirical Bayes analysis. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
career %>%
    filter(at_bats > 500) %>%
    ggplot(aes(x=batting_average)) +
    geom_histogram(bins = 50) +
    labs(title='Batting Average Distribution of Players with >500 At-Bats')
```


### Prior Distribution

```{r message=FALSE, warning=FALSE}
library(ebbr)
prior <- career %>%
  filter(at_bats >= 500) %>%
  ebb_fit_prior(hits, at_bats)

prior
```

I'm not sure how `tidy(prior)$mean` is calculated.

```{r}
as.numeric(tidy(prior)$mean)

career %>%
    filter(at_bats >= 500) %>%
    summarise(sum(hits) / sum(at_bats))

career %>%
    filter(at_bats >= 500) %>%
    summarise(mean(batting_average))

career %>%
    filter(at_bats >= 500) %>%
    summarise(median(batting_average))
```

```{r}
beta_distribution <- data.frame(x=seq(0.17,0.35,0.001)) %>%
    mutate(y=dbeta(x, prior$parameters$alpha, prior$parameters$beta))

beta_distribution %>%
    ggplot(aes(x=x, y=y)) +
    geom_line() +
    labs(title="Beta Distribution using Alpha/Beta from Calculated Priors",
         subtitle = glue::glue("({ round(prior$parameters$alpha, 2) }, { round(prior$parameters$beta, 2) })"))

career %>%
    filter(at_bats > 500) %>%
    ggplot(aes(x=batting_average)) +
    geom_histogram(bins = 50) +
    geom_line(data=beta_distribution, aes(x=x, y=y*17), color='red') +
    labs(title="Batting Average Distribution of Players with >500 At-Bats",
         subtitle=glue::glue("Red Line is Beta Distribution using Alpha/Beta from Calculated Priors ({ round(prior$parameters$alpha, 2) }, { round(prior$parameters$beta, 2) })"))
```

### Updating Observations based on Priors

> The second step of empirical Bayes analysis is updating each observation based on the overall statistical model. Based on the philosophy of the broom package, this is achieved with the augment() function. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
head(augment(prior, data = career))
```

### `add_ebb_estimate`

> Notice we've now added several columns to the original data, each beginning with . (which is a convention of the augment verb to avoid rewriting existing columns). We have the .alpha1 and .beta1 columns as the parameters for each player's posterior distribution, as well as .fitted representing the new posterior mean (the "shrunken average"). We often want to run these two steps in sequence: estimating a model, then using it as a prior for each observation. The ebbr package provides a shortcut, combining them into one step with add_ebb_estimate(). (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
eb_career <- career %>%
  add_ebb_estimate(hits, at_bats,
                   prior_subset = at_bats >= 500)
```

```{r}
all(eb_career$batting_average == eb_career$.raw)
```

> This  [the graph below] was one of the most important visualizations in Chapter 3. I like how it captures what empirical Bayes estimation is doing: moving all batting averages towards the prior mean (the dashed red line), but moving them less if there is a lot of information about that player (high at_bats). (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

The red line represents points where the raw batting average and the estimated batting average are identical.

The dashed red line represents the "prior mean" which is similar, but doesn't seem to be exactly, the average of the batting averages (of people with >= 500 at-bats).

So for example, look at the dots/people to the left of the graph. These are people that had very few at-bats (dark color), had a very low batting average (near 0 on x-axis), but we shifted from their raw batting average to the prior mean batting average (i.e. shifted from solid red line to dashed red line).

The people with a lot of at bats, tend to have estimated batting average that is very close to the raw batting average.

```{r message=FALSE, warning=FALSE}
eb_career %>%
    ggplot(aes(x=.raw, y=.fitted, color = at_bats)) +
    geom_point() +
    geom_abline(color = "red") +
    scale_color_continuous(trans = "log", breaks = c(1, 10, 100, 1000)) +
    geom_hline(yintercept = tidy(prior)$mean, color = "red", lty = 2) +
    coord_cartesian(xlim = c(0, .6), ylim=c(0, 0.6)) +
    scale_x_continuous(breaks = pretty_breaks()) +
    scale_y_continuous(breaks = pretty_breaks()) +
    labs(x = "Raw batting average",
       y = "Shrunken batting average")
```

```{r message=FALSE}
eb_career %>%
    filter(at_bats > 10) %>%
    rename(Raw = .raw, Shrunken = .fitted) %>%
    gather(type, estimate, Raw, Shrunken) %>%
    ggplot(aes(at_bats, estimate)) +
    geom_point() +
    geom_smooth(method='lm') +
    geom_hline(yintercept = tidy(prior)$mean, color = "red", lty = 2) +
    facet_wrap(~ type) +
    scale_x_log10()
```

This graph shows that we are shrinking people with a low number of at bats toward the prior mean (red dotted line) rather than towards the fitted regression line, which shows that the more at-bats someone has, the higher their batting-average tends to be (the better you are the more at bats you get). We'll solve this below, but for now, this means that, of the people who have very few at bats, we're likely over-estimating their ability/batting-average. 

```{r}
eb_career %>%
    head(10) %>%
    mutate(name = reorder(name, .fitted)) %>%
    ggplot(aes(x=.fitted, y=name)) +
    geom_point() +
    geom_errorbarh(aes(xmin = .low, xmax = .high)) +
    geom_point(aes(x=.raw), color='red') +
    geom_text(aes(x=.raw, label=glue::glue("({ hits } / { at_bats })")), vjust=-0.7, size=3) +
    labs(x = "Estimated batting average (w/ 95% confidence interval)",
         y = "Player")
```


### Hierarchical Modeling

> In Chapters 7 and 8, we examined how this beta-binomial model may not be appropriate, because of the relationship between a player's at-bats and their batting average. Good batters tend to have long careers, while poor batters may retire quickly. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

This means that, in the previous estimates and graphs, it's not appropriate to move people who have very few bats (and for example very low batting averages), all the way up to the prior mean batting average (``r tidy(prior)$mean``). We should assume (had they keep getting more and more at-bats) that they would have a lower batting average than average.

```{r message=FALSE}
career %>%
  filter(at_bats >= 10) %>%
  ggplot(aes(at_bats, hits / at_bats)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10()
```

This graphs shows that the more at-bats someone has, the better their batting average tends to be, which makes sense. This is the line that we should be pushing people towards who have very few at bats, rather than the dotted red line above.

> We solved this by fitting a prior that depended on AB, through the process of beta-binomial regression. The add_ebb_estimate() function from ebbr offers this option, by setting method = "gamlss" and providing a formula to mu_predictors.3 (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

Obviously, now we do not want to filter our prior based on >=500 at-bats because we are using at-bats directly in our estimation.

```{r message=FALSE, warning=FALSE}
eb_career_ab <- career %>%
  add_ebb_estimate(hits, at_bats, method = "gamlss",
                    mu_predictors = ~ log10(at_bats))

head(eb_career_ab)
```

```{r message=FALSE}
eb_career_ab %>%
    filter(at_bats > 10) %>%
    rename(Raw = .raw, Shrunken = .fitted) %>%
    gather(type, estimate, Raw, Shrunken) %>%
    ggplot(aes(at_bats, estimate)) +
    geom_point() +
    geom_smooth(method='lm') +
    geom_hline(yintercept = tidy(prior)$mean, color = "red", lty = 2) +
    facet_wrap(~ type) +
    scale_x_log10()
```

Now, we are shrinking the players with fewer at-bats to a lower batting average.

```{r}
eb_career_ab %>%
    head(10) %>%
    mutate(name = reorder(name, .fitted)) %>%
    ggplot(aes(x=.fitted, y=name)) +
    geom_point() +
    geom_errorbarh(aes(xmin = .low, xmax = .high)) +
    geom_point(aes(x=.raw), color='red') +
    geom_text(aes(x=.raw, label=glue::glue("({ hits } / { at_bats })")), vjust=-0.7, size=3) +
    labs(x = "Estimated batting average (w/ 95% confidence interval)",
         y = "Player")
```

Now, Andy Abad and Dan Abbot have much lower estimated batting-averages compared to the graph above. In the graph above Dan Abbot was estimated to have a slightly higher batting average than Kurt Abbott, who has many more at-bats than Dan. Now, Dan is estimated to have a much lower batting-avearge.











