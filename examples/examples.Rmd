---
title: ""
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
# library(rtools)
# library(stringr)
# library(ggrepel)
# library(forecast)
library(scales)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(knitr)

options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)

theme_set(theme_light())

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
beer_awards <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-20/beer_awards.csv') %>%
  mutate(state = str_to_upper(state),
         medal = fct_relevel(medal, c("Bronze", "Silver")))

cetaceans_raw <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-12-18/allCetaceanData.csv') %>% 
    select(-X1) %>%
    mutate(birthYear = as.numeric(birthYear))
cetaceans <- cetaceans_raw %>% select(species, originLocation)

ikea <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv') %>%
    select(-X1) %>%
    mutate(price_usd = 0.27 * price,
           short_description = str_squish(short_description)) %>%
    add_count(category, name = "category_total") %>%
    select(item_id, name, price_usd, category, category_total, short_description, depth, height, width, other_colors)

mr_boston <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv')

restaurant_inspections_by_dba <- readRDS('data/restaurant_inspections_by_dba.RDS')
```

# Data Cleaning

---

```{r eval=FALSE, include=FALSE}
CO2
starwars
storms
```

## General

### `clean_names()`

```{r}
iris %>% colnames()
```

```{r}
iris %>% janitor::clean_names() %>% colnames()
```

---

### `extract()`

Turn `1` column into `x` columns based on regex

`convert=TRUE` converts them to numeric

```{r}
.df <- data.frame(season_epison = paste0('S', c(1, 1, 1, 2, 2, 2, 3, 3, 3),
                                         'E', c(1, 2, 3, 1, 2, 3, 1, 2, 3)))

.df %>% extract(season_epison, c('season', 'episode'), 'S(.*)E(.*)', convert = TRUE, remove = FALSE)
```

---

### regex_left_join

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=1715)

Join data.frames based on matching regex.

In this example, we will create categories and add them to our data.frame based on regex.

```{r message=FALSE, warning=FALSE}
head(cetaceans)
```

```{r}
#install.packages('fuzzyjoin')
#library(fuzzyjoin)

regexes <- tribble(
  ~ regex, ~ category,
  "Unknown", "Unknown",
  "Gulf of Mexico", "Gulf of Mexico",
  "Florida|FL", "Florida",
  "Texas|TX", "Texas",
  "SeaWorld", "SeaWorld",
  "Pacific", "Pacific Ocean",
  "Atlantic", "Atlantic Ocean"
)

cetaceans %>%
    # left-join will create multiple rows if there are muliple matches
    # so create a row number to track unique rows
    mutate(unique_id = row_number()) %>%
    # join on regexes, based on regex
    fuzzyjoin::regex_left_join(regexes, c(originLocation = "regex")) %>%
    # only keep the unique/distinct rows from the data.frame
    # If there are multiple rows for a given combination of inputs, only the first row will be preserved.
    # If omitted, will use all variables.
    # .keep_all:  If TRUE, keep all variables in .data.
    distinct(unique_id, .keep_all = TRUE) %>%
    # coalesce gets the first value that is not NA
    # so if category is not NA then use category, else use originLocation
    mutate(category = coalesce(category, originLocation)) %>%
    head(20)
```

---

## Aggregation

### `group_by() & which.max()`

```{r}
which.max(c(2, 1, 4, 3))
which.max(c(2, 4, 4, 3))
```

`first(name[which.max(height)])`

```{r}
starwars %>%
    group_by(gender) %>%
    summarise(n = n(),
              tallest_person = first(name[which.max(height)]),
              tallest_height = max(height, na.rm = TRUE),
              oldest_person = first(name[which.min(birth_year)]))
```

[Tidy Tuesday screencast: analyzing franchise revenue - YouTube](https://youtu.be/1xsbTs9-a50?t=365)

---

### `group_by()` & `top_n()`

This gets the `N` rows associated with the top `N` values for each category being grouped

`top_n(3, height)`

```{r}
starwars %>%
    group_by(gender) %>%
    top_n(3, height) %>%
    select(gender, name, height) %>%
    arrange(gender, height) %>%
    ungroup()
```

---

### `summarise()` & `across`

* Summarize multiple columns with multiple functions
* name columns with `glue` style convention

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(across(starts_with("d"),
                   list(mean = mean,
						   sd = sd),
                   .names = "{col}_{fn}"))
```

---

same as above using formulas

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(across(starts_with("d"), 
                   list(minus_sd = ~ (mean(.x) - sd(.x)), 
                        mean = mean, 
                        plus_sd = ~ (mean(.x) + sd(.x)))
                   ))
```

---

### `summarise_at()`

```{r}
iris %>% group_by(Species) %>% summarise_at(vars(Sepal.Length, Sepal.Width), sum, na.rm = TRUE)
```

---

### `add_count`

```{r}
iris %>% add_count(Species, name = 'num_species') %>% head()
```

`add_count(Species, name = 'num_species')` is equivalent to `group_by(Species) %>% mutate(num_species = n()) %>% ungroup()`

---

# dplyr/tidyverse 

## `join` `suffix`

```{r}
band_members %>%
    inner_join(band_members, by = 'name',
               suffix = c('.x', '.y'))
```

---

## `semi-join`

```{r}
band_members
```

```{r}
band_instruments
```

`x %>% semi_join(y)` is an `inner_join(y)` that returns only `x` (doesn't include any columns from `y`)

```{r}
band_members %>% semi_join(band_instruments, by = 'name')
```

```{r}
band_members %>% inner_join(band_instruments, by = 'name') %>% select(name, band)
```

---

## `separate_rows`

```{r}
df <- tibble(
  x = 1:3,
  y = c("a", "d,e,f", "g,h"),
  z = c("1", "2,3,4", "5,6")
)
df
separate_rows(df, y, z, convert = TRUE)
```

---

## Indirection

Create a dplyr-like function that uses the column names of the dataframe rather than strings or the object directly.

https://dplyr.tidyverse.org/articles/programming.html

### dplyr-like function

```{r}
var_summary <- function(data, var) {
    # note the {{ var }}
    data %>%
        summarise(n = n(),
                  min = min({{ var }}),
                  max = max({{ var }}))
}

mtcars %>% 
    group_by(cyl) %>% 
    var_summary(mpg)
```

> If you want the user to provide a set of data-variables that are then transformed, use across():

```{r}
var_summary <- function(data, .group_by, .var) {
    # note the {{ var }}
    data %>%
        group_by(across({{ .group_by }})) %>%
        summarise(n = n(),
                  min = min({{ .var }}),
                  max = max({{ .var }}))
}

mtcars %>% var_summary(.group_by=cyl, .var=mpg)
```

```{r}
mtcars %>% var_summary(.group_by=c(cyl, vs), .var=mpg)
```

---

> Use the .names argument to across() to control the names of the output.

(Note the vignette uses `.col` instead of `col`, which fails.)

```{r}
my_summarise <- function(data, .group_by, .summarise_vars) {
  data %>%
    group_by(across({{ .group_by }})) %>% 
    summarise(across({{ .summarise_vars }},
                     mean,
                     .names = "mean_{col}"))
}
mtcars %>% my_summarise(cyl, mpg)
```

```{r}
mtcars %>% my_summarise(c(cyl, vs), .summarise_vars=c(mpg, hp))
```

---

### .data

> "Note that `.data` is not a data frame; it’s a special construct, a pronoun, that allows you to access the current variables either directly, with `.data$x` or indirectly with `.data[[var]]`. Don’t expect other functions to work with it."

```{r}
var <- 'cyl'
mtcars %>% count(.data[[var]])
```

```{r}
# ROW_NUMBER() OVER (PARITION BY col_x ORDER BY col_y) AS index
row_number_over_partition_by <- function(data, .partition_by, .order_by) {
    data %>%
        group_by(across({{ .partition_by }})) %>%
        mutate(index = row_number({{ .order_by }})) %>%
        ungroup()
}
mtcars %>% 
    select(cyl, mpg) %>%
    row_number_over_partition_by(cyl, mpg) %>%
    arrange(cyl, mpg)
```

```{r}
mtcars %>% 
    select(cyl, mpg) %>%
    row_number_over_partition_by(cyl, desc(mpg)) %>%
    arrange(cyl, mpg)
```

### eval_tidy

```{r}
with_data <- function(data, .x) {
  expr <- rlang::enquo(.x)
  print(expr)
  rlang::eval_tidy(expr, data = data)
}
mtcars %>% with_data(.x=mean(cyl) * 10)
```

### other examples

```{r}
# this is a hack because row_number only takes one column
#.partition_by, 
row_number_over_partition_by <- function(.x, .col_name, .partition_by, ...) {
    .result <- .x %>%
        mutate(temp______original_order = row_number()) %>%
        group_by(across({{ .partition_by }})) %>%
        arrange( ... , .by_group = TRUE) %>%
        mutate(grouped_index = row_number()) %>%
        ungroup() %>%
        arrange(temp______original_order) %>%
        pull(grouped_index)

    .x[deparse(substitute(.col_name))] <- .result
    
    return (.x)
}
mtcars %>% 
    select(cyl, am, vs, mpg) %>%
    row_number_over_partition_by(
        # new column name
        .col_name = my_index,
        # "partition by"
        .partition_by = c(cyl, am),
        # "order by" i.e. sort index in order of these variable
        desc(vs), mpg
    ) %>%
    arrange(desc(cyl), am, desc(vs), mpg)
```

```{r}
filter_over_partition_by <- function(.x, .partition_by, ...) {
    .x %>%
        mutate(temp______original_order = row_number()) %>%
        group_by(across({{ .partition_by }})) %>%
        arrange( ... , .by_group = TRUE) %>%
        filter(row_number() == 1) %>%
        ungroup() %>%
        arrange(temp______original_order) %>%
        select(-temp______original_order)
}
mtcars %>% 
    select(cyl, am, vs, mpg) %>%
    filter_over_partition_by(
        # "partition by"
        .partition_by = c(cyl, am),
        # "order by" i.e. sort index in order of these variable
        desc(vs), mpg
    ) %>%
    arrange(desc(cyl), am, desc(vs), mpg)
```

# ggplot

### `reorder_within()`

Reorder sub-categories within category e.g. for faceting.

For example, the order of `setosa`, `versicolor`, `virginica` is allowed to change for each measurement (e.g. `Petal.Length`, `Sepal.Width`)

other examples: https://juliasilge.com/blog/reorder-within/

```{r reorder_within_iris}
iris_gathered <- iris %>% pivot_longer(-Species, names_to = 'metric', values_to = 'value')
iris_gathered %>%
    ggplot(aes(x=tidytext::reorder_within(x=Species, by=value, within=metric),
               y=value)) +
    geom_boxplot() +
    # this is necessary to undo the transformations to the values that reorder_within did
    tidytext::scale_x_reordered() +
    facet_wrap(~ metric, scales = 'free_x')
```

---

### `scale_x_log10()` with seconds

	* scales the x-axis ticks but keeps the actual values the same
	* this works when viewing time (in seconds)

```{r scale_x_log10_seconds}
set.seed(1)
data.frame(value = abs(rnorm(1000, sd = (60^4) / 3))) %>%
ggplot(aes(x='', y=value)) +
    geom_point() +
    scale_y_log10(breaks = c(0, 60 ^ (0:4)),
                  labels = c("0", "Second", "Minute", "Hour", "2.5 Days", "120 Days"))
```

---

### ggplot2 with `interaction()`

group by two columns in ggplot2

```{r interaction}
# Data frame with two continuous variables and two factors 
set.seed(0)
x <- rep(1:10, 4)
y <- c(rep(1:10, 2)+rnorm(20)/5, rep(6:15, 2) + rnorm(20)/5)
treatment <- gl(2, 20, 40, labels=letters[1:2])
replicate <- gl(2, 10, 40)
d <- data.frame(x=x, y=y, treatment=treatment, replicate=replicate)
d %>%
    ggplot(aes(x=x, y=y, colour=treatment, shape = replicate,
               group=interaction(treatment, replicate))) +
    geom_point() +
    geom_line()
```

---

### spinogram

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=1244)

We will use `geom_area` but need to fill in the missing data with `complete`

First, here is what it would look like if we didn't use `complete`.

Notice the missing gaps. That is because we have missing years.

```{r spinogram_dolphins_v1, message=FALSE, warning=FALSE}
cateaceans_acquisition_by_decade <- cetaceans_raw %>%
  filter(originDate >= "1960-01-01") %>%
  count(acquisition,
        decade = 5 * (year(originDate) %/% 5))

cateaceans_acquisition_by_decade %>%
    mutate(acquisition = fct_reorder(acquisition, n, sum)) %>%
    group_by(decade) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(decade, percent, fill = acquisition)) +
    geom_area() +
    scale_y_continuous(labels = percent_format()) +
    scale_fill_manual(values=rtools::rt_colors()) +
    theme_minimal() +
    labs(x = "year",
       y = "% of dolphins recorded")
```

This shows everything in `cateaceans_acquisition_by_decade_complete` that is not in `cateaceans_acquisition_by_decade`. `complete` filled in the gabs.

```{r}
cateaceans_acquisition_by_decade_complete <- cateaceans_acquisition_by_decade %>%
  complete(acquisition, decade, fill = list(n = 0))
```

What did `complete()` do? Lets look at the rows that were added (i.e. the rows in _complete that are not in _decade i.e. `anti_join`)?

`complete` filled in the missing combinations with a default value of `n` = `1`

```{r}
cateaceans_acquisition_by_decade_complete %>%
    anti_join(cateaceans_acquisition_by_decade, by = c("acquisition", "decade", "n")) %>%
    head(10)
```

```{r spinogram_dolphins_v2}
cateaceans_acquisition_by_decade_complete %>%
    mutate(acquisition = fct_reorder(acquisition, n, sum)) %>%
    group_by(decade) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(decade, percent, fill = acquisition)) +
    geom_area() +
    scale_y_continuous(labels = percent_format()) +
    scale_fill_manual(values=rtools::rt_colors()) +
    theme_minimal() +
    labs(x = "year",
       y = "% of dolphins recorded")
```

---

### Joy Plot

Example from David Robinson's screencast (https://youtu.be/lY0YLDZhT88?t=665)

```{r}
ikea %>% select(category, name, price_usd) %>% head()
```

```{r ggridges_1}
#library(ggridges)
ikea %>%
  mutate(category = glue::glue("{ category } ({ category_total })"),
         category = fct_reorder(category, price_usd)) %>%
  ggplot(aes(price_usd, category)) +
  ggridges::geom_density_ridges() +
  # geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar) +
  labs(x = "Price (USD)",
       y = "",
       title = "How much do items in each category cost?")
```

```{r ggridges_2}
ikea %>%
  mutate(category = glue::glue("{ category } ({ category_total })"),
         category = fct_reorder(category, price_usd)) %>%
  ggplot(aes(price_usd, category, fill = other_colors)) +
  ggridges::geom_density_ridges(alpha = .5) +
  # geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar) +
  labs(x = "Price (USD)",
       y = "",
       title = "How much do items in each category cost?")
```

> "If we were building a predictive model, we'd probably include both category and other_colors"

---

### Sankey

https://github.com/davidsjoberg/ggsankey

```{r}
# devtools::install_github("davidsjoberg/ggsankey")
#library(ggsankey)

example_dat <- mtcars %>%
  ggsankey::make_long(cyl, vs, am, gear, carb) # function in ggsankey to format data correctly

ggplot(example_dat,
       aes(x = x,
           next_x = next_x, 
           node = node, 
           next_node = next_node,
           fill = factor(node))) +
    ggsankey::geom_sankey(flow.alpha = .6) +
    theme_minimal()
```

---

# Advanced

## Confidence Intervals w/ t-tests

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/em4FXPf4H-Y?t=1783)

```{r}
head(restaurant_inspections_by_dba, 10)
```

`nest(data=-cuisine)` groups by cuisine and creates a data-frame (tibble) out of all of the rest of the columns. That is, it `nests` the all of the data (a data.frame) with each row corresponding to a cuisine.

Then we take each of those data.frames, and run a `t.test` of `ave_score`.

```{r}
#library(broom)
cuisine_conf_ints <- restaurant_inspections_by_dba %>%
    add_count(cuisine) %>%  # adds number of dbas for each cuisine
    filter(n > 100) %>%  # only keep cuisines that have >100 invidual DBAs
    nest(data=-cuisine) %>%  # take all columns except for cuisine and collapse the resulting data.frame into the row's cell
    mutate(num_dbas = map_int(data, ~ nrow(.))) %>%
    mutate(mean_avg_score = map_dbl(data, ~ mean(.$avg_score))) %>%
    mutate(model = map(data, ~ t.test(.$avg_score))) %>%
    mutate(model = map(model, ~ tidy(.))) %>%
    unnest(model)
# note: the p-value is meaningless, the null hypothesis is that the mean is equal to 0
# we are just using this to get us confidence intervals

round_2 <- function(.x) {round(.x, 2)}
head(cuisine_conf_ints %>% mutate_if(is.numeric, round_2), 10)
```

```{r t_test_confidence_intervals}
cuisine_conf_ints %>%
  mutate(cuisine = str_remove(cuisine, " \\(.*"),
         cuisine = fct_reorder(cuisine, estimate)) %>%
  ggplot(aes(estimate, cuisine)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high)) +
  labs(x = "Average inspection score (higher means more violations)",
       y = "Type of cuisine",
       title = "Average inspection score by type of cuisine in NYC",
       subtitle = "Each restaurant chain was counted once based on its average score")
```

---

## `optimize`

The function optimize searches the interval from lower to upper for a minimum or maximum of the function f with respect to its first argument.

Example: flipping a coin. `dbinom` gives the probability of observing x “successes” for a given sample size and assumed probability of observing x.

The probability of observing `7` heads in `10` flips if the true probability of flipping heads is `50%`, is `11.7%`. dbinom over all values of heads for a given probability of heads (i.e. 0.5) is a probability distribution.

```{r}
dbinom(7, size=10, prob=0.5)
sum(dbinom(0:10, size=10, prob=0.5))
```

However, let's say we want the relative **relative** probability/plausibility of observing `7` heads over `10` flips if we didn't know the true probability was `0.5`

Now we would hold `7` fixed and search `prob` across a grid of possible values.

```{r}
sample_size <- 10
num_heads <- 7
probability_grid <- seq(0, 1, 0.001)
likelihood <- dbinom(num_heads, size=sample_size, prob=probability_grid)
```

But we also want to find the most plausible likelihood.

```{r}
most_plausible <- optimize(function (.x) -dbinom(num_heads,
                                                 size=sample_size,
                                                 prob=.x),
                           c(0, 1),
                           tol = 0.0001)
most_plausible
```

This is **not** a probability distribution, it is a distribution of relative plausibilities.

```{r}
plot(probability_grid, likelihood)
abline(v = most_plausible$minimum, col='red', lwd=3)
```

See https://github.com/shane-kercheval/r-examples/blob/main/examples/examples.md#confidence-intervals-w-t-tests for a more indepth example. 

---

## Survival Analysis

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=2424)

Context is dolphins. Are some dolphins living longer than they used to? This is hard because some dolphins in are dataset are still alive.

```{r}
#library(survival)
cetaceans <- cetaceans_raw
dolphin_survival <- cetaceans %>%
  filter(status %in% c("Alive", "Died")) %>%
         # deathYear needs a value even if they didn't die (status will indicate if they did die)
  mutate(deathYear = ifelse(status == "Alive", 2017, year(statusDate)),
         status = ifelse(status == "Alive", 0, 1),  # note: alive == 0
         age = deathYear - birthYear) %>%
  filter(!is.na(deathYear)) %>%
  select(birthYear, deathYear, status, sex, age, acquisition, species) %>%
  filter(deathYear >= birthYear) %>%
  filter(sex != "U")
head(dolphin_survival)
```

`status` is `0` if `alive`, `1` if `died`

So the followin gives the median age of death, with confidence intervals.

```{r}
model <- survival::survfit(survival::Surv(age, status) ~ 1, dolphin_survival)
model
```

```{r survival_dolphins}
broom::tidy(model) %>%
  ggplot(aes(time, estimate)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```


The followin gives the median age of death, by sex, with confidence intervals.

```{r}
model <- survival::survfit(survival::Surv(age, status) ~ sex, dolphin_survival)
model
```

```{r survival_dolphins_sex}
broom::tidy(model) %>%
  ggplot(aes(time, estimate, color = strata)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```

How can we tell if sex is actually meaningful (i.e. the survival rate is actual different and not due to chance?)

We can use a `Cox proportional hazards regression model`

```{r}
survival::coxph(survival::Surv(age, status) ~ sex, dolphin_survival) %>%
  tidy()
```

p.value is not statistically significant (confience intervals include 0) so we can say that there is an actual difference.

We can do the same thing for acquisition.

```{r survival_dolphins_acquisition}
model <- survival::survfit(survival::Surv(age, status) ~ acquisition, dolphin_survival)
broom::tidy(model) %>%
  filter(strata != "acquisition=Unknown") %>%
  ggplot(aes(time, estimate, color = strata)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```

If we do `coxph` it looks like the holdout group is `Born` and each category is being compared to that. `Capture` is not statistically significant, but `Rescue` and `Unknown` are.

```{r}
survival::coxph(survival::Surv(age, status) ~ acquisition, dolphin_survival) %>%
  tidy()
```

## Pairwise Correlation

In this case, we are finding correlations of `lifeExp` among `countries` across/by `year`. So, which countries tend to have similar life-expectencies over time (i.e. by year)?

```{r message=FALSE, warning=FALSE}
#library(gapminder)
#library(widyr)

life_expectency_pairwise_cor <- gapminder::gapminder %>% 
         filter(continent == 'Americas') %>%  # filter for one continent just so we can fit into a single graph.
         # add_count(country) %>%
         widyr::pairwise_cor(country, year, lifeExp, sort = TRUE)

head(life_expectency_pairwise_cor)
```

```{r pairwise_correlations_us_mexico_canada, fig.height=5, fig.width=7}
life_expectency_pairwise_cor %>%
    filter(item1 %in% c('United States', 'Mexico', 'Canada')) %>%
    ggplot(aes(x=correlation, y=tidytext::reorder_within(x=item2, by=correlation, within=item1))) +
    geom_col() +
    tidytext::scale_y_reordered() +
    facet_wrap(~item1, scales='free') +
    labs(title="Countries with Similar Life-Expectencies to US, Mexico, Canada",
         x='Correlation of Life-Expectencies',
         y=NULL)
```

```{r pairwise_correlations_country_life_expectencies, fig.height=7, fig.width=7}
life_expectency_pairwise_cor %>%
    mutate(item1=factor(item1, ordered = TRUE),
           item2=factor(item2, ordered = TRUE)) %>%
    filter(item1 < item2) %>%
    ggplot(aes(item1, item2, fill= correlation)) +
    geom_tile() +
    scale_fill_gradient(low="white", high="blue") +
    theme(axis.text.x=element_text(angle=90, hjust=1)) +
    labs(title='Which countries tend to have similar life-expectencies, over time?',
         subtitle = '(Americas)',
         x=NULL,
         y=NULL)
```

```{r}
life_expectency_pairwise_cor <- life_expectency_pairwise_cor %>%
    mutate(item1=factor(item1),
           item2=factor(item2)) %>%
    arrange(item2) %>%
    pivot_wider(names_from = item2, values_from = correlation) %>%
    arrange(item1)

life_expectency_pairwise_cor[1:5, 1:5]
```

## `ebbr` package: Empirical Bayes on the Binomial in R

> Methods for empirical Bayes shrinkage and estimation on data with many observations of success/total counts.

https://github.com/dgrtwo/ebbr

Examples from `Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.`

```{r example_11.1, echo=FALSE}
#devtools::install_github("dgrtwo/ebbr")
#install.packages('Lahman')
#library(Lahman)

# Grab career batting average of non-pitchers
# (allow players that have pitched <= 3 games, like Ty Cobb)
pitchers <- Lahman::Pitching %>%
    group_by(playerID) %>%
    summarize(gamesPitched = sum(G)) %>%
    filter(gamesPitched > 3)

# Add player names
player_names <- Lahman::Master %>%
    tibble::as_tibble() %>%
    dplyr::select(playerID, nameFirst, nameLast, bats) %>%
    unite(name, nameFirst, nameLast, sep = " ")

# include the "bats" (handedness) and "year" column for later
career_full <- Lahman::Batting %>%
    filter(AB > 0) %>%
    # remove pitchers
    anti_join(pitchers, by = "playerID") %>%
    group_by(playerID) %>%
    summarize(H = sum(H),
              AB = sum(AB),
              year = mean(yearID)) %>%
    ungroup() %>%
    inner_join(player_names, by = "playerID") %>%
    filter(!is.na(bats)) %>%
    rename(hits=H, at_bats=AB) %>%
    mutate(batting_average = hits / at_bats)
# We don't need all this data for every step
career <- career_full %>% 
    select(playerID, name, hits, at_bats, batting_average)

head(career)
```

> In Chapter 3, we noticed that the distribution of player batting averages looked roughly like a beta distribution (Figure 11.1). We thus wanted to estimate the beta prior for the overall dataset, which is the first step of empirical Bayes analysis. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r ebb_batting_average_distribution}
career %>%
    filter(at_bats > 500) %>%
    ggplot(aes(x=batting_average)) +
    geom_histogram(bins = 50) +
    labs(title='Batting Average Distribution of Players with >500 At-Bats')
```


### Prior Distribution

```{r message=FALSE, warning=FALSE}
#devtools::install_github("dgrtwo/ebbr")
#library(ebbr)
prior <- career %>%
  filter(at_bats >= 500) %>%
  ebbr::ebb_fit_prior(hits, at_bats)

prior
```

I'm not sure how `tidy(prior)$mean` is calculated.

```{r}
as.numeric(tidy(prior)$mean)

career %>%
    filter(at_bats >= 500) %>%
    summarise(sum(hits) / sum(at_bats))

career %>%
    filter(at_bats >= 500) %>%
    summarise(mean(batting_average))

career %>%
    filter(at_bats >= 500) %>%
    summarise(median(batting_average))
```

```{r ebb_beta_distribution_prior}
beta_distribution <- data.frame(x=seq(0.17,0.35,0.001)) %>%
    mutate(y=dbeta(x, prior$parameters$alpha, prior$parameters$beta))

beta_distribution %>%
    ggplot(aes(x=x, y=y)) +
    geom_line() +
    labs(title="Beta Distribution using Alpha/Beta from Calculated Priors",
         subtitle = glue::glue("({ round(prior$parameters$alpha, 2) }, { round(prior$parameters$beta, 2) })"))

career %>%
    filter(at_bats > 500) %>%
    ggplot(aes(x=batting_average)) +
    geom_histogram(bins = 50) +
    geom_line(data=beta_distribution, aes(x=x, y=y*17), color='red') +
    labs(title="Batting Average Distribution of Players with >500 At-Bats",
         subtitle=glue::glue("Red Line is Beta Distribution using Alpha/Beta from Calculated Priors ({ round(prior$parameters$alpha, 2) }, { round(prior$parameters$beta, 2) })"))
```

### Updating Observations based on Priors

> The second step of empirical Bayes analysis is updating each observation based on the overall statistical model. Based on the philosophy of the broom package, this is achieved with the augment() function. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
head(augment(prior, data = career))
```

### `add_ebb_estimate`

> Notice we've now added several columns to the original data, each beginning with . (which is a convention of the augment verb to avoid rewriting existing columns). We have the .alpha1 and .beta1 columns as the parameters for each player's posterior distribution, as well as .fitted representing the new posterior mean (the "shrunken average"). We often want to run these two steps in sequence: estimating a model, then using it as a prior for each observation. The ebbr package provides a shortcut, combining them into one step with add_ebb_estimate(). (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
eb_career <- career %>%
  ebbr::add_ebb_estimate(hits, at_bats,
                   prior_subset = at_bats >= 500)
```

```{r}
all(eb_career$batting_average == eb_career$.raw)
```

> This  [the graph below] was one of the most important visualizations in Chapter 3. I like how it captures what empirical Bayes estimation is doing: moving all batting averages towards the prior mean (the dashed red line), but moving them less if there is a lot of information about that player (high at_bats). (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

The red line represents points where the raw batting average and the estimated batting average are identical.

The dashed red line represents the "prior mean" which is similar, but doesn't seem to be exactly, the average of the batting averages (of people with >= 500 at-bats).

So for example, look at the dots/people to the left of the graph. These are people that had very few at-bats (dark color), had a very low batting average (near 0 on x-axis), but we shifted from their raw batting average to the prior mean batting average (i.e. shifted from solid red line to dashed red line).

The people with a lot of at bats, tend to have estimated batting average that is very close to the raw batting average.

```{r ebb_raw_vs_shrunken, message=FALSE, warning=FALSE}
eb_career %>%
    ggplot(aes(x=.raw, y=.fitted, color = at_bats)) +
    geom_point() +
    geom_abline(color = "red") +
    scale_color_continuous(trans = "log", breaks = c(1, 10, 100, 1000)) +
    geom_hline(yintercept = tidy(prior)$mean, color = "red", lty = 2) +
    coord_cartesian(xlim = c(0, .6), ylim=c(0, 0.6)) +
    scale_x_continuous(breaks = pretty_breaks()) +
    scale_y_continuous(breaks = pretty_breaks()) +
    labs(x = "Raw batting average",
       y = "Shrunken batting average")
```

```{r ebb_raw_vs_shurnked_at_bats, message=FALSE}
eb_career %>%
    filter(at_bats > 10) %>%
    rename(Raw = .raw, Shrunken = .fitted) %>%
    gather(type, estimate, Raw, Shrunken) %>%
    ggplot(aes(at_bats, estimate)) +
    geom_point() +
    geom_smooth(method='lm') +
    geom_hline(yintercept = tidy(prior)$mean, color = "red", lty = 2) +
    facet_wrap(~ type) +
    scale_x_log10()
```

This graph shows that we are shrinking people with a low number of at bats toward the prior mean (red dotted line) rather than towards the fitted regression line, which shows that the more at-bats someone has, the higher their batting-average tends to be (the better you are the more at bats you get). We'll solve this below, but for now, this means that, of the people who have very few at bats, we're likely over-estimating their ability/batting-average. 

```{r ebb_estimated_batting_average}
eb_career %>%
    head(10) %>%
    mutate(name = reorder(name, .fitted)) %>%
    ggplot(aes(x=.fitted, y=name)) +
    geom_point() +
    geom_errorbarh(aes(xmin = .low, xmax = .high)) +
    geom_point(aes(x=.raw), color='red') +
    geom_text(aes(x=.raw, label=glue::glue("({ hits } / { at_bats })")), vjust=-0.7, size=3) +
    labs(x = "Estimated batting average (w/ 95% confidence interval)",
         y = "Player")
```


### Hierarchical Modeling

> In Chapters 7 and 8, we examined how this beta-binomial model may not be appropriate, because of the relationship between a player's at-bats and their batting average. Good batters tend to have long careers, while poor batters may retire quickly. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

This means that, in the previous estimates and graphs, it's not appropriate to move people who have very few bats (and for example very low batting averages), all the way up to the prior mean batting average (``r tidy(prior)$mean``). We should assume (had they keep getting more and more at-bats) that they would have a lower batting average than average.

```{r ebb_at_bats_regression, message=FALSE}
career %>%
  filter(at_bats >= 10) %>%
  ggplot(aes(at_bats, hits / at_bats)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10()
```

This graphs shows that the more at-bats someone has, the better their batting average tends to be, which makes sense. This is the line that we should be pushing people towards who have very few at bats, rather than the dotted red line above.

> We solved this by fitting a prior that depended on AB, through the process of beta-binomial regression. The add_ebb_estimate() function from ebbr offers this option, by setting method = "gamlss" and providing a formula to mu_predictors.3 (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

Obviously, now we do not want to filter our prior based on >=500 at-bats because we are using at-bats directly in our estimation.

```{r message=FALSE, warning=FALSE}
eb_career_ab <- career %>%
  ebbr::add_ebb_estimate(hits, at_bats, method = "gamlss",
                    mu_predictors = ~ log10(at_bats))

head(eb_career_ab)
```

```{r ebb_at_bats_raw_vs_shrunken, message=FALSE}
eb_career_ab %>%
    filter(at_bats > 10) %>%
    rename(Raw = .raw, Shrunken = .fitted) %>%
    gather(type, estimate, Raw, Shrunken) %>%
    ggplot(aes(at_bats, estimate)) +
    geom_point() +
    geom_smooth(method='lm') +
    geom_hline(yintercept = tidy(prior)$mean, color = "red", lty = 2) +
    facet_wrap(~ type) +
    scale_x_log10()
```

Now, we are shrinking the players with fewer at-bats to a lower batting average.

```{r ebb_estimated_batting_average_at_bats}
eb_career_ab %>%
    head(10) %>%
    mutate(name = reorder(name, .fitted)) %>%
    ggplot(aes(x=.fitted, y=name)) +
    geom_point() +
    geom_errorbarh(aes(xmin = .low, xmax = .high)) +
    geom_point(aes(x=.raw), color='red') +
    geom_text(aes(x=.raw, label=glue::glue("({ hits } / { at_bats })")), vjust=-0.7, size=3) +
    labs(x = "Estimated batting average (w/ 95% confidence interval)",
         y = "Player")
```

Now, Andy Abad and Dan Abbot have much lower estimated batting-averages compared to the graph above. In the graph above Dan Abbot was estimated to have a slightly higher batting average than Kurt Abbott, who has many more at-bats than Dan. Now, Dan is estimated to have a much lower batting-avearge.

An important note is that this will not work for rookies, it is only meant to assess the players' entire career. 

http://varianceexplained.org/r/beta_binomial_baseball/

Question in comments

> About this post; don't you think that letting the estimate being affected by AB biases the estimate for young players who have low AB because they are starting and not because are not playing? Would it be interesting to use something like mean of AB per game?

Answer by Dave

> Your question raises a very important issue- I bring it up in the following post in this series (http://varianceexplained.or.... In short you're 100% right that this doesn't work for predicting future performance of rookies without more adjustments. "One important aspect of this prediction is that it won’t be useful when we’ve just hired a “rookie” player, and we’re wondering what his batting average will be. This observed variable ABAB is based on a player’s entire career, such that a low number is evidence that a player didn’t have much of a chance to bat. (If we wanted to make a prediction, we’d have to consider the distribution of possible ABAB’s the player could end up with and integrate over that, which is beyond the scope of this post)."

```{r message=FALSE, warning=FALSE}
#library(splines)
eb_career_prior <- career_full %>%
  ebbr::ebb_fit_prior(hits, at_bats, method = "gamlss",
                      mu_predictors = ~ 0 + splines::ns(year, df = 5) * bats + log(at_bats))

head(eb_career_prior)
```

```{r ebb_gamlss_splines, message=FALSE, warning=FALSE}
# fake data ranging from 1885 to 2013
fake_data <- tidyr::crossing(hits = 300,
                             at_bats = 1000,
                             year = seq(1885, 2013),
                             bats = c("L", "R"))

# find the mean of the prior, as well as the 95% quantiles,
# for each of these combinations. This does require a bit of
# manual manipulation of alpha0 and beta0:
augment(eb_career_prior, newdata = fake_data) %>%
    mutate(prior = .alpha0 / (.alpha0 + .beta0),
         prior.low = qbeta(.025, .alpha0, .beta0),
         prior.high = qbeta(.975, .alpha0, .beta0)) %>%
    ggplot(aes(year, prior, color = bats)) +
    geom_line() +
    geom_ribbon(aes(ymin = prior.low, ymax = prior.high), alpha = .1, lty = 2) +
    ylab("Prior distribution (mean + 95% quantiles)")
```

### Hypothesis Testing

> For example, we wanted to get a posterior probability for the statement "this player's true batting average is greater than .300", so that we could construct a "Hall of Fame" of such players. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
test_300 <- career %>%
    ebbr::add_ebb_estimate(hits, at_bats, method = "gamlss", mu_predictors = ~ log10(at_bats)) %>%
    ebbr::add_ebb_prop_test(.300, sort = TRUE)

round_4 <- function(.x) {
    round(.x, 4)
}

test_300 %>%
    select(name, hits, at_bats, .fitted, .low, .high, .pep, .qvalue) %>%
    mutate_if(is.numeric, round_4) %>%
    head()
```

> `.pep`: the posterior error probability- the probability that this player's true batting average is less than .3. `.qvalue`: the q-value, which corrects for multiple testing by controlling for false discovery rate (FDR). Allowing players with a q-value `below .05` would mean only 5% of the ones included would be false discoveries. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
sum(test_300$.qvalue < .05)
sum(test_300$.qvalue < .01)
```

#### Player-Player AB-test

> Chapter 6 discussed the case where instead of comparing each observation to a single threshold (like .300) we want to compare to another player's posterior distribution. We noted that this is similar to the problem of "A/B testing", where we might be comparing two clickthrough rates, each represented by successes / total. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

```{r}
piazza <- eb_career_ab %>%
  filter(name == "Mike Piazza")

piazza_params <- c(piazza$.alpha1, piazza$.beta1)
piazza_params ## [1] 2281 5183 This vector of two parameters, an alpha and a beta, can be passed into add_ebb_prop_test just like we passed in a threshold.4 
```

```{r}
compare_piazza <- eb_career_ab %>%
  ebbr::add_ebb_prop_test(piazza_params, approx = TRUE, sort = TRUE)

compare_piazza %>%
    select(name, hits, at_bats, .fitted, .low, .high, .pep, .qvalue) %>%
    mutate_if(is.numeric, round_4) %>%
    head()
```

> Just like the one-sample test, the function has added .pep and .qvalue columns. From this we can see a few players who we're extremely confident are better than Piazza. (Robinson, David. Introduction to Empirical Bayes: Examples from Baseball Statistics . Kindle Edition.)

## Tidy Text

```{r message=FALSE, warning=FALSE}
head(mr_boston)
```

## Pairwise Correlations (again)

Example from David Robinson screencase (https://youtu.be/EC0SVkFB2OU?t=1234)

```{r}
#library(widyr)
#library(tidytext)
ingredient_pairs <- mr_boston %>%
  add_count(ingredient) %>%
  filter(n >= 10) %>%
  widyr::pairwise_cor(ingredient, name, sort = TRUE)

head(ingredient_pairs)
```

```{r pairwise_correlations_cocktails}
ingredient_pairs %>%
  filter(item1 %in% c("Gin", "Tequila", "Absinthe",
                      "Mezcal", "Bourbon whiskey",
                      "Vodka")) %>%
  group_by(item1) %>%
  top_n(10, correlation) %>%
  mutate(item2 = tidytext::reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(correlation, item2)) +
  geom_col() +
  facet_wrap(~ item1, scales = "free_y") +
  tidytext::scale_y_reordered() +
  labs(title = "What ingredients are most correlated with particular ingredients?")
```

## Network Graph

```{r}
mr_boston_parsed <- mr_boston %>%
    extract(measure, "amount", regex = "(.*) oz", remove = FALSE) %>%
    extract(amount, "ones", regex = "(^\\d+$|^\\d+ )", convert = TRUE, remove = FALSE) %>%
    extract(amount, c("numerator", "denominator"),
          regex = "(\\d+)\\/(\\d+)", convert = TRUE, remove = FALSE) %>%
    replace_na(list(ones = 0, numerator = 0, denominator = 1)) %>%
    mutate(oz = ones + numerator / denominator,
         oz = na_if(oz, 0))


ingredients_summarized <- mr_boston_parsed %>%
    # first, groupr by drink name so that we can create a percentile (based off of ingredient number) for all ingrediates within their drink
    group_by(name) %>%
    mutate(percentile = row_number() / n()) %>%
    ungroup() %>%
    # now group by ingredient.
    group_by(ingredient) %>%
    summarize(n = n(),
            n_with_oz = sum(!is.na(oz)),
            avg_position = mean(percentile),
            avg_serving = mean(oz, na.rm = TRUE)) %>%
    arrange(desc(n))

head(ingredients_summarized)
```

`avg_position` is the step number that the ingredient, on average, shows up when mixing the drink.

https://youtu.be/EC0SVkFB2OU?t=1269

```{r network_graph_cocktails, fig.height=7, fig.width=7}
#library(ggraph)
#library(igraph)
top_cors <- ingredient_pairs %>% head(150)  # most correlated cocktail ingrediants
ingredient_info <- ingredients_summarized %>% filter(ingredient %in% top_cors$item1)

set.seed(2)
top_cors %>%
  igraph::graph_from_data_frame(vertices = ingredient_info) %>%
  ggraph::ggraph(layout = "fr") +
  ggraph::geom_edge_link() +
  ggraph::geom_node_text(aes(label = name), repel = TRUE) +
  ggraph::geom_node_point(aes(size = 1.1 * n)) +
  ggraph::geom_node_point(aes(size = n, color = avg_position)) +
  scale_color_gradient2(low = "red", high = "blue", midpoint = .5,
                        labels = scales::percent_format()) +
  labs(size = "# of recipes",
       color = "Avg position in drink",
       title = "The constellations of cocktail ingredients",
       subtitle = "Connected ingredients tend to appear in the same recipes. Red ingredients are early in the recipe, blue tend to be later")
```

## Singular Value Decomposition (PCA)

https://youtu.be/EC0SVkFB2OU?t=3009

What dimensions drive a lot of the variation among cocktails?

Here we have broken the data down by dimensions.

What pairs of ingrediants would never appear in the same 'type' of cocktail together?

This would look better if we removed duplicate ingrediants (e.g. combine `Fresh lemon juice` and `Juice of a Lemon`)

```{r singular_value_decomposition_ingrediants}
ingredient_svd <- mr_boston %>%
    distinct(ingredient, name) %>%
    mutate(value = 1) %>%
    # we're interested in components by intgrediate across name
    widyr::widely_svd(ingredient, name, value)

ingredient_svd %>%
    filter(dimension > 1, 
           dimension <= 5) %>%
    mutate(dimension = paste0("PC", dimension)) %>%
    group_by(dimension) %>%
    top_n(16, abs(value)) %>%
    mutate(ingredient = tidytext::reorder_within(ingredient, value, dimension)) %>%
    ggplot(aes(value, ingredient, fill = value > 0)) +
    geom_col(show.legend = FALSE) +
    tidytext::scale_y_reordered() +
    facet_wrap(~ dimension, scales = "free_y") +
    labs(x = "Principal component value",
         y = "Ingredient",
         title = "What are the sources of variation in ingredients?")
```

```{r singular_value_decomposition_recipes}
recipe_svd <- mr_boston %>%
  distinct(name, ingredient) %>%
  mutate(value = 1) %>%
  widyr::widely_svd(name, ingredient, value)

recipe_svd %>%
  filter(dimension > 1, dimension <= 5) %>%
  mutate(dimension = paste0("PC", dimension)) %>%
  group_by(dimension) %>%
  top_n(16, abs(value)) %>%
  mutate(recipe = tidytext::reorder_within(name, value, dimension)) %>%
  ggplot(aes(value, recipe, fill = value > 0)) +
  geom_col(show.legend = FALSE) +
  tidytext::scale_y_reordered() +
  facet_wrap(~ dimension, scales = "free_y") +
  labs(x = "Principal component value",
       y = "Ingredient",
       title = "What are the sources of variation in recipes?")
```

## Logistic Regression

```{r message=FALSE, warning=FALSE}
head(beer_awards)
```

```{r}
awards_by_year_state <- beer_awards %>%
  add_count(year, name = "total_yearly_awards") %>%
  mutate(state = fct_lump(state, 9)) %>%
  count(year, state, total_yearly_awards, name='awards_won', sort = TRUE) %>%
  mutate(pct_awards_won = awards_won / total_yearly_awards,
         awards_not_won = total_yearly_awards - awards_won)

head(awards_by_year_state)
```

```{r beer_awards_by_year_state}
awards_by_year_state %>%
  filter(state != "Other") %>%
  ggplot(aes(year, pct_awards_won, color = state)) +
  geom_line() +
  expand_limits(y = 0) +
  scale_y_continuous(labels = percent) +
  facet_wrap(~ state)
```

How has the probability of successes vs failtures (i.e. awards vs not awards) changed over time?

See https://github.com/shane-kercheval/r-examples/blob/main/examples/logistic_regression/logistic_regression.md

> Logistic Regression is a linear model for log odds. The odds of an event are the probability that it happens over the probability that it doesn’t.

> `log[ p / (1-p) ] = B0 + B1X1 + B2X2 + ... + error`


`log( succeses / failures)` i.e. `log( awards_won / awards_not_won)`

```{r}
wi_model <- awards_by_year_state %>%
    filter(state == "WI") %>%
    # cbind(successes, failures)
    glm(cbind(awards_won, awards_not_won) ~ year,
      data = .,
      family = "binomial")

wi_model %>% summary()
```

What do the coefficients mean?

> This is a log-odds ratio. So this is how much the log-odds ratio changes each year.  (https://youtu.be/BV_afpCDQ70?t=3001)

```{r}
(.log_odds <- coef(wi_model)['year'])
(.odds <- -1 / exp(.log_odds))
```

- http://had.co.nz/notes/modelling/logistic-regression.html
- https://www.flutterbys.com.au/stats/tut/tut10.5a.html

```{r}
#library(broom)
models_by_state <- awards_by_year_state %>%
  filter(state != "Other") %>%
  mutate(state = state.name[match(state, state.abb)]) %>%
  group_by(state) %>%
  summarize(model = list(glm(cbind(awards_won, awards_not_won) ~ year, family = "binomial"))) %>%
  mutate(tidied = map(model, broom::tidy, conf.int = TRUE)) %>%
  unnest(tidied) %>%
  filter(term == "year") %>%
  mutate(#p.value = format.pval(p.value),
         state = fct_reorder(state, estimate))

models_by_state %>% mutate_if(is.numeric, function(.x) {round(.x, 3)})
```

```{r logistic_regression_coefficients_beer_awards}
models_by_state %>%
  ggplot(aes(estimate, state, color=p.value < .05)) +
  geom_point() +
  geom_vline(xintercept = 0, lty = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .2) +
  labs(x = "Estimated slope",
       title = "Which states become more or less frequent medal winners?",
       y = "")
```

## Nested Regression

Example from David Robinson's screencast (https://youtu.be/lY0YLDZhT88?t=2977)

```{r}
.ikea <- ikea %>% select(price_usd, category, other_colors, depth, height, width)
head(.ikea)
```

```{r}
.ikea_volume <- .ikea %>%
  mutate(volume_m3 = depth * height * width / 1e6) %>%
  filter(!is.na(volume_m3),
         volume_m3 >= .001) %>%
  arrange(desc(volume_m3)) %>%
  add_count(category, name = "category_total")

head(.ikea_volume)
```

```{r}
.ikea_model <-lm(log2(price_usd) ~ log2(volume_m3), data = .ikea_volume)
```

If we were to predict price based on only the volumn (cubic meters).

```{r}
.ikea_model %>% summary()
```

Then what this says that our Intercept (which is where `log2(volume_m3)` is `0`; or in other words when the volumn_m3 is 1 (cubic meter) because `log2(1)` equals `0`) ...

```{r}
log2(1)
```

then `log2(price_usd)` is ``r coef(.ikea_model)['(Intercept)']`` 

```{r}
coef(.ikea_model)['(Intercept)']
```

In other words, the price is ``r 2 ^ coef(.ikea_model)['(Intercept)']``

```{r}
2 ^ coef(.ikea_model)['(Intercept)']
```

Putting it together, when the volume of the furniture is 1 cubic meter, we would predict that the price is `$`r 2 ^ coef(.ikea_model)['(Intercept)']``

For each increase in `log2(volume_m3)` we would expect an increase in `log2(price_usd)` by ``r coef(.ikea_model)['log2(volume_m3)']``

```{r}
coef(.ikea_model)['log2(volume_m3)']
```

i.e. every time we doulbe in cubic meters we would expect an increase to the price by a factor of ``r 2 ^ coef(.ikea_model)['log2(volume_m3)']``

```{r}
2 ^ coef(.ikea_model)['log2(volume_m3)']
```

But now lets include `category` and `other_colors`.

```{r nested_regression_ikea, fig.height=5, fig.width=7}
#library(broom)
.ikea_volume %>%
    # we are making `Tables & desk` the first factor level and therefore the reference category in the model for category
    mutate(category = fct_relevel(category, "Tables & desks")) %>%
    lm(log2(price_usd) ~ log2(volume_m3) + category + other_colors, data = .) %>%
    broom::tidy(conf.int = TRUE) %>%
    filter(term != "(Intercept)") %>%
    mutate(term = ifelse(term == "log2(volume_m3)", "Item volume (doubling)", term),
           term = str_remove(term, "^category")) %>%
    mutate(term = fct_reorder(term, estimate)) %>%
    ggplot(aes(estimate, term, color=p.value <= 0.05)) +
    geom_point() +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .2) +
    geom_vline(xintercept = 0, color = "red", lty = 2) +
    labs(x = "Impact on price (relative to Tables & desks)",
         y = "",
         title = "What objects are unusually expensive/inexpensive relative to volume?")
```

```{r include=FALSE}
knitr::knit_exit()
```

```{r eval=FALSE, include=FALSE}
restaurant_inspections_raw <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")
restaurant_inspections <- restaurant_inspections_raw %>% 
  janitor::clean_names() %>%
  select(-phone, -grade_date, -record_date, -building, -street) %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  separate(inspection_type, c("inspection_program", "inspection_type"), sep = " / ")

inspections <- restaurant_inspections %>%
  group_by(camis,
           dba,
           boro,
           zipcode,
           cuisine_description,
           inspection_date,
           action,
           score,
           grade,
           inspection_type,
           inspection_program) %>%
  summarize(critical_violations = sum(critical_flag == "Critical", na.rm = TRUE),
            non_critical_violations = sum(critical_flag == "Not Critical", na.rm = TRUE)) %>%
  ungroup()

most_recent_cycle_inspection <- inspections %>%
  filter(inspection_program == "Cycle Inspection",
         inspection_type == "Initial Inspection") %>%
  arrange(desc(inspection_date)) %>%
  distinct(camis, .keep_all = TRUE)

restaurant_inspections_by_dba <- most_recent_cycle_inspection %>%
  group_by(dba, cuisine = cuisine_description) %>%
  summarize(locations = n(),
            avg_score = mean(score),
            median_score = median(score)) %>%
  ungroup() %>%
  arrange(desc(locations))

saveRDS(restaurant_inspections_by_dba, 'data/restaurant_inspections_by_dba.RDS')
```