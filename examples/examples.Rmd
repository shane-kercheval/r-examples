---
title: ""
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#devtools::install_github('shane-kercheval/rtools')
#library(rtools)
# library(stringr)
# library(ggrepel)
# library(forecast)
library(scales)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(knitr)

options(scipen=999) # non-scientific notation
options(dplyr.summarise.inform=F)

theme_set(theme_light())

calculate_plot_width <- function(plot_height) { plot_height * 1.61803398875 }
plot_width_height_6 <- calculate_plot_width(6)
plot_width_height_7 <- calculate_plot_width(7)
plot_width_height_8 <- calculate_plot_width(8)
```

# Packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(janitor)
library(ggplot2)
library(tidytext)
```

# Data Cleaning

---

```{r eval=FALSE, include=FALSE}
CO2
starwars
storms
```

## General

### `clean_names()`

```{r}
iris %>% colnames()
```

```{r}
iris %>% janitor::clean_names() %>% colnames()
```

---

### `extract()`

Turn `1` column into `x` columns based on regex

`convert=TRUE` converts them to numeric

```{r}
.df <- data.frame(season_epison = paste0('S', c(1, 1, 1, 2, 2, 2, 3, 3, 3),
                                         'E', c(1, 2, 3, 1, 2, 3, 1, 2, 3)))

.df %>% extract(season_epison, c('season', 'episode'), 'S(.*)E(.*)', convert = TRUE, remove = FALSE)
```

---

### regex_left_join

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=1715)

Join data.frames based on matching regex.

In this example, we will create categories and add them to our data.frame based on regex.

```{r message=FALSE, warning=FALSE}
#install.packages('fuzzyjoin')
library(fuzzyjoin)

cetaceans_raw <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-12-18/allCetaceanData.csv') %>% 
    select(-X1) %>%
    mutate(birthYear = as.numeric(birthYear))
cetaceans <- cetaceans_raw %>% select(species, originLocation)
head(cetaceans)
```

```{r}

regexes <- tribble(
  ~ regex, ~ category,
  "Unknown", "Unknown",
  "Gulf of Mexico", "Gulf of Mexico",
  "Florida|FL", "Florida",
  "Texas|TX", "Texas",
  "SeaWorld", "SeaWorld",
  "Pacific", "Pacific Ocean",
  "Atlantic", "Atlantic Ocean"
)

cetaceans %>%
    # left-join will create multiple rows if there are muliple matches
    # so create a row number to track unique rows
    mutate(unique_id = row_number()) %>%
    # join on regexes, based on regex
    regex_left_join(regexes, c(originLocation = "regex")) %>%
    # only keep the unique/distinct rows from the data.frame
    # If there are multiple rows for a given combination of inputs, only the first row will be preserved.
    # If omitted, will use all variables.
    # .keep_all:  If TRUE, keep all variables in .data.
    distinct(unique_id, .keep_all = TRUE) %>%
    # coalesce gets the first value that is not NA
    # so if category is not NA then use category, else use originLocation
    mutate(category = coalesce(category, originLocation)) %>%
    head(20)
```

---

## Aggregation

### `group_by() & which.max()`

```{r}
which.max(c(2, 1, 4, 3))
which.max(c(2, 4, 4, 3))
```

`first(name[which.max(height)])`

```{r}
starwars %>%
    group_by(gender) %>%
    summarise(n = n(),
              tallest_person = first(name[which.max(height)]),
              tallest_height = max(height, na.rm = TRUE),
              oldest_person = first(name[which.min(birth_year)]))
```

[Tidy Tuesday screencast: analyzing franchise revenue - YouTube](https://youtu.be/1xsbTs9-a50?t=365)

---

### `group_by()` & `top_n()`

This gets the `N` rows associated with the top `N` values for each category being grouped

`top_n(3, height)`

```{r}
starwars %>%
    group_by(gender) %>%
    top_n(3, height) %>%
    select(gender, name, height) %>%
    arrange(gender, height) %>%
    ungroup()
```

---

### `summarise()` & `across`

* Summarize multiple columns with multiple functions
* name columns with `glue` style convention

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(across(starts_with("d"),
                   list(mean = mean,
						   sd = sd),
                   .names = "{col}_{fn}"))
```

---

same as above using formulas

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(across(starts_with("d"), 
                   list(minus_sd = ~ (mean(.x) - sd(.x)), 
                        mean = mean, 
                        plus_sd = ~ (mean(.x) + sd(.x)))
                   ))
```

---

### `summarise_at()`

```{r}
iris %>% group_by(Species) %>% summarise_at(vars(Sepal.Length, Sepal.Width), sum, na.rm = TRUE)
```

---

### `add_count`

```{r}
iris %>% add_count(Species, name = 'num_species') %>% head()
```

`add_count(Species, name = 'num_species')` is equivalent to `group_by(Species) %>% mutate(num_species = n()) %>% ungroup()`

---

# dplyr/tidyverse 

## `join` `suffix`

```{r}
band_members %>%
    inner_join(band_members, by = 'name',
               suffix = c('.x', '.y'))
```


## `semi-join`

```{r}
band_members
```

```{r}
band_instruments
```

`x %>% semi_join(y)` is an `inner_join(y)` that returns only `x` (doesn't include any columns from `y`)

```{r}
band_members %>% semi_join(band_instruments, by = 'name')
```

```{r}
band_members %>% inner_join(band_instruments, by = 'name') %>% select(name, band)
```

---

## Indirection

Create a dplyr-like function that uses the column names of the dataframe rather than strings or the object directly.

https://dplyr.tidyverse.org/articles/programming.html

### dplyr-like function

```{r}
var_summary <- function(data, var) {
    # note the {{ var }}
    data %>%
        summarise(n = n(),
                  min = min({{ var }}),
                  max = max({{ var }}))
}

mtcars %>% 
    group_by(cyl) %>% 
    var_summary(mpg)
```

> If you want the user to provide a set of data-variables that are then transformed, use across():

```{r}
var_summary <- function(data, .group_by, .var) {
    # note the {{ var }}
    data %>%
        group_by(across({{ .group_by }})) %>%
        summarise(n = n(),
                  min = min({{ .var }}),
                  max = max({{ .var }}))
}

mtcars %>% var_summary(.group_by=cyl, .var=mpg)
```

```{r}
mtcars %>% var_summary(.group_by=c(cyl, vs), .var=mpg)
```

---

> Use the .names argument to across() to control the names of the output.

(Note the vignette uses `.col` instead of `col`, which fails.)

```{r}
my_summarise <- function(data, .group_by, .summarise_vars) {
  data %>%
    group_by(across({{ .group_by }})) %>% 
    summarise(across({{ .summarise_vars }},
                     mean,
                     .names = "mean_{col}"))
}
mtcars %>% my_summarise(cyl, mpg)
```

```{r}
mtcars %>% my_summarise(c(cyl, vs), .summarise_vars=c(mpg, hp))
```

---

### .data

> "Note that `.data` is not a data frame; it’s a special construct, a pronoun, that allows you to access the current variables either directly, with `.data$x` or indirectly with `.data[[var]]`. Don’t expect other functions to work with it."

```{r}
var <- 'cyl'
mtcars %>% count(.data[[var]])
```

```{r}
# ROW_NUMBER() OVER (PARITION BY col_x ORDER BY col_y) AS index
row_number_over_partition_by <- function(data, .partition_by, .order_by) {
    data %>%
        group_by(across({{ .partition_by }})) %>%
        mutate(index = row_number({{ .order_by }})) %>%
        ungroup()
}
mtcars %>% 
    select(cyl, mpg) %>%
    row_number_over_partition_by(cyl, mpg) %>%
    arrange(cyl, mpg)
```

```{r}
mtcars %>% 
    select(cyl, mpg) %>%
    row_number_over_partition_by(cyl, desc(mpg)) %>%
    arrange(cyl, mpg)
```

### eval_tidy

```{r}
with_data <- function(data, .x) {
  expr <- rlang::enquo(.x)
  print(expr)
  rlang::eval_tidy(expr, data = data)
}
mtcars %>% with_data(.x=mean(cyl) * 10)
```

### other examples

```{r}
# this is a hack because row_number only takes one column
#.partition_by, 
row_number_over_partition_by <- function(.x, .col_name, .partition_by, ...) {
    .result <- .x %>%
        mutate(temp______original_order = row_number()) %>%
        group_by(across({{ .partition_by }})) %>%
        arrange( ... , .by_group = TRUE) %>%
        mutate(grouped_index = row_number()) %>%
        ungroup() %>%
        arrange(temp______original_order) %>%
        pull(grouped_index)

    .x[deparse(substitute(.col_name))] <- .result
    
    return (.x)
}
mtcars %>% 
    select(cyl, am, vs, mpg) %>%
    row_number_over_partition_by(
        # new column name
        .col_name = my_index,
        # "partition by"
        .partition_by = c(cyl, am),
        # "order by" i.e. sort index in order of these variable
        desc(vs), mpg
    ) %>%
    arrange(desc(cyl), am, desc(vs), mpg)
```

```{r}
filter_over_partition_by <- function(.x, .partition_by, ...) {
    .x %>%
        mutate(temp______original_order = row_number()) %>%
        group_by(across({{ .partition_by }})) %>%
        arrange( ... , .by_group = TRUE) %>%
        filter(row_number() == 1) %>%
        ungroup() %>%
        arrange(temp______original_order) %>%
        select(-temp______original_order)
}
mtcars %>% 
    select(cyl, am, vs, mpg) %>%
    filter_over_partition_by(
        # "partition by"
        .partition_by = c(cyl, am),
        # "order by" i.e. sort index in order of these variable
        desc(vs), mpg
    ) %>%
    arrange(desc(cyl), am, desc(vs), mpg)
```

# ggplot

### `reorder_within()`

Reorder sub-categories within category e.g. for faceting.

For example, the order of `setosa`, `versicolor`, `virginica` is allowed to change for each measurement (e.g. `Petal.Length`, `Sepal.Width`)

other examples: https://juliasilge.com/blog/reorder-within/

```{r}
iris_gathered <- iris %>% pivot_longer(-Species, names_to = 'metric', values_to = 'value')
iris_gathered %>%
    ggplot(aes(x=tidytext::reorder_within(x=Species, by=value, within=metric),
               y=value)) +
    geom_boxplot() +
    # this is necessary to undo the transformations to the values that reorder_within did
    tidytext::scale_x_reordered() +
    facet_wrap(~ metric, scales = 'free_x')
```

---

### `scale_x_log10()` with seconds

	* scales the x-axis ticks but keeps the actual values the same
	* this works when viewing time (in seconds)

```{r}
data.frame(value = abs(rnorm(1000, sd = (60^4) / 3))) %>%
ggplot(aes(x='', y=value)) +
    geom_point() +
    scale_y_log10(breaks = c(0, 60 ^ (0:4)),
                  labels = c("0", "Second", "Minute", "Hour", "2.5 Days", "120 Days"))
```

---

### ggplot2 with `interaction()`

group by two columns in ggplot2

```{r}
# Data frame with two continuous variables and two factors 
set.seed(0)
x <- rep(1:10, 4)
y <- c(rep(1:10, 2)+rnorm(20)/5, rep(6:15, 2) + rnorm(20)/5)
treatment <- gl(2, 20, 40, labels=letters[1:2])
replicate <- gl(2, 10, 40)
d <- data.frame(x=x, y=y, treatment=treatment, replicate=replicate)
d %>%
    ggplot(aes(x=x, y=y, colour=treatment, shape = replicate,
               group=interaction(treatment, replicate))) +
    geom_point() +
    geom_line()
```

---

### spinogram

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=1244)

We will use `geom_area` but need to fill in the missing data with `complete`

First, here is what it would look like if we didn't use `complete`.

Notice the missing gaps. That is because we have missing years.

```{r message=FALSE, warning=FALSE}
cetaceans <- cetaceans_raw

cateaceans_acquisition_by_decade <- cetaceans %>%
  filter(originDate >= "1960-01-01") %>%
  count(acquisition,
        decade = 5 * (year(originDate) %/% 5))

cateaceans_acquisition_by_decade %>%
    mutate(acquisition = fct_reorder(acquisition, n, sum)) %>%
    group_by(decade) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(decade, percent, fill = acquisition)) +
    geom_area() +
    scale_y_continuous(labels = percent_format()) +
    scale_fill_manual(values=rtools::rt_colors()) +
    theme_minimal() +
    labs(x = "year",
       y = "% of dolphins recorded")
```

This shows everything in `cateaceans_acquisition_by_decade_complete` that is not in `cateaceans_acquisition_by_decade`. `complete` filled in the gabs.

```{r}
cateaceans_acquisition_by_decade_complete <- cateaceans_acquisition_by_decade %>%
  complete(acquisition, decade, fill = list(n = 0))

cateaceans_acquisition_by_decade_complete %>%
    anti_join(cateaceans_acquisition_by_decade, by = c("acquisition", "decade", "n")) %>%
    head(10)
```

```{r}
cateaceans_acquisition_by_decade_complete %>%
    mutate(acquisition = fct_reorder(acquisition, n, sum)) %>%
    group_by(decade) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(decade, percent, fill = acquisition)) +
    geom_area() +
    scale_y_continuous(labels = percent_format()) +
    scale_fill_manual(values=rtools::rt_colors()) +
    theme_minimal() +
    labs(x = "year",
       y = "% of dolphins recorded")
```


# Advanced

## Confidence Intervals w/ t-tests

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/em4FXPf4H-Y?t=1783)

```{r eval=FALSE, include=FALSE}
restaurant_inspections_raw <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")
restaurant_inspections <- restaurant_inspections_raw %>% 
  janitor::clean_names() %>%
  select(-phone, -grade_date, -record_date, -building, -street) %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  separate(inspection_type, c("inspection_program", "inspection_type"), sep = " / ")

inspections <- restaurant_inspections %>%
  group_by(camis,
           dba,
           boro,
           zipcode,
           cuisine_description,
           inspection_date,
           action,
           score,
           grade,
           inspection_type,
           inspection_program) %>%
  summarize(critical_violations = sum(critical_flag == "Critical", na.rm = TRUE),
            non_critical_violations = sum(critical_flag == "Not Critical", na.rm = TRUE)) %>%
  ungroup()

most_recent_cycle_inspection <- inspections %>%
  filter(inspection_program == "Cycle Inspection",
         inspection_type == "Initial Inspection") %>%
  arrange(desc(inspection_date)) %>%
  distinct(camis, .keep_all = TRUE)

restaurant_inspections_by_dba <- most_recent_cycle_inspection %>%
  group_by(dba, cuisine = cuisine_description) %>%
  summarize(locations = n(),
            avg_score = mean(score),
            median_score = median(score)) %>%
  ungroup() %>%
  arrange(desc(locations))

saveRDS(restaurant_inspections_by_dba, 'data/restaurant_inspections_by_dba.RDS')
```

```{r}
restaurant_inspections_by_dba <- readRDS('data/restaurant_inspections_by_dba.RDS')
head(restaurant_inspections_by_dba, 109)
```

`nest(data=-cuisine)` groups by cuisine and creates a data-frame (tibble) out of all of the rest of the columns. That is, it `nests` the all of the data (a data.frame) with each row corresponding to a cuisine.

Then we take each of those data.frames, and run a `t.test` of `ave_score`.

```{r}
library(broom)
cuisine_conf_ints <- restaurant_inspections_by_dba %>%
    add_count(cuisine) %>%
    filter(n > 100) %>%
    nest(data=-cuisine) %>%
    mutate(model = map(data, ~ t.test(.$avg_score))) %>%
    mutate(model = map(model, ~ tidy(.))) %>%
    unnest(model)
head(cuisine_conf_ints, 10)
```

```{r}
cuisine_conf_ints %>%
  mutate(cuisine = str_remove(cuisine, " \\(.*"),
         cuisine = fct_reorder(cuisine, estimate)) %>%
  ggplot(aes(estimate, cuisine)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high)) +
  labs(x = "Average inspection score (higher means more violations)",
       y = "Type of cuisine",
       title = "Average inspection score by type of cuisine in NYC",
       subtitle = "Each restaurant chain was counted once based on its average score")
```

## Survival Analysis

Example from David Robinson Tidy-Tuesday Screencast (https://youtu.be/KiqpX-gNIS4?t=2424)

Context is dolphins. Are some dolphins living longer than they used to? This is hard because some dolphins in are dataset are still alive.

```{r}
library(survival)
cetaceans <- cetaceans_raw
dolphin_survival <- cetaceans %>%
  filter(status %in% c("Alive", "Died")) %>%
  mutate(deathYear = ifelse(status == "Alive", 2017, year(statusDate)),
         status = ifelse(status == "Alive", 0, 1),  # note: alive == 0
         age = deathYear - birthYear) %>%
  filter(!is.na(deathYear)) %>%
  select(birthYear, deathYear, status, sex, age, acquisition, species) %>%
  filter(deathYear >= birthYear) %>%
  filter(sex != "U")
head(dolphin_survival)
```

`status` is `0` if `alive`, `1` if `died`

So the followin gives the median age of death, by sex, with confidence intervals.

```{r}
model <- survival::survfit(Surv(age, status) ~ sex, dolphin_survival)
model
```

```{r}
broom::tidy(model) %>%
  ggplot(aes(time, estimate, color = strata)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```

How can we tell if sex is actually meaningful (i.e. the survival rate is actual different and not due to chance?)

We can use a `Cox proportional hazards regression model`

```{r}
survival::coxph(Surv(age, status) ~ sex, dolphin_survival) %>%
  tidy()
```

p.value is not statistically significant (confience intervals include 0) so we can say that there is an actual difference.

We can do the same thing for acquisition.

```{r}
model <- survival::survfit(Surv(age, status) ~ acquisition, dolphin_survival)
broom::tidy(model) %>%
  filter(strata != "acquisition=Unknown") %>%
  ggplot(aes(time, estimate, color = strata)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  scale_y_continuous(labels = percent_format()) +
  labs(y = "Estimated % survival",
       x = "Age of Dolphin")
```

If we do `coxph` it looks like the holdout group is `Born` and each category is being compared to that. `Capture` is not statistically significant, but `Rescue` and `Unknown` are.

```{r}
survival::coxph(Surv(age, status) ~ acquisition, dolphin_survival) %>%
  tidy()
```
